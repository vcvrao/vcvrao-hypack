<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">

<HTML>
<HEAD>
<TITLE> C-DAC,Pune : High-Perf. Comp. Frontier Technologies Exploration Group and
 CMSD, University of Hyderabad, Technology Workshop hyPACK (October 15-18), 2013 </TITLE>

<META http-equiv=Content-Type content="text/html; charset=iso-8859-1">

<meta name="Description" content="Center for Development of Advanced Computing (C-DAC) Pune and 

   Centre for Modelling Simulation and Design (CMSD), High-Performance Computing (HPC) Facility
   University of Hyderabad, Hyderabad are jointly organizing four days technology 
   workshop on   Hybrid Computing - Coprocessors &amp; Accelerators - 
   Power-aware Computing &amp;  Performance of  
  Application Kernels (HyPACK-2013)
  (Initiatives on Measurement Power Consumption &amp; Performance of Kernels"
    which is scheduled from October 15-18, 2013 at CMSD,
   High-Performance Computing (HPC) facility University of 
Hyderabad, Hyderabad.The hyPACK-2013 is designed four days for HPC GPU Cluster
 for Applications."/>

<meta name="KeyWords" content="Multi-Core, Parallel Processing,
 Software threading,GPGPU,GPU computing,MPI,OpenCL, 
NVIDIA -CUDA,AMD-APP Computing, Intel MIC, C-DAC workshops,OpenMP,Pthreads,Heterogenous Computing,Multi-Core tools,MultiCore 
Processors,GPU Programming, HPC GPU Cluster, 
Performance CDAC Technology training programme(S),Intel Software tools" />

<META id=Copyright content="Copyright (c) 2013,C-DAC." name=Copyright>

<META http-equiv=imagetoolbar content=no>

<LINK href="./../hypack13-files/hypack-main.css" type=text/css rel=stylesheet>
<LINK href="./../hypack13-files/hypack-home.css" type=text/css rel=stylesheet>
<LINK href="./../hypack13-files/hypack-schedule.css" rel=stylesheet>

<SCRIPT language=JavaScript src="./../hypack13-files/hypack-main.js" type=text/javascript></SCRIPT>

<META content="MSHTML 6.00.2900.5726" name=GENERATOR></HEAD>
<BODY style="MARGIN: 0px" leftMargin=0 topMargin=0 marginheight="0" 
marginwidth="0">

<TABLE class=container cellSpacing=0 cellPadding=0 border=0>
  <TBODY>
  <TR>
    <TD class=container>
      <TABLE class=header cellSpacing=0 cellPadding=0 border=0>
        <TBODY>
        <TR>
          <TD class=headerlogo>
           <A href="./../index.html"><IMG alt=hypack-2013 src="./../hypack13-files/hypack-2013-header.jpg" border=0>
           </A>
       </TD>
 
     </TR>
     </TBODY>
     </TABLE>
      

<SCRIPT language=JavaScript1.2 type=text/javascript>

</SCRIPT>

     
    
  <TABLE class=mainmenubar cellSpacing=0 cellPadding=0>
        <TBODY>
        <TR>
          <TD align=middle>
            <TABLE cellSpacing=0 cellPadding=0>
              <TBODY>
 
	      <TR>

                <TD class=menu1><A class=menu1 id=mainmenurow1 
                  onmouseover="javascript:hypackShowSubMenuDelay('1', 1)" 
                  onmouseout="javascript:hypackHideSubMenuDelay('1', 1)" 
       		  href="./../hypack13-about-overview.html">About</A></TD>

                <TD class=menusep></TD>
                <TD class=menu1><A class=menu1 id=mainmenurow2 
                  onmouseover="javascript:hypackShowSubMenuDelay('2', 1)" 
                  onmouseout="javascript:hypackHideSubMenuDelay('2', 1)" 
                  href="./../hypack13-tech-prog-topics-overview.html">  Tech. Prog. </A></TD>
 
               
                <TD class=menusep></TD>
                <TD class=menu1><A class=menu1 id=mainmenurow3 
                  onmouseover="javascript:hypackShowSubMenuDelay('3', 1)" 
                  onmouseout="javascript:hypackHideSubMenuDelay('3', 1)" 
                  href="./../hypack13-mode01-multicore-lab-overview.html">Muti-Core </A></TD>
                
               <TD class=menusep></TD>
                <TD class=menu1><A class=menu1 id=mainmenurow4 
                  onmouseover="javascript:hypackShowSubMenuDelay('4', 1)" 
                  onmouseout="javascript:hypackHideSubMenuDelay('4', 1)" 
                  href="./../hypack13-mode02-arm-proc-lab-overview.html"> ARM Proc</A></TD>

                <TD class=menusep></TD>
                <TD class=menu1><A class=menu1 id=mainmenurow5 
                  onmouseover="javascript:hypackShowSubMenuDelay('5', 1)" 
                  onmouseout="javascript:hypackHideSubMenuDelay('5', 1)" 
                  href="./../hypack13-mode03-coprocessor-lab-overview.html">Coprocessors </A></TD>
                           
                <TD class=menusep></TD>
                <TD class=menu1><A class=menu1 id=mainmenurow6
                  onmouseover="javascript:hypackShowSubMenuDelay('6', 1)" 
                  onmouseout="javascript:hypackHideSubMenuDelay('6', 1)" 
                  href="./../hypack13-mode04-gpgpu-lab-overview.html"> GPUs </A> </TD>

                <TD class=menusep></TD>
                <TD class=menu1><A class=menu1 id=mainmenurow7 
                  onmouseover="javascript:hypackShowSubMenuDelay('7', 1)" 
                  onmouseout="javascript:hypackHideSubMenuDelay('7', 1)" 
                  href="./../hypack13-mode05-hpc-cluster-lab-overview.html"> HPC Cluster</A></TD>

                <TD class=menusep></TD>
                <TD class=menu1><A class=menu1 id=mainmenurow6
                  onmouseover="javascript:hypackShowSubMenuDelay('8', 1)" 
                  onmouseout="javascript:hypackHideSubMenuDelay('8', 1)" 
                  href="./../hypack13-mode06-app-kernels-lab-overview.html"> App. Kernels</A> </TD>

                <TD class=menusep></TD>
                <TD class=menu1><A class=menu1 id=mainmenurow7 
                  onmouseover="javascript:hypackShowSubMenuDelay('9', 1)" 
                  onmouseout="javascript:hypackHideSubMenuDelay('9', 1)" 
                  href="./../hypack13-reg-overview.html">Registration </A></TD>

                                                          
</TR></TBODY></TABLE></TD></TR></TBODY>
	</TABLE>

      <DIV class=menu2>

 <!--   Sub menu for **** Row-1-About ****  start here -->

      <TABLE id=submenutab1 onMouseOver="javascript:hypackShowSubMenuDelay('1',1)" 
       style="VISIBILITY: hidden; MARGIN-LEFT: 0px; POSITION: absolute" 
       onmouseout="javascript:hypackHideSubMenuDelay('1',1)" cellSpacing=0 
       cellPadding=0 width=150>

       <TBODY>
       <TR>
       <TD class=menu2>


      <A class=menu2 id=submenurow1s1 
            onmouseover="javascript:hypackShowSubMenuDelay('1', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('1', 1)" 
            href="./../hypack13-about-overview.html"><B> Overview </B></A>
  
      <A class=menu2 id=submenurow1s2 
            onmouseover="javascript:hypackShowSubMenuDelay('1', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('1', 1)" 
            href="./../hypack13-about-venue.html"><B>  Venue : CMSD, UoH </B></A>

       <A class=menu2 id=submenurow1s3 
            onmouseover="javascript:hypackShowSubMenuDelay('1', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('1', 1)" 
            href="./../hypack13-about-keynote-invited-talks.html"><B>  Key-Note/Invited Talks </B> </A>

        <A class=menu2 id=submenurow1s4 
            onmouseover="javascript:hypackShowSubMenuDelay('1', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('1', 1)" 
            href="./../hypack13-about-faculty.html"><B>  Faculty / Speakers </B></A>

       <A class=menu2 id=submenurow1s5 
            onmouseover="javascript:hypackShowSubMenuDelay('1', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('1', 1)" 
            href="./../hypack13-about-proceedings.html"><B>   Proceedings </B></A>

	<A class=menu2 id=submenurow1s6 
            onmouseover="javascript:hypackShowSubMenuDelay('1', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('1', 1)" 
            href="./../hypack13-about-download-software.html"><B>  Downloads  </B> </A>
  
	<A class=menu2 id=submenurow1s7
            onmouseover="javascript:hypackShowSubMenuDelay('1', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('1', 1)" 
            href="./../hypack13-about-past-workshops.html"><B>  Past Tech. Workshops </B></A>

       <A class=menu2 id=submenurow1s8 
            onmouseover="javascript:hypackShowSubMenuDelay('1', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('1', 1)" 
            href="./../hypack13-about-audience.html"><B> Target Audience </B></A>

       <A class=menu2 id=submenurow1s9 
            onmouseover="javascript:hypackShowSubMenuDelay('1', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('1', 1)" 
            href="./../hypack13-about-benefits.html"><B> Benefits</B></A>

	<A class=menu2 id=submenurow1s10 
            onmouseover="javascript:hypackShowSubMenuDelay('1', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('1', 1)" 
            href="./../hypack13-about-organisers.html"><B>  Organisers </B> </A>

        <A class=menu2 id=submenurow1s11 
            onmouseover="javascript:hypackShowSubMenuDelay('1', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('1', 1)" 
            href="./../hypack13-about-accommodation.html"><B>  Accommodation </B></A>

        <A class=menu2 id=submenurow1s12 
            onmouseover="javascript:hypackShowSubMenuDelay('1', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('1', 1)" 
            href="./../hypack13-about-local-travel.html"><B> Local Travel</B></A>

	<A class=menu2 id=submenurow1s13 
            onmouseover="javascript:hypackShowSubMenuDelay('1', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('1', 1)" 
            href="./../hypack13-about-sponsors.html"><B>  Sponsors </B></A>

        <A class=menu2 id=submenurow1s14 
            onmouseover="javascript:hypackShowSubMenuDelay('1', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('1', 1)" 
            href="./../hypack13-about-feedback.html"><B>  Feedback </B></A>

        <A class=menu2 id=submenurow1s15 
            onmouseover="javascript:hypackShowSubMenuDelay('1', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('1', 1)" 
            href="./../hypack13-about-acknowledgements.html"><B>  Acknowledgements </B> </A>

        <A class=menu2 id=submenurow1s16 
            onmouseover="javascript:hypackShowSubMenuDelay('1', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('1', 1)" 
            href="./../hypack13-about-contact-address.html"><B>  Contact </B> </A>

        <A class=menu2 id=submenurow1s17 
            onmouseover="javascript:hypackShowSubMenuDelay('1', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('1', 1)" 
            href="./../index.html"><B>   Home  </B> </A>

 
      </TD></TR></TBODY></TABLE>

<!--   Sub menu for **** Row-1-About **** End  here --> 



<!--   Sub menu for **** Row-2-Topics of interest ****  start  here --> 

      <TABLE id=submenutab2 onMouseOver="javascript:hypackShowSubMenuDelay('2',1)" 
      style="VISIBILITY: hidden; MARGIN-LEFT: 60px; POSITION: absolute" 
      onmouseout="javascript:hypackHideSubMenuDelay('2',1)" cellSpacing=0 
      cellPadding=0 width=150>
        <TBODY>
        <TR>
          <TD class=menu2>

          <A class=menu2 id=submenurow2s1 
            onmouseover="javascript:hypackShowSubMenuDelay('2', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('2', 1)" 
            href="./../hypack13-tech-prog-topics-overview.html"><B>  Topics of Interest </B></A>

          <A class=menu2 id=submenurow2s2 
            onmouseover="javascript:hypackShowSubMenuDelay('2', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('2', 1)" 
            href="./../hypack13-tech-prog-schedule.html"><B> Tech. Prog. Schedule</B></A>
   
	 <A class=menu2 id=submenurow2s3 
            onmouseover="javascript:hypackShowSubMenuDelay('2', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('2', 1)" 
            href="./../hypack13-topics-mode01-multicore.html"><B>  Topic : Multi-Core </B></A>

         <A class=menu2 id=submenurow2s4 
            onmouseover="javascript:hypackShowSubMenuDelay('2', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('2', 1)" 
            href="./../hypack13-topics-mode02-arm-proc.html"><B>  Topic : ARM Proc. </B></A>

         <A class=menu2 id=submenurow2s5 
            onmouseover="javascript:hypackShowSubMenuDelay('2', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('2', 1)" 
            href="./../hypack13-topics-mode03-coprocessor.html"><B>  Topic : Coprocessors </B></A>

         <A class=menu2 id=submenurow2s6 
            onmouseover="javascript:hypackShowSubMenuDelay('2', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('2', 1)" 
            href="./../hypack13-topics-mode04-gpgpu.html"><B>  Topic : GPGPUs </B></A>

         <A class=menu2 id=submenurow2s7 
            onmouseover="javascript:hypackShowSubMenuDelay('2', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('2', 1)" 
            href="./../hypack13-topics-mode05-hpc-cluster.html"><B>  Topic : HPC  Cluster</B></A>

         <A class=menu2 id=submenurow2s8 
            onmouseover="javascript:hypackShowSubMenuDelay('2', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('2', 1)" 
            href="./../hypack13-topics-mode06-app-kernels.html"><B>  Topic : App. Kernels.</B></A>
                    
         <A class=menu2 id=submenurow2s9
            onmouseover="javascript:hypackShowSubMenuDelay('2', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('2', 1)" 
            href="./../hypack13-topics-laboratory.html"><B>  Topic : Lab. Session</B></A>

         <A class=menu2 id=submenurow2s10 
            onmouseover="javascript:hypackShowSubMenuDelay('2', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('2', 1)" 
            href="./../hypack13-topics-keynote-invited-talks.html"><B> Key-Note / Invited Talks</B> </A>

           
         <A class=menu2 id=submenurow2s11 
            onmouseover="javascript:hypackShowSubMenuDelay('2', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('2', 1)" 
            href="./../index.html"><B>    Home  </B> </A>
 
	
      </TD></TR></TBODY></TABLE>

<!--   Sub menu for **** Row-2-Topics of interest ***** End  here -->



<!--   Sub menu for **** Row-3 : Mode 1 (Multi-Cores): Hands-on ****  start here  -->

      <TABLE id=submenutab3 onMouseOver="javascript:hypackShowSubMenuDelay('3',1)" 
      style="VISIBILITY: hidden; MARGIN-LEFT: 200px; POSITION: absolute" 
      onmouseout="javascript:hypackHideSubMenuDelay('3',1)" cellSpacing=0 
      cellPadding=0 width=150>

        <TBODY>
        <TR>
          <TD class=menu2>
        <A class=menu2 id=submenurow3s1 
            onmouseover="javascript:hypackShowSubMenuDelay('3', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('3', 1)" 
            href="./../hypack13-mode01-multicore-lab-overview.html"><B>   Mode-1 Multi-Core </B></A>

        <A class=menu2 id=submenurow3s2 
            onmouseover="javascript:hypackShowSubMenuDelay('3', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('3', 1)" 
            href="./../hypack13-mode01-multicore-memory-allocators.html"><B> Memory Allocators</B></A>

        <A class=menu2 id=submenurow3s3
            onmouseover="javascript:hypackShowSubMenuDelay('3', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('3', 1)" 
            href="./../hypack13-mode01-multicore-openmp.html"><B>OpenMP  </B></A>

        <A class=menu2 id=submenurow3s4 
            onmouseover="javascript:hypackShowSubMenuDelay('3', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('3', 1)" 
            href="./../hypack13-mode01-multicore-intel-tbb.html"><B> Intel TBB </B></A>

        <A class=menu2 id=submenurow3s5 
            onmouseover="javascript:hypackShowSubMenuDelay('3', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('3', 1)" 
            href="./../hypack13-mode01-multicore-pthreads.html"><B>  Pthreads  </B></A>
	
       <A class=menu2 id=submenurow3s6 
            onmouseover="javascript:hypackShowSubMenuDelay('3', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('3', 1)" 
            href="./../hypack13-mode01-multicore-java-threads.html"><B> Java - Threads  </B></A>

       <A class=menu2 id=submenurow3s7 
            onmouseover="javascript:hypackShowSubMenuDelay('3', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('3', 1)" 
            href="./../hypack13-mode01-multicore-charmplusplus.html"><B> Charm++ Prog. </B></A>

        <A class=menu2 id=submenurow3s8
            onmouseover="javascript:hypackShowSubMenuDelay('3', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('3', 1)" 
            href="./../hypack13-mode01-multicore-mpi.html"><B> Message Passing (MPI) </B></A>
 
        <A class=menu2 id=submenurow3s9
            onmouseover="javascript:hypackShowSubMenuDelay('3', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('3', 1)" 
            href="./../hypack13-mode01-multicore-mpi-openmp.html"><B>  MPI - OpenMP</B></A>
  
       <A class=menu2 id=submenurow3s10
            onmouseover="javascript:hypackShowSubMenuDelay('3', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('3', 1)" 
            href="./../hypack13-mode01-multicore-mpi-tbb.html"><B>  MPI - Intel TBB </B></A>
 
       <A class=menu2 id=submenurow3s11
            onmouseover="javascript:hypackShowSubMenuDelay('3', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('3', 1)" 
            href="./../hypack13-mode01-multicore-mpi-pthreads.html"><B>  MPI - Pthreads </B></A>

        <A class=menu2 id=submenurow3s12 
            onmouseover="javascript:hypackShowSubMenuDelay('3', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('3', 1)" 
            href="./../hypack13-mode01-multicore-compiler-tune-perf.html"><B> Compilers - Opt. Features </B></A>

        <A class=menu2 id=submenurow3s13 
            onmouseover="javascript:hypackShowSubMenuDelay('3', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('3', 1)" 
            href="./../hypack13-mode01-multicore-perf-math-lib.html"><B> Threads-Perf. Math. Lib.</B></A>

        <A class=menu2 id=submenurow3s14 
            onmouseover="javascript:hypackShowSubMenuDelay('3', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('3', 1)" 
            href="./../hypack13-mode01-multicore-software-tools.html"><B>  Threads-Prof. &amp; Tools</B></A>

       <A class=menu2 id=submenurow3s15 
            onmouseover="javascript:hypackShowSubMenuDelay('3', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('3', 1)" 
            href="./../hypack13-mode01-multicore-threads-io-perf.html"><B>Threads - I/O Perf. </B></A>
  
        <A class=menu2 id=submenurow3s16 
            onmouseover="javascript:hypackShowSubMenuDelay('3', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('3', 1)" 
            href="./../hypack13-mode01-multicore-pgas-langlib.html"><B> PGAS : UPC / CAF/ GA</B></A>

        <A class=menu2 id=submenurow3s17
            onmouseover="javascript:hypackShowSubMenuDelay('3', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('3', 1)" 
            href="./../hypack13-mode01-multicore-power-perf.html"><B> Power &amp; Perf.  </B></A>

       <A class=menu2 id=submenurow3s18 
            onmouseover="javascript:hypackShowSubMenuDelay('3', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('3', 1)" 
            href="./../index.html"><B>  Home  </B> </A>

      </TD></TR></TBODY></TABLE>

<!--   Sub menu **** Row-3 : Mode 1 (Multi-Cores ) : Hands-on ****  End here  -->



<!--   Sub menu for **** Row-4 : Mode 2 (ARM Processor) Hands-on ****  start here  -->

      <TABLE id=submenutab4 onMouseOver="javascript:hypackShowSubMenuDelay('4',1)" 
      style="VISIBILITY: hidden; MARGIN-LEFT: 200px; POSITION: absolute" 
      onmouseout="javascript:hypackHideSubMenuDelay('4',1)" cellSpacing=0 
      cellPadding=0 width=150>

        <TBODY>
        <TR>
          <TD class=menu2>
        <A class=menu2 id=submenurow4s1 
            onmouseover="javascript:hypackShowSubMenuDelay('4', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('4', 1)" 
            href="./../hypack13-mode02-arm-proc-lab-overview.html"><B>   Mode-2 ARM  </B></A>

        <A class=menu2 id=submenurow4s2 
            onmouseover="javascript:hypackShowSubMenuDelay('4', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('4', 1)" 
            href="./../hypack13-mode02-arm-proc-prog-env.html"><B> Prog. Env </B></A>
      
       <A class=menu2 id=submenurow4s3 
            onmouseover="javascript:hypackShowSubMenuDelay('4', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('4', 1)" 
            href="./../hypack13-mode02-arm-proc-benchmarks.html"><B> Benchmarks</B></A>

       <A class=menu2 id=submenurow4s4
            onmouseover="javascript:hypackShowSubMenuDelay('4', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('4', 1)" 
            href="./../hypack13-mode02-arm-proc-power-perf.html"><B> Power &amp; Perf.  </B></A>

       <A class=menu2 id=submenurow4s5 
            onmouseover="javascript:hypackShowSubMenuDelay('4', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('4', 1)" 
            href="./../index.html"><B>  Home  </B> </A>

      </TD></TR></TBODY></TABLE>


<!--   Sub menu **** Row-4 : Mode 2 (ARM Processor) : Hands-on ****  End here  -->




<!--   Sub menu for **** Row-5 : Mode 3 (Coprocessor) Hands-on ****  start here  -->

      <TABLE id=submenutab5 onMouseOver="javascript:hypackShowSubMenuDelay('5',1)" 
      style="VISIBILITY: hidden; MARGIN-LEFT: 200px; POSITION: absolute" 
      onmouseout="javascript:hypackHideSubMenuDelay('5',1)" cellSpacing=0 
      cellPadding=0 width=150>

        <TBODY>
        <TR>
          <TD class=menu2>
        <A class=menu2 id=submenurow5s1 
            onmouseover="javascript:hypackShowSubMenuDelay('5', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('5', 1)" 
            href="./../hypack13-mode03-coprocessor-lab-overview.html"><B>   Mode-3 Coprocessors </B></A>

        <A class=menu2 id=submenurow5s2 
            onmouseover="javascript:hypackShowSubMenuDelay('5', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('5', 1)" 
            href="./../hypack13-mode03-coprocessor-arch-software.html"><B> Arch. Software </B></A>

        <A class=menu2 id=submenurow5s3 
            onmouseover="javascript:hypackShowSubMenuDelay('5', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('5', 1)" 
            href="./../hypack13-mode03-coprocessor-compiler-vect.html"><B> Compiler &amp; Vect. </B></A>
     
        <A class=menu2 id=submenurow5s4 
            onmouseover="javascript:hypackShowSubMenuDelay('5', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('5', 1)" 
            href="./../hypack13-mode03-coprocessor-prog-env.html"><B> Prog. Env. </B></A>

        <A class=menu2 id=submenurow5s5 
            onmouseover="javascript:hypackShowSubMenuDelay('5', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('5', 1)" 
            href="./../hypack13-mode03-coprocessor-benchmarks.html"><B> Benchmarks</B></A>

        <A class=menu2 id=submenurow5s6
            onmouseover="javascript:hypackShowSubMenuDelay('5', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('5', 1)" 
            href="./../hypack13-mode03-coprocessor-power-perf.html"><B> Power &amp; Perf.  </B></A>

        <A class=menu2 id=submenurow5s7 
            onmouseover="javascript:hypackShowSubMenuDelay('5', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('5', 1)" 
            href="./../index.html"><B>  Home  </B> </A>

      </TD></TR></TBODY></TABLE>


<!--   Sub menu **** Row-5 : Mode 3 (Coprocessor) : Hands-on ****  End here  -->



<!--   Sub menu for **** Row-6 : Mode 4 (GPGPUs): Hands-on **** Start here  -->

      <TABLE id=submenutab6 onMouseOver="javascript:hypackShowSubMenuDelay('6',1)" 
      style="VISIBILITY: hidden; MARGIN-LEFT: 285px; POSITION: absolute" 
      onmouseout="javascript:hypackHideSubMenuDelay('6',1)" cellSpacing=0 
      cellPadding=0 width=150>
        <TBODY>
        <TR>
          <TD class=menu2>

	<A class=menu2 id=submenurow6s1 
            onmouseover="javascript:hypackShowSubMenuDelay('6', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('6', 1)" 
            href="./../hypack13-mode04-gpgpu-lab-overview.html"><B> Mode-4 GPGPUs  </B></A>

	<A class=menu2 id=submenurow6s2 
            onmouseover="javascript:hypackShowSubMenuDelay('6', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('6', 1)" 
            href="./../hypack13-mode04-gpgpu-nvidia-gpu-cuda.html"><B>NVIDIA - CUDA/OpenCL </B></A>


	<A class=menu2 id=submenurow6s3 
            onmouseover="javascript:hypackShowSubMenuDelay('6', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('6', 1)" 
            href="./../hypack13-mode04-gpgpu-amd-opencl.html"><B>AMD  APP - OpenCL</B></A>


	<A class=menu2 id=submenurow6s4 
            onmouseover="javascript:hypackShowSubMenuDelay('6', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('6', 1)" 
            href="./../hypack13-mode04-gpgpu-opencl.html"><B> GPGPUs - OpenCL </B></A>

        <A class=menu2 id=submenurow6s5 
            onmouseover="javascript:hypackShowSubMenuDelay('6', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('6', 1)" 
            href="./../hypack13-mode04-gpgpu-power-perf.html"><B> GPGPUs : Power &amp; Perf. </B></A>

        <A class=menu2 id=submenurow6s6 
            onmouseover="javascript:hypackShowSubMenuDelay('6', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('6', 1)" 
            href="./../index.html"><B>   Home  </B> </A>


	   </TD></TR> </TBODY></TABLE>

<!--  Sub menu for **** Row-6 : Mode-4 (GPGPUs) : Hands-on **** End here  -->



<!--   Sub menu for **** Row-7 : Mode-5 (HPC GPU Cluster): Hands-on  **** start  here -->

      <TABLE id=submenutab7 onMouseOver="javascript:hypackShowSubMenuDelay('7',1)" 
      style="VISIBILITY: hidden; MARGIN-LEFT: 365px; POSITION: absolute" 
      onmouseout="javascript:hypackHideSubMenuDelay('7',1)" cellSpacing=0 
      cellPadding=0 width=150>


        <TBODY>
        <TR>
          <TD class=menu2>

        <A class=menu2 id=submenurow7s1 
            onmouseover="javascript:hypackShowSubMenuDelay('7', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('7', 1)" 
            href="./../hypack13-mode05-hpc-cluster-lab-overview.html"><B>  Mode-5  HPC Cluster </B></A>
        
        <A class=menu2 id=submenurow7s2 
            onmouseover="javascript:hypackShowSubMenuDelay('7', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('7', 1)" 
            href="./../hypack13-mode05-hpc-message-passing-cluster.html"><B> HPC  MPI   Cluster   </B></A>

	<A class=menu2 id=submenurow7s3
            onmouseover="javascript:hypackShowSubMenuDelay('7', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('7', 1)" 
            href="./../hypack13-mode05-hpc-gpu-cluster-nvidia-cuda.html"><B> GPU Cluster - NVIDIA   </B></A>

        <A class=menu2 id=submenurow7s4 
            onmouseover="javascript:hypackShowSubMenuDelay('7', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('7', 1)" 
            href="./../hypack13-mode05-hpc-gpu-cluster-amd-opencl.html"><B>GPU Cluster - AMD APP </B></A>


        <A class=menu2 id=submenurow7s5 
            onmouseover="javascript:hypackShowSubMenuDelay('7', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('7', 1)" 
            href="./../hypack13-mode05-hpc-intel-coprocessor-cluster.html"><B> Cluster - Intel Coprocessors </B></A>

        <A class=menu2 id=submenurow7s6 
            onmouseover="javascript:hypackShowSubMenuDelay('7', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('7', 1)" 
            href="./../hypack13-mode05-hpc-cluster-power-perf.html"><B>  Cluster- Power &amp; Perf.  </B> </A>

        <A class=menu2 id=submenurow7s7 
            onmouseover="javascript:hypackShowSubMenuDelay('7', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('7', 1)" 
            href="./../index.html"><B>  Home  </B> </A>
      
	</TD></TR>
        </TBODY>
        </TABLE>


<!--   Sub menu for **** Row-7 : MODe-5 : (HPC GPU Cluster) Hands-on ****  End  here -->



<!--   Sub menu for **** Row-8 :Mode-6 Application  program  **** start here -->


      <TABLE id=submenutab8 onMouseOver="javascript:hypackShowSubMenuDelay('8',1)" 
      style="VISIBILITY: hidden; MARGIN-LEFT: 510px; POSITION: absolute" 
      onmouseout="javascript:hypackHideSubMenuDelay('8',1)" cellSpacing=0 
      cellPadding=0 width=150>
        <TBODY>
        <TR>
          <TD class=menu2>
 
	 <A class=menu2 id=submenurow8s1 
            onmouseover="javascript:hypackShowSubMenuDelay('8', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('8', 1)" 
            href="./../hypack13-mode06-app-kernels-lab-overview.html"><B> Mode-6 App. Kernels </B></A>
                  
	<A class=menu2 id=submenurow8s2 
            onmouseover="javascript:hypackShowSubMenuDelay('8', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('8', 1)" 
            href="./../hypack13-mode06-pdesolvers-fdm-fem.html"><B>  PDE Solvers : FDM/FEM  </B></A>

        <A class=menu2 id=submenurow8s3 
            onmouseover="javascript:hypackShowSubMenuDelay('8', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('8', 1)" 
            href="./../hypack13-mode06-image-processing-fft.html"><B>  Image Processing - FFT </B></A>    

         <A class=menu2 id=submenurow8s4
            onmouseover="javascript:hypackShowSubMenuDelay('8', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('8', 1)" 
            href="./../hypack13-mode06-phys-monte-carlo.html"><B> Monte Carlo Methods </B> </A>

     	 <A class=menu2 id=submenurow8s5 
            onmouseover="javascript:hypackShowSubMenuDelay('8', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('8', 1)" 
            href="./../hypack13-mode06-string-srch.html"><B>  String Srch. </B></A>
    

     	 <A class=menu2 id=submenurow8s6 
            onmouseover="javascript:hypackShowSubMenuDelay('8', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('8', 1)" 
            href="./../hypack13-mode06-seq-analysis.html"><B>  Seq. Analy.</B></A>
       
         <A class=menu2 id=submenurow8s7
            onmouseover="javascript:hypackShowSubMenuDelay('8', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('8', 1)" 
            href="./../hypack13-mode06-video-processing.html"><B> Video Process. </B> </A>

        <A class=menu2 id=submenurow8s8 
            onmouseover="javascript:hypackShowSubMenuDelay('8', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('8', 1)" 
            href="./../hypack13-mode06-intrusion-detection-sys.html"><B>  Intr. Detcn. Sys  </B> </A>

        <A class=menu2 id=submenurow8s9 
            onmouseover="javascript:hypackShowSubMenuDelay('8', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('8', 1)" 
            href="./../hypack13-mode06-app-kernels-power-perf.html"><B>  App. Power &amp; Perf.  </B> </A>

        <A class=menu2 id=submenurow8s10 
            onmouseover="javascript:hypackShowSubMenuDelay('8', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('8', 1)" 
            href="./../index.html"><B>   Home  </B> </A>


       </TD></TR></TBODY></TABLE>

<!--   Sub menu for **** Row-8 :Mode-6 Application Program **** End here -->


<!--  Sub menu for **** Row-9-Registration **** Start here  -->

      <TABLE id=submenutab9 onMouseOver="javascript:hypackShowSubMenuDelay('9',1)" 
      style="VISIBILITY: hidden; MARGIN-LEFT: 610px; POSITION: absolute" 
      onmouseout="javascript:hypackHideSubMenuDelay('9',1)" cellSpacing=0 
      cellPadding=0 width=150>

      <TBODY>
      <TR>
      <TD class=menu2>

      <A class=menu2 id=submenurow9s1 
            onmouseover="javascript:hypackShowSubMenuDelay('9', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('9', 1)" 
            href="./../hypack13-reg-overview.html"><B> Reg. Overview</B></A>
           
      <A class=menu2 id=submenurow9s2 
            onmouseover="javascript:hypackShowSubMenuDelay('9', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('9', 1)" 
            href="./../hypack13-reg-private-sector.html"><B>Pvt. Sector</B></A>

      <A class=menu2 id=submenurow9s3 
            onmouseover="javascript:hypackShowSubMenuDelay('9', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('9', 1)" 
            href="./../hypack13-reg-govt-public-sector.html"><B>Pub. Sector</B></A>

      <A class=menu2 id=submenurow9s4 
            onmouseover="javascript:hypackShowSubMenuDelay('9', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('9', 1)" 
            href="./../hypack13-reg-govt-academic-staff.html"><B>Govt. Acad. Staff </B></A>

      <A class=menu2 id=submenurow9s5 
            onmouseover="javascript:hypackShowSubMenuDelay('9', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('9', 1)" 
            href="./../hypack13-reg-students.html"><B>Students Reg. </B></A>

      <A class=menu2 id=submenurow9s6
            onmouseover="javascript:hypackShowSubMenuDelay('9', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('9', 1)" 
            href="./../hypack13-reg-online-registration.html"><B>On-line Reg.</B></A>
 
      <A class=menu2 id=submenurow9s7 
            onmouseover="javascript:hypackShowSubMenuDelay('9', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('9', 1)" 
             href="./../hypack13-reg-accommodation.html"><B>Accommodation </B></A>

      <A class=menu2 id=submenurow9s8 
            onmouseover="javascript:hypackShowSubMenuDelay('9', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('9', 1)" 
            href="./../hypack13-reg-contact-address.html"><B>Contact</B></A>

       <A class=menu2 id=submenurow9s9 
            onmouseover="javascript:hypackShowSubMenuDelay('9', 1)" 
            onmouseout="javascript:hypackHideSubMenuDelay('9', 1)" 
            href="./../index.html"><B>  Home  </B> </A>

      </TD></TR></TBODY></TABLE>

  <!--   Sub menu for  **** Row-9-Registration **** End here  -->


  </DIV>

<!--  *** left section code for about link start here ***  -->

   
     <INPUT id=menuval 
      type=hidden name=menuval> <INPUT id=menuval2 type=hidden name=menuval2> 
      <TABLE class=mainctnt cellSpacing=0 cellPadding=0 border=0>
        <TBODY>
        <TR>
          <TD class=mainctntcell>
            <TABLE cellSpacing=0 cellPadding=0 border=0>
              <TBODY>
              <TR>
                <TD class=leftmenu><BR>
                  
              
           <A class=menul
	      href="./../hypack13-mode01-multicore-lab-overview.html">
              &#149; Mode-1 Multi-Core  </A>
                       
	   <A class=menul
	      href="./../hypack13-mode01-multicore-memory-allocators.html">
              &#149;  Memory Allocators</A>
   		
	   <A class=menul
              href="./../hypack13-mode01-multicore-openmp.html">
              &#149; OpenMP</A>

	   <A class=menul  
              href="./../hypack13-mode01-multicore-intel-tbb.html">
              &#149; Intel TBB </A>

           <A class=menul
              href="./../hypack13-mode01-multicore-pthreads.html">
              &#149; Pthreads</A>
              
           <A class=menul
	      href="./../hypack13-mode01-multicore-java-threads.html">
              &#149; Java - Threads</A>
             
          <A class=menul
	      href="./../hypack13-mode01-multicore-charmplusplus.html">
              &#149; Charm++ Prog.</A>
           
              <!-- ** -->
	      <A class=menulslct
               href="./../hypack13-mode01-multicore-mpi.html">
               &#149; Message Passing (MPI)</A>
              <!-- ** -->
              
           <A class=menul 
	      href="./../hypack13-mode01-multicore-mpi-openmp.html">
              &#149; MPI - OpenMP</A>

           <A class=menul 
	      href="./../hypack13-mode01-multicore-mpi-tbb.html">
              &#149; MPI - Intel TBB</A>  
        
           <A class=menul 
	      href="./../hypack13-mode01-multicore-mpi-pthreads.html">
              &#149; MPI - Pthreads</A> 

	   <A class=menul
	      href="./../hypack13-mode01-multicore-compiler-tune-perf.html">
              &#149; Compiler Opt. Features</A>
             
	   <A class=menul 
	      href="./../hypack13-mode01-multicore-perf-math-lib.html">
              &#149; Threads-Perf. Math.Lib. </A>

           <A class=menul
	      href="./../hypack13-mode01-multicore-software-tools.html">
              &#149; Threads-Prof. &amp; Tools </A>
  
           <A class=menul
	      href="./../hypack13-mode01-multicore-threads-io-perf.html">
              &#149; Threads-I/O Perf. </A>

           <A class=menul
	      href="./../hypack13-mode01-multicore-pgas-langlib.html">
              &#149; PGAS : UPC / CAF / GA </A>

           <A class=menul
	      href="./../hypack13-mode01-multicore-power-perf.html">
              &#149; Power-Perf.   </A>
 
          <A class=menul 
	      href="./../index.html">
              &#149; Home</A>


<!-- ****************** left section code for about link End  here ************* -->
	
	<BR>
         <DIV style="BACKGROUND: url(images/) no-repeat 100% 100%; WIDTH: auto; HEIGHT:300px">        </DIV><BR><BR><BR><BR>

      </TD>
             
 <TD class=rightctnt> 

<!--  content of web page start here  --> 



<TABLE cellSpacing=0 cellPadding=0 border=0 >
<TBODY>

<TR>
<H1> hyPACK-2013 : Parallel Prog. Using MPI-2.0 </H1> 


<TABLE cellPadding=3  width = "100%" border=0> 
<TBODY>
<TR> 

<table  border="0"  height="28">
<tbody>



<!-- *************** MP Overview  start here *************** -->


<TR>
<TD>
<p align = "justify"> <span class ="content"> 


 MPI (Message Passing Interface) is a standard specification for message
 passing libraries. MPI makes it relatively easy to write portable parallel programs. MPI does provide 
message-passing routines for exchanging all the information needed to allow a single MPI implementation 
to operate in a heterogeneous environment. 


The MPI-2 has new areas for message-passing model such as  parallel I/O, remote memory operations, and
 dynamic process management. In addition, MPI-2 introduces a number of features designed to make all 
of MPI more robust and convenient to use, such as external interface specifications, C++ and fortran-90 
bindings, support for threads, and mixed-language programming.  


 MPI 3.0 Standardization efforts and research work on hybrid programming (treating threads as MPI Processes,
 Dynamic thread levels) is going on. The current multi- and future many-core processors require extended
 MPI facilities for dealing with threads. The efforts on point-to-point and collective communications 
will be further tuned on multi-core and many-core processors. 

Examples on two types of commonly used MPI programming Paradigms such as SPMD (Single Program Multiple 
Data) and MPMD 
(Multiple Program Multiple Data) have been discussed. Some of the examples are based on numerical 
and non-numerical 
Computations. <span> </p> 


</TD></TR>
</TBODY> </TABLE> 

<!-- *************** Overview  Ends here *************** -->



<!-- *************** Link Section start here *************** -->

<table  border="0"  height="28">
<tbody>
  <tr>
    <td height="24" align="left" >
         
<BR> 

<font size="2" face="Arial" color="red"> <B>  MPI 1.X   : </b>  </font> </a> 
 &nbsp;  &nbsp; &nbsp; 


<a href="#mpi-intro">
<font size="2" face="Arial" color="blue"> <B>  Introduction  </b>  </font> </a> 
 &nbsp; &nbsp; 

<a href="#mpi-1.x">
<font size="2" face="Arial" color="blue"> <B> MPI 1.1  </b>  </font> </a> 
 &nbsp; &nbsp; 


<a href="#mpi-1.x-c-fortran-lib-calls">
<font size="2" face="Arial" color="blue"> <B> MPI 1.X (C-lang. &amp; fortran) Lib. Calls  </b>  </font> </a> 
 <BR> <BR>

<font size="2" face="Arial" color="red"> <B>  MPI 2.X  : </b>  </font> </a> 
 &nbsp;  &nbsp; &nbsp; 

<a href="#mpi-2.X-Overview">
<font size="2" face="Arial" color="blue"> <B>  MPI 2.X Overview  </b>  </font> </a> 
 &nbsp; &nbsp;
 
<a href="#mpi-2.x-c-fortran-lib-calls">
<font size="2" face="Arial" color="blue"> <B> MPI 2.X (C-lang. &amp; fortran) Lib. Calls  </b>  </font> </a> 

<BR> <BR>

<font size="2" face="Arial" color="red"> <B>  MPI C++  : </b>  </font> </a> 
 &nbsp; &nbsp; 

<a href="#mpi-cpp-overview">
<font size="2" face="Arial" color="blue"> <B>  MPI C++  Overview  </b>  </font> </a> 
 &nbsp; &nbsp;

<a href="#mpi-cpp-lib-calls">
<font size="2" face="Arial" color="blue"> <B> MPI C++ Lib. Calls  </b>  </font> </a> 

<BR> <BR>



  
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
<a href="#MPI-Performance">
<font size="2" face="Arial" color="blue"> <B> MPI Performance Tools </b>  </font> </a> 
 &nbsp; &nbsp; &nbsp;



<a href="#MPI-compile-execute">
<font size="2" face="Arial" color="blue"> <B> Compilation &amp; Execution of MPI 1.X codes </b>  </font> </a>  
 &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; 


 <BR> <BR>



 <font size="2" face="Arial" color="#FF0011">  
 <B>  MPI 2.X - Example Program : &nbsp; &nbsp;  </font>
   
    <a href="#mpi-2.X-example-c-lang"> <font size="2" face="Arial" color="blue"> MPI 2.X -C </a>  &nbsp; &nbsp; &nbsp; </font> 
        
   <a href="#mpi-2.X-example-f77"> <font size="2" face="Arial" color="blue"> MPI 2.X-fortran  </a>  &nbsp; &nbsp; &nbsp; </font><BR><BR>


<BR> <BR>

<!-- ************************ Reference Starts Here ************************** -->

<font size="2" face="Arial" color="red"><B> References  : </b>  </font> </a> 

<a href="../reference-hypack-2013/reference-overview-hypack13-mode01-multicore.html#multicore-prog-ref"> 
 <font size="2" face="Arial" color="blue"><B> Multi-threading </b>  </font> </a> 
 &nbsp; &nbsp; 

<a href="../reference-hypack-2013/reference-overview-hypack13-mode01-multicore.html#multicore-prog-openmp-ref"> 
 <font size="2" face="Arial" color="blue"><B> OpenMP</b>  </font> </a> 
 &nbsp; &nbsp;  

<a href="../reference-hypack-2013/reference-overview-hypack13-mode01-multicore.html#mcp-ref-java-threads"> 
 <font size="2" face="Arial" color="blue"><B> Java Threads</b>  </font> </a> 
 &nbsp; &nbsp; 

<a href="../reference-hypack-2013/reference-overview-hypack13-mode01-multicore.html#multicore-parcomp-prog-ref"> 
 <font size="2" face="Arial" color="blue"><B> Books</b>  </font> </a>
  &nbsp; &nbsp; 

<a href="../reference-hypack-2013/reference-overview-hypack13-mode01-multicore.html#multicore-prog-mpi-ref"> 
 <font size="2" face="Arial" color="blue"><B> MPI </b>  </font> </a>
  &nbsp;  

<a href="../reference-hypack-2013/reference-overview-hypack13-mode01-multicore.html#mc-benchmarks-ref"> 
 <font size="2" face="Arial" color="blue"><B> Benchmarks</b>  </font> </a>
  &nbsp;  
<!-- ************* Reference Ends Here ************************ -->




<HR>

<!-- ***************** List of MPI 2.XProgram Starts ********************** -->

 

<!-- *************** Table for Listing of Programs Starts  **************** -->

<TABLE width = 100% cellPadding=3  border=0> 
<TBODY>

<BR> 

   <!-- ************ Example 1.1 ******************* -->
    <tr>
     <td width="90" height="2" valign="top"> 
       <a href="./mpi-2x-html/mpi-2x-programs.html#mpi-2x-par-prog-id11">
       <font size="2" face="Arial" color="blue">
            <b>  Example 1.1 <BR> </b> </font> <BR>  
       </a>  
      </td>
   
      <td width="500" height="2" valign="top">
          <p> <span class ="content">
          MPI program for calculating sum of first <i> n </i>  integers using Remote Memory Access calls (Memory Windows) 
    </span> </p>
     </td>
     </tr>

 <!-- ********************** Example 1.2 ******************* -->
    <tr>
     <td width="90" height="2" valign="top"> 
      <a href="./mpi-2x-html/mpi-2x-programs.html#mpi-2x-par-prog-id12">
       <font size="2" face="Arial" color="blue">
            <b>  Example 1.2  </b> </font> <BR>  
       </a>  
      </td>
   
      <td width="500" height="2" valign="top">
           <p> <span class ="content">
         MPI program for writing <I> n </i>  files using Parallel I/O 

       </span> </p>
     </td>

   </tr>


   <!-- ****************** Example 1.3  ******************** -->
    <tr>
     <td width="90" height="2" valign="top"> 
    <a href="./mpi-2x-html/mpi-2x-programs.html#mpi-2x-par-prog-id13">
         <font size="2" face="Arial" color="blue">
            <b>  Example 1.3 <BR> </b> </font> <BR>  
       </a>  
      </td>
   
      <td width="600" height="2" valign="top">
          <p> <span class ="content">
         Write a  MPI program to create the process dynamically (dynamic process management ( <B> Assignment </b>)
        </span> </p>
     </td>
     </tr>

 <!-- ******************** Example 1.4 ********************** -->
    <tr>
     <td width="90" height="2" valign="top"> 
     <a href="./mpi-2x-html/mpi-2x-programs.html#mpi-2x-par-prog-id14">
         <font size="2" face="Arial" color="blue">
            <b>  Example 1.4 <BR> </b> </font> <BR>  
       </a>  
      </td>
   
      <td width="600" height="2" align="left">
      <p> <span class ="content">
    MPI program for computation of <i> pie  </i> value by Numerical Intgration using Remote Memory Access (RMA)   

         </span> </p>
     </td>

   </tr>


   <!-- ******************** Example 1.5 ******************** -->
    <tr>
     <td width="90" height="2" valign="top"> 
    <a href="./mpi-2x-html/mpi-2x-programs.html#mpi-2x-par-prog-id15">
         <font size="2" face="Arial" color="blue">
            <b>  Example 1.5 <BR> </b> </font> <BR>  
       </a>  
      </td>
   
      <td width="500" height="2" valign="top">
       <p> <span class ="content">
          Write a MPI program to compute the Matrix into Vector Multiplication 
     Vector Multiplication using 
 Self-Scheduling Algorithm & MPI 2  Dynamic Process Management (<B> Assignment </b> )

       </span> </p>
     </td>
     </tr>


<tr>
<td> </td>
<td>

  <p align="right"><a href="mpi-2x-overview.html">
<IMG src="./mpi-2x-images/up.gif" border="0" width="13" height="13"></a></P>


</td>
</tr>

</TBODY> 
</TABLE> 




<!-- **************** About MPI Starts ********************** -->


<a name="mpi-intro"> </a>

 <TABLE cellPadding=3  border=0> 
 <TBODY> 
 
<!-- ..............


 <TR>
 <TD>  

   <DIV align=Left><font size="2"  color = "red" face = "Arial"> 
     <p  align="justify"><b><font face="Verdana" color="red">
             An overview of MPI   </b>  </font> </p>
     </DIV> 
   <BR> 
  </TD> 
  </TR> 

..............-->

 <tr>
  <TD bgColor = #cccdd77889>  

   <DIV align=Left> 
    <p  align="justify">
      <b><font size="2" face="Verdana" color="black"> MPI  - Introduction   </b> </p>
    </font> 
   </DIV> 
  </TD> </tr>  


<TR>
<TD>

<BR>
  <P align=justify><span class="content">
<font color="black"><b> Introduction to MPI : </font> </B>
  
		
   A proposed standard <I>Message Passing Interface</I> (<B> MPI</B>) is originally designed for writing applications and 
 libraries for distributed memory environments. The main advantages of establishing a message-passing interface for such environments 
 are portability and ease of use, and a standard memory-passing interface is a key component in building a concurrent computing 
 environment in which applications, software libraries, and tools can be transparently ported between different machines.<BR> <BR>


<B>MPI</B> is intended to be a standard message-passing interface for  applications and libraries running on concurrent 
 computers with logically distributed memory. <B>MPI</B> is not specifically designed for use by parallelizing compilers.
 <B>MPI</B> provides no explicit support for multithreading, since the design goals of <B>MPI</B> standard do not include 
 the mandate that an implementation should be interoperable with other <B>MPI</B> implementations. However, MPI does provide 
 message passing routines for exchanging all the information needed to allow a single <B>MPI</B> implementation 
 to operate in a heterogeneous environment.  <BR> <BR>

  <B>MPI</B> (Message Passing Interface) is a standard specification for message passing libraries. <B>MPI</B> makes it 
  relatively easy to write portable parallel programs. 
</span> </p> 

</td>
</tr>
 


<!-- ******************** About MPI Introduction  Ends ********************** -->



<!-- ************************** About MPI Forum  Starts ************************ -->



   

<TR>
<TD>


<p align = "justify">
<span class = "content">
						
 In the past, both commercial and software makers provided different solutions to the users on the message passing paradigms. 
 Important issues for the user community are portability, performance, and features. The user community, which quite definitely includes the software suppliers themselves, recently 
 determined to address these issues. <BR> <BR>

 In April 1992, the Center for Research in Parallel Computation sponsored a 
 one-day workshop of Standards for Message Passing in a Distributed-Memory  Environment. The result of that workshop, which 
 featured presentations of many systems, was a realization both that people were eager to cooperate on the definition of a standard.
 At the Supercomputing' 92 conference in November, a committee was formed to define a message-passing standard. At the time of creation, few knew what the outcome might look like, but the effort was begun 
 with the following goals: </span> </p>

 <UL>
<p align = "justify">
<span class = "content">

  <LI> 
  <p align = "justify">
   Define a portable standard for message passing. It would not be an official, ANSI - like standard, but it would attract both 
   implementers and users.  </p>
  </LI> 
									
  <LI> <p align = "justify">
  Operate in a completely open way. Anyone would be free to join the discussions, either by attending meetings in person or 
  by monitoring e-mail discussions.  </p>

 </LI> 
 <LI> <p align = "justify">
  Be finished in one year </p>
 </LI>

</span> 

 </UL>

<p align = "justify">
<span class = "content">
			
 The <B>MPI</B> effort has been a lively one, as a result of the tensions among these three goals.  The <B>MPI</B> Forum decided to 
 follow the format used by the High Performance Fortran Forum, which had been well received by its community. The <B>MPI</B> effort
 will be successful in attracting a wide class of vendors and users because the <B>MPI</B> Forum itself was so broadly based. 

 Convex, Cray, IBM, Intel, Meiko, nCUBE, NEC, and Thinking Machines represented the parallel computer vendors. Members of the
 groups associated with the portable software libraries were also present. PVM, p4, Zipcode, Chameleon, PARMACS, TCGMSG, and Express 
 were all represented. In addition, a number of parallel application specialists  were on hand.<BR> <BR>

 <B>MPI</B> achieves portability by providing a public domain, platform-independent standard of message-passing library. 
 <B>MPI</B> specifies this library in a language-independent form, and provides Fortran and C bindings. This specification does 
  not contain any feature that is specific to  any particular vendor, operating system, or hardware. Due to these reasons, <B>MPI</B>
 has gained wide acceptance in the parallel computing community. <B>MPI</B> has been implemented on IBM PCs on Windows, all main 
 Unix workstations, and all major parallel computers. This means that a parallel program written in standard C or Fortran, 
 using <B>MPI</B> for message passing, could run without change on a single PC, a workstation, a  network of workstations, an MPP,
 from any vendor, on any operating system. <BR> <BR>

<B>MPI</B> is not a stand-alone, self-contained software system. It serves as a message-passing communication layer on top of the native parallel programming 
 environment, which takes care of necessities such as process management and I/O. Besides these proprietary environments, there are several public-domain<B>MPI</B>
 environments. Examples include the CHIMP implementation developed at Edinburg University, and the LAM (Local Area Multicomputer) developed at the Ohio Supercomputer Center, which is an <b>MPI</b> programming environment for 
 heterogeneous Unix clusters. <BR> <BR>

<B>MPICH</B> The most popular public-domain implementation is <B>MPICH</B>, developed jointly by Argonne National Laboratory 
 and Mississippi State University.<B>MPICH</B> is a portable implementation of <B>MPI</B> on a wide range of machines, from IBM PC's, 
 networks of workstations, to SMPs and MPPs. The portability of <B>MPICH</B> means that one can simply retrieve the same <B>MPICH</B> Package and install it on 
 almost any platform. <B>MPICH</B> also has good performance on Many parallel machines, because it often runs in the more efficient 
 native mode rather than over the common TCP/IP sockets. <BR> <BR>

In addition to meetings every six weeks for more than a year, there were continuous discussions via electronic mail, in which many
 persons from the worldwide parallel computing community participated. Equally important, an early commitment to producing a model
 implementation helped to demonstrate that an implementation of <B>MPI</B> was feasible. The MPI Standard is just being 
 completed (May 1994). Till now <I>mpich</I> Versions have been updated from Version-1.1 to Version-2.0 (MPI 1.2.6  & MPI 2.0) <BR> <BR> 

</span> </p>



<p align = "justify">
<span class = "content">
 For more information of <I>mpich</I> Verisons refer to  
 <A HREF="http://www-unix.mcs.anl.gov/mpi/mpich"> 
<font color = "blue">http://www-unix.mcs.anl.gov/mpi/mpich </font> </A>  <BR> <BR>


<div align ="right">
  <a href="#top">
<img src="./../hypack13_images/top.gif" align="right" border="0" width="13"height="13"></a>
</div> 
</TD>
</TR>
</tbody>
</table>

<!-- ********************* About MPI Forum   Ends  ********************** -->



<!-- ***************** About MPI-1.X Starts ****************** -->



 <a name="mpi-1.x"> </a> 
 <TABLE cellPadding=3  border=0> 
 <TBODY> 
 <TR>
 <TD bgColor = "#cccdd77889"> 

<p align = "justify">


<font color="black"><b>MPI-1.1 </font> </B>: 

</TD>
 </TR> 


<TR>
<TD>

<p align = "justify">
<span class = "content">
Perhaps the best way to introduce the concepts in <B>MPI</B> that might initially appear unfamiliar is to show how they have arisen as necessary 

extensions of quite familiar concepts. Let us consider what is perhaps the most 
elementary operation in a message-passing library, the basic send operation. In 
most of the current message-passing systems, it looks very much like this:
</span>
</p>


<p align = "center">
<span class = "content">
<FONT color="red"><I><B> send (address, length, destination, tag)</b>, </font> </i>
</span>
</p>

<p align = "justify"> where, </p>

<UL>
<p align = "justify">


<LI>
<span class = "content">
<I>address</I>
is a memory location signifying the beginning of the buffer containing the data 
to be sent, 
</span> 
</li>

<LI>
<span class = "content">
<I>length</I>
is the length in bytes of the message,&nbsp;

</span>
</li>

<LI>

<span class = "content">
<I>destination</I>
is the process identifier of the process to which this message is sent ( usually an integer), and
</span> 
</li>


<LI>

<span class = "content">
<I>tag</I> is an arbitrary non-negative integer to restrict receipt of the  message ( sometimes also called type)
</span>
</LI>

</p> 
</UL>



<p align ="justify">  <span class = "content">
  This particular set of parameters is frequently chosen because it is a good 
  compromise between what the programmers needs and what the hardware can do 
  efficiently (transfer a contiguous area of memory from one processor to 
  another). In particular, the system software is expected to supply queuing 
 capabilities so that a receive operation&nbsp; </p>


<p align = "center">
<span class = "content">
<FONT color="red"><I><B> recv (address, maxlen, source, tag, actlen)</b>, </font> </i>
</span>
</p>



<P align ="justify"> 
<span class = "content">
will complete successfully only if a message is received with the correct tag. 
Other messages are queued until a matching receive is executed. In most current 
systems <I>source</I> is an output argument indicating where the message came 
from, although in some systems it can also be used to restrict matching and to useful,
cause message queuing. On receive, <I>address</I> and <I>maxlen</I> together 
describe the buffer into which the received data is to be put, <I>actlen</I>
is the number of bytes received. </span> </p>


<P align ="justify"> 
<span class = "content">
Message-passing systems with this sort of syntax and semantics have proven 
extremely useful; yet have imposed restrictions that are now recognized as 
undesirable by a large user community. The <B>MPI</B> Forum has sought to lift 
these restrictions by providing more flexible versions of each of these 
parameters, while retaining the familiar underlying meanings of the basic <I>send</I>
and <I>receive</I> operations.
</span> </p>


<P align ="justify"> 
<span class = "content">

Let us examine these parameters one by one, in each case discussing first the 
current restrictions and then the <B>MPI</B> version. The (<I>address, length</I>) 
specification of the message to be sent was a good match for early hardware but 
is no longer adequate for two different reasons:
</span> </p>

<BR>



<UL>
<LI>
<P align ="justify"> 
<span class = "content">
In many situations, the message to be sent is not contiguous. In the simplest 
case, it may be a row of a matrix that is stored column wise. In general, it may 
consist of an irregularly dispersed collection of structures of different 
sizes. In the past, programmers (or libraries) have provided code to pack this 
data into contiguous buffers before sending it and to unpack it at the 
receiving end. However, as communications processors appear that can deal 
directly with striped or even more generally distributed data, it becomes more 
critical for performance that the packing be done "on the fly" by the 
communication processor in order to avoid the extra data movement. This cannot 
be done unless we describe the data in its original (distributed) form to the 
communication library.</span> </p>

<BR>
<LI>
<P align ="justify"> 
<span class = "content">
The past few years have seen a rise in the popularity of heterogeneous 
computing. The popularity comes from two sources. The first is the distribution 
of various parts of a complex calculation among different semi-specialized 
computers (e.g., SIMD, vector, graphics). The second is the use of workstation 
networks as parallel computers. Workstation networks, consisting of machines 
acquired over time, are frequently made up of a variety of machine types. In 
both of these situations, messages must be exchanged between machines of 
different architectures, where (<I>address, length</I>) is no longer an 
adequate specification of the semantic content of the message. For example, 
with a vector of floating-point numbers, not only the floating-point format be 
different, but even the length may be different. This situation is true for 
integers as well. The communication library can do the necessary conversion if 
it is told precisely what is being transmitted.</span> </p>
</LI>
</UL>

<P align ="justify"> 
<span class = "content">
The <B>MPI</B> solution for both of these problems is, to specify messages at a 
higher level and in a more flexible way to reflect the fact that the contents 
of a message contain much more structure than just a string of bits. Instead, 
an <B>MPI</B> message buffer is defined by a triple (<I>address, count, datatype</I>), 
describing <I>count</I> occurrences of the data type <I>datatype</I> starting 
at <I>address</I>. The power of this mechanism comes from the flexibility in 
the values of datatype. To begin with, <I>datatype</I> can take on the values 
of elementary data types in the host language. Thus (A,300,MPI_REAL) describes 
a vector <B>A</B> of <B>300</B> real numbers in Fortran, regardless of the 
length or format of a floating point number. An <B>MPI</B> implementation for 
heterogeneous networks guarantees that the same <B>300</B> real numbers will be 
received, even if the receiving machine has a very different floating-point 
format. The real power of data types, however, comes from the fact that users 
can construct their own data types using <B>MPI</B>
routines and that these data types can describe noncontiguous data.
</span> </p>


<P align ="justify"> 
<span class = "content">
A nice feature of the <B>MPI</B> design is that <B>MPI</B> provides a powerful 
functionality based on four orthogonal concepts. These four concepts in <B>MPI</B>
are message data types, communicators, communication operations, and virtual 
topology. 
</span> </p>



<P align ="justify"> 
<span class = "content">

<B>Separating Families of Messages : </B>
</span. ></p>

<P align ="justify"> 
<span class = "content">
Nearly, all message-passing systems provide a tag argument for the send and 
receive operations. This argument allows the programmer to deal with the 
arrival of messages in an orderly way, even if the messages that arrive "of the 
wrong tag" until the program (peer) is ready for them. Usually a facility 
exists for specifying wild-card tags that match any tag. This mechanism has 
proven necessary but insufficient, because the arbitrariness of the tag choices 
means that the entire program must use tags in a predefined, coherent way. 
Particular difficulties arise in the case of libraries, written far from the 
application programmer in time and space, whose messages must not be 
accidentally received by the application program.
</span> </p>

<P align ="justify"> 
<span class = "content">

<B>MPI</B>'s solution is to extend the notion of tag with a new concept: <I>context</I>. 
Contexts are allocated at run time by the system in response to user (and 
library) requests and are used for matching messages. They differ from tags in 
that they are allocated by the system instead of the user and no wild-card 
matching is permitted. The usual notion of message tag, with wild card 
matching, is retained in <B>MPI</B>
</span> </p>



<P align ="justify"> 
<span class = "content">

<B>Naming Processes&nbsp;</B>

Processes belong to groups. If a group contains <I>n</I> processes, then its 
processes are identified within the group by ranks, which are integers form 0 
to n-1. There is an initial group to which all processes in an <B>MPI</B>
implementation belongs. Within this group, processes are numbered similarly to 
the way in which they are numbered in many existing message-passing systems, 
from 0 up to 1 less than the total number of processes.<BR> <BR>

<B>Communicators</B>

The notions of context and group are combined in a single object called a <I>communicator</I>, 
which becomes an argument to most point-to-point and collective operations. 
Thus the <I>destination</I> or <I>source</I> specified in a <I>send</I> or <I>receive</I>
operation always refers to the rank of the process in the group identified with 
the given communicator. That is, in <B>MPI</B> the basic (blocking) <I>send</I> 
operation has become </span> </p>

<BR>
<p align = "center">
<span class = "content">
<I><FONT color="red">MPI_Send (buf, count, datatype, dest, tag, comm)</FONT> </i>
</p> </span> 

<p align = "left">
<span class = "content">
<FONT color="#000000">where</FONT> <BR> <BR>

 
 <UL>

 <LI>

  <I>buf</I>, <I>count</I>, <I>datatype</I> describes count occurrences of items of the form datatype starting at 
    <I>buf</I> </li>

  <LI>
  <I>dest</I> is the rank of the destination in the group associated with the communicator <I>comm</I>,&nbsp; </li>
 
  <LI>
    <I>tag</I>is as usual, and </li>

  <LI>
   <I>comm</I> identifies a group of processes and a communication context.
  </LI>
</span>

</UL>
</p>


<p align = "justify">
<span class = "content">						
The receive has become <BR> 
<CENTER><I><FONT color="red">MPI_Recv (buf, count, datatype, dest, tag, comm, 
status)</FONT></I>
</span> </p> 


<p align = "justify">
<span class = "content">
The <I>source</I>, <I>tag</I>, and <I>count</I> of the message actually received 
can be retrieved from <I>status</I>. Several other message-passing systems 
return the "<I>status</I>" parameters by separate cells that implicitly 
reference the most recent message received. <B>MPI</B>'s method is one aspect 
of its effort to the reliable in the situation where multiple threads are 
receiving messages on behalf of a process. The processes involved in the 
execution of a parallel program using <B>MPI</B>, are identified by a sequence 
of non-negative integers.&nbsp; If there are <I>p</I> processes executing a 
program, they will have ranks 0, 1,2,...., <I>p</I>
-1. <BR> <BR>


A set of routines that support point-to-point communication between pairs of 
processes. Blocking and non-blocking versions of the routines are provided 
which may be used in four different communication modes. These modes correspond 
to different communication protocols. Message selectivity in point-to-point 
communication is by <I>source</I> <I>process</I> and <I>message tag</I>
each of which may be wild carded to indicate that any valid value is 
acceptable.&nbsp; <BR> <BR>

The communicator abstraction that provides support for the design of safe, modular parallel software libraries. 
General or derived data types, those permits the specification of messages of noncontiguous data of different data types. 
Application topologies that specify the logical layout of processes. A common example is a Cartesian grid which is often used 
in two and three-dimensional problems. A rich set of collective communication routines that perform coordinated communication 
among a set of processes.  <BR><BR>

In MPI there is no mechanism for creating processes, and an MPI program is parallel  
abinitio i.e., there is a fixed number of processes from the start to the end of an application program. All processes are 
members of at least one process group. Initially all processes are members of the same group, and a number of routines are 
provided that allow an application to create (and destroy) new subgroups. Within a group each process is assigned a unique 
rank in the range 0 to n-1, where n is the number of processes in the group. This rank is used to identify a process, and, 
in particular, is used to specify the source and destination processes in a point-to-point communication operation, and  
the root process in certain collective communication operations.  <br> <BR>

MPI was designed as a message passing interface rather than a complete parallel programming environment,
 and thus in its current form intentionally omits many desirable features. For example, MPI lacks mechanisms for process 
creation and control, one-sided communication operations that would permit put and get messages, and active messages, 
non blocking collective communication operations, and the ability for a collective communication operation to involve 
more than one group, language bindings for Fortran 90 and C++.  These issues and other possible extensions to MPI,
 have been considered in the MPI-2 effort. Extensions to MPI 
for performing parallel I/O have also been considered in MPI-2.
 </span> </p> 


<div align ="right">
  <a href="#top"><img src="./../hypack13_images/top.gif" align="right" border="0" width="13"height="13"></a>
</div> 


<p align = "justify">
<span class = "content">

<B>The Communicator Abstraction</B> <BR> <BR> 

Communicators provide support for the design of safe, modular software libraries. Here means that messages intended for receipt 
by a particular receive call in an application will not be incorrectly intercepted by a different receive call. Thus, communicators 
are a powerful mechanism for avoiding unintentional non-determinism in message passing. This may be a particular problem when using
 third-party software libraries that perform message passing. The point here is that the application developer has no way of knowing
 if the tag, group, and rank completely disambiguate the message traffic of different libraries and the rest of the application.
 Communicator arguments are passed to all MPI message-passing routines, and a message can be communicated only if the communicator 
arguments passed to the send and receive routines match. Thus, in effect communicators provide an additional criterion for
 message selection, and hence permit the construction of independent tag spaces.  <BR> <BR> 

If communicators are not used to disambiguate message traffic there are two ways in which a call to a library routine can lead to 
unintended behaviour. In the first case, the processes enter a library routine synchronously when a send has been initiated 
for which the matching receive is not posted until after the library call. In this case the message may be incorrectly received 
in the library routine. The second possibility arises when different processes enter a library routine asynchronously 
resulting in a no deterministic behaviour. If the program behaves correctly, processes 0 and 1 each receive a message from 
process 2, using a wildcard selection criterion to indicate that they are prepared to receive a message from any process. 
The three processes then pass data around in a ring within the library routine. If separate communicators are not used for 
the communication inside and outside of the library routine this program may intermittently fail. Suppose we delay the sending 
of the second message sent by process 2, for example, by inserting some computation. In this case the wild carded receive in 
process 0 is satisfied by a message sent from process 1, rather than from process 2, and deadlock results. By supplying a 
different communicator to the library routine we can ensure that the program is executed correctly, regardless of when the 
processes enter the library routine.  <BR> <BR>

Communicators are opaque objects, which means they can only be manipulated using MPI routines. The key point about communi
cators is that when a communicator is created by an MPI routine it is guaranteed to be unique. Thus it is possible to create a 
communicator and pass it to a software library provided that communicator is not used for any message passing outside of the 
library.  <BR> <BR>

Communicators have a number of attributes. The group attribute identifies the process group relative to which process ranks 
are interpreted, and/or which identifies the process group involved in a collective communication operation. Communicators
 also have a topology attribute which gives the topology of the process group. In addition, users may associate arbitrary 
attributes with communicators through a mechanism known as caching.  </span> </p>

<div align ="right">
  <a href="#top"><img src="./../hypack13_images/top.gif" align="right" border="0" width="13"height="13"></a>
</div> 

	

<p align = "justify">
<span class = "content">
<B> Point-To-Point Communication  </b> <BR> <BR>  

MPI provides routines for sending and receiving blocking and nonblocking messages. A blocking send does not return until 
it is safe for the application to alter the message buffer on the sending process without corrupting or changing the 
message sent. A nonblocking send may return while the message buffer on the sending process is still volatile, and it 
should not be changed until it is guaranteed that this will not corrupt the message. This may be done by either calling 
a routine that blocks until the message buffer may be safely reused, or by calling a routine that performs a nonblocking 
check on the message status. A blocking receive suspends execution on the receiving process until the incoming message has
 been placed in the specified application buffer. A nonblocking receive may return before the message is actually received 
into the specified application buffer, and a subsequent call must be made to ensure that this occurs before the buffer is 
reused. </span> </p>

<p align = "justify">
<span class = "content">

<B> Communication Modes  </b> <BR> <BR>   

In MPI, a message may be sent in one of four communication modes, which approximately corresponds to the most 
common protocols used for point-to-point communication. In ready mode a message may be sent only if a corresponding 
receive has been initiated. In standard mode a message may be sent regardless of whether a corresponding receive has been 
initiated. MPI includes a synchronous mode, which is the same as the standard mode, except that the send operation will 
not be complete until a corresponding receive has been initiated on the destination process. Finally, there is a buffered mode. 
To use buffered mode the user must first supply a buffer and associate it with a communicator. When a subsequent send is performed 
using that communicator, MPI may use the associated buffer to send the message. A buffered send may be performed regardless of 
whether a corresponding receive has been initiated. <BR>  <BR>
  
MPI has both the blocking send and receive operations described above and nonblocking versions whose completion can 
be tested for and waited for explicitly. It is possible to test and wait on multiple operations simultaneously. MPI 
also has multiple communication modes. The standard mode corresponds to current common practice in message-passing systems. 
The synchronous mode requires sends to block until the corresponding receive has occurred (as opposed to the standard mode 
blocking send which blocks only until the buffer can be reused). The ready mode (for sends) is a way for the programmer 
to notify the system that, receive has been posted, so that the underlying system can use a faster protocol if it is
 available.  <BR> <BR>

There are, therefore, 8 types of send and 2 types of receive operation. In addition, routines are provided that perform send 
and receive simultaneously. Different calls are provided for when the send and receive buffers are distinct, and when they 
are the same. The send/receive operation is blocking, so does not return until the send buffer is ready for reuse, and the 
incoming message has been received. The two-send/receive routines bring the total number of point-to-point message passing 
routines up to 12.  </span> </p>

</span> </p> 

<div align ="right">
  <a href="#top"><img src="./../hypack13_images/top.gif" align="right" border="0" width="13"height="13"></a>
</div> 

<p align = "justify">
<span class = "content">

<B> Message-Passing Modes    </B> <BR> <BR>

It is customary in message-passing systems to use the term communication to refer to all interaction operations, i.e., 
communication, synchronization, and aggregation. Communications usually occur within processes of the same group. However,
 inter-group communications are also supported by some systems (e.g., MPI). There are three aspects of a communication mode 
that a user should understand </span> </p>


<p align = "center">
<span class = "content">



<I> <font color ="red">
   How many processes are involved?<BR> <BR>
</font> </I>

<I> <font color ="red">
 How are the processes synchronized?<BR> <BR>
</font> </I>

<I> <font color ="red">
 How are communication buffers managed?<BR> <BR>
</font> </I>

</ul>

</span> 
</p>


<p align = "justify">
<span class = "content">
Three communication modes are used in today's message-passing systems. We 
describe these communication modes below from the user's viewpoint, using a 
pair of send and receive, in three different ways. We use the following code 
example to demonstrate the ideas.</span> </p>

<blockquote>
<I>Send and receive buffers in message passing</I> <BR> <BR>
 </blockquote> 

<p align = "justify">
<span class = "content">
In the following code, process P sends a message 
contained in variable M to process Q, which receives the message into its 
variable S. <BR> <BR>
</span> </p> 



<table  width = "400" border="1" bordercolor="#000000">
<tbody>
<tr>

       <td>
	 <div align="center"><b>Processor P</b></div>
	</td>

	<td>
	<div align="center"><b>Processor Q </b></div>
	</td>
	</tr>

	<tr bordercolor="#000000">

	<td>
	<div align="center">M=10</div>
	</td>
	<td>
	<div align="center">S=-100</div>
	</td>
	</tr>

        <tr>
	<td>
	<div align="center">L1 : Send M to Q; </div>
	</td>
	<td>
	<div align="center">L1 : receive S from P; </div>
	</td>
  

	<tr>
	<td>
	<div align="center">L2 : M=20; </div>
	</td>
	<td>
	<div align="center">L2 : X = S + 1; </div>
	</td>
	</tr>

	<tr>

	<td>
	<div align="center">goto L1; </div>
	</td>
	<td>
	<div align="center">-</div>
	</td>
	</tr>

  </tbody>
  </table>


<p align = "justify">
<span class = "content">

The variable M is often called the send message buffer (or send buffer), and S is called the receive message buffer 
(or receive buffer).</span> </p>


<p align = "justify">
<span class = "content">
<i> Synchronous Message Passing : </i>  When process P executes a synchronous send M to Q, it has to wait until process Q executes 
a corresponding synchronous receive S from P. Both processes will not return from the send or receive until the message 
At is both sent and received. This means in the above code that variable X should 
 evaluate to 11.  <Br> <Br> 

        When the send and receive return, M can be immediately overwritten by P and S can be immediately read by Q, in the 
subsequent statements L2. Note that no additional buffer needs to be provided in synchronous message passing. The receive message 
buffer S is available to hold the arriving message. <BR> <BR>

<i> Blocking Send/Receive : </i>  A blocking send is executed when a process reaches it, without waiting for a corresponding receive.
 A blocking send does not return until the message is sent, meaning the message variable M can be safely rewritten. Note that 
when the send returns, a corresponding receive is not necessarily finished, or even started. All we know is that the message is
 out of M. It may have been received. But it may be temporarily buffered in the sending node, somewhere in the network, or it may
 have arrived at a buffer in the receiving node.  <BR> <BR>

        A blocking receive is executed when a process reaches it, without waiting for a corresponding send. However,
 it cannot return until the message is received. In the above code, X should evaluate to 11 with blocking send/receive. 
Note that the system may have to provide a temporary buffer for blocking-mode message passing. <BR> <BR>   

<I> NonBlocking Send/Receive : </i>  A nonblocking send is executed when a process reaches it, without waiting for a corresponding 
receive. A nonblocking send can return immediately after it notifies the system that the message M is to be sent. The message 
data are not necessarily out of M. Therefore, it is unsafe to overwrite M.  <BR> <BR>

        A nonblocking receive is executed when a process reaches it, without waiting for a corresponding send. It can return 
immediately after it notifies the system that there is a message to be received. The message may have already arrived, may be 
still in transient, or may have not even been sent yet. With the nonblocking mode, X could evaluate to 11, 21, or -99 in the 
above code, depending on the relative speeds of the two processes. The system may have to provide a temporary buffer for 
nonblocking message passing. These three modes are compared in below table. 

</span> </p> 


</td>
</tr>
</tbody>
</table>

<!-- ****************  Comparison of Three Communication Modes TABLE Starts ****************  -->

     

<p><b>Comparison of Three Communication Modes</b></p>

<table  border="1" bordercolor="#000000">
<tbody>
<tr>

       <td>
	 <div align="center"><b>Communication Event</b></div>
	</td>

	<td>
	<div align="center"><b>Synchronous</b></div>
	</td>

	<td>
	<div align="center"><b>Blocking</b></div>
	</td>
	
        <td>
	<div align="center"><b>NonBlocking</b></div>
	</td>

	</tr>

	<tr bordercolor="#000000">

	<td>
	<div align="center">Send Start Condition</div>
	</td>
	<td>
	<div align="center">Both send and receive reached</div>
	</td>
	<td>
       <div align="center">Send reached</div>
       </td>
	<td>
        <div align="center">Send reached</div>
	</td>

	</tr>

        <tr>

	<td>
	<div align="center">Return of send indicates</div>
	</td>
	<td>
	<div align="center">Message received</div>
	</td>
       <td>
	<div align="center">Message sent</div>
	</td>
	<td>
	<div align="center">Message send initiated</div>
	</td>

	</tr>

	<tr>

	<td>
	<div align="center">Semantics</div>
	</td>
	<td>
	<div align="center">Clean</div>
	</td>
	<td>
	<div align="center">In-Between</div>
	</td>
	<td>
	<div align="center">Error-Prone</div>
	</td>

	</tr>

	<tr>

	<td>
	<div align="center">Buffering Message</div>
	</td>
	<td>
	<div align="center">Not needed</div>
	</td>
	<td>
	<div align="center">Needed</div>
	</td>
	<td>
	<div align="center">Needed</div>
	</td>

	</tr>

	<tr>

	<td>
	<div align="center">Status Checking</div>
	</td>
	<td>
	<div align="center">Not needed</div>
	</td>
	<td>
	<div align="center">Not needed</div>
       </td>
	<td>
	<div align="center">Needed</div>
	</td>

	</tr>

	<tr>

	<td>
	<div align="center">Wait Overhead</div>
	</td>
	<td>
	<div align="center">Highest</div>
	</td>
	<td>
	<div align="center">In-Between</div>
	</td>
	<td>
	<div align="center">Lowest</div>
	</td>

	</tr>

	<tr>

	<td>
	<div align="center">Overlapping in Communications and Computations</div>
       </td>
	<td>
	<div align="center">No</div>
	</td>
        <td>
	<div align="center">Yes</div>
	</td>
	<td>
	<div align="center">Yes</div>
	</td>

	</tr>
</tbody>
	</table>
<!-- ****************  Comparison of Three Communication Modes TABLE Ends ****************  -->




<table  border="0">
<tbody>
<tr>
<td>

<p align = "justify">
<span class = "content">

<BR> <BR> 
  In real parallel systems, there are variants on this definition of synchronous (or the other two) mode. For example, 
in some systems, a synchronous send could return when the corresponding receive is started but not finished. A different 
term may be used to refer to the synchronous mode. The term asynchronous is used to refer to a mode that is not synchronous, 
such as blocking and nonblocking modes. <BR> <BR>


        Blocking and nonblocking modes are used in almost all existing message-passing systems. They both reduce the 
wait time of a synchronous send. However, sufficient temporary buffer space must be available for an asynchronous send, 
because the corresponding receive may not be even started; thus the memory space to put the received message may not be known. <BR> <BR>


        The main motivation for using the nonblocking mode is to overlap communication and computation. However, 
nonblocking introduces its own inefficiencies, such as extra memory space for the temporary buffers, allocation 
of the buffer, copying message into and out of the buffer, and the execution of an extra wait-for function. These 
overheads may significantly offset any gains from overlapping communication with computation. <BR> <BR>


<B> Persistent Communication Requests </b> <BR> <BR>    

MPI also provides a set of routines for creating communication request objects that completely describe a send or 
receive operation by binding together all the parameters of the operation. A handle to the communication object so 
formed is returned, and may be passed to a routine that actually initiates the communication. As with the nonblocking
communication routines, a subsequent call should be made to ensure completion of the operation.  <BR> <BR>

Persistent communication objects may be used to optimize communication performance, particularly when the same 
communication pattern is repeated many times in an application. For example, if a send routine is called within a loop, 
performance may be improved by creating a communication request object that describes the parameters of the send prior to
 entering the loop, and then initiating the communication inside the loop to send the data on each pass through the loop.  <BR> <BR>

There are five routines for creating communication objects: four for send operations (one corresponding to each 
communication mode), and one for receive operations. A persistent communication object should be deallocated when no 
longer needed.  <BR> <BR>

<B> Application Topologies   </b> <BR> <BR>  

In many applications the processes are arranged with a particular topology, such as a two or three-dimensional grid.
 MPI provides support for general application topologies that are specified by a graph in which processes that 
communicate a significant amount are connected by an arc. If the application topology is an n-dimensional Cartesian grid then this generality 
is not needed, so as a convenience MPI provides explicit support for such topologies. For a Cartesian grid, periodic or nonperiodic boundary 
conditions may apply in any specified grid dimension. In MPI, a group either has a Cartesian or graph topology, or no topology. In addition to 
providing routines for translating between process ran and location in the topology, MPI also: allows knowledge of the application topology to 
be exploited in order to efficiently assign processes to physical processors, provides a routine for partitioning a Cartesian grid into hyper-plane 
groups by removing a specified set of dimensions, provides support for shifting data along a specified dimension of Cartesian grid. By dividing a
 Cartesian grid into hyper-plane groups, it is possible to perform collective communication operations within these groups. In particular, if all 
but one dimension is removed a set of one-dimensional subgroups is formed, and it is possible, for example, to perform a multicast in the 
corresponding direction.  <BR> <BR>
</span> </p> 

<div align ="right">
  <a href="#top"><img src="./../hypack13_images/top.gif" align="right" border="0" width="13"height="13"></a>
</div> 

<p align = "justify">
<span class = "content">


<B> Collective Communication   </b> <BR> <BR>  

Collective communication routines provide for coordinated communication among a group of processes. The communicator object that is input to the routine gives the process group.
 The MPI collective communication routines have been
 designed so that their syntax and semantics are consistent with those of the point-to-point routines. The collective 
communication routines maybe (but do not have to be) implemented using the MPI point-to-point routines. Collective 
communication routines do not have message tag arguments, though an implementation in terms of the point-to-point 
routines may need to make use of tags. All members of the group with consistent arguments must call a collective communication routine.
As soon as a process has completed its role in the collective communication it may continue
 with other tasks. Thus, a collective communication is not necessarily barrier synchronization for the group. MPI does
 not include nonblocking forms of the collective communication routines. MPI collective communication routines are divided 
into two broad classes: data movement routines, and global computation routines.
 Collective operations are of two kinds:  
</span> </p>



<uL>
<li> <p align = "justify">
<span class = "content">
Data movement operations are used to rearrange data among the processes. The simplest of these is a broadcast, but many 
elaborate scattering and gathering operations can be defined (and are supported in MPI) 
 </span></p> </lI>  </ul> 

<ul> 
<li>   
<p align = "justify">
<span class = "content">
 Collective computation operations (minimum, maximum, sum, logical OR, etc., as well as user-defined operations).  
</span> </p> </li>  </ul>  

<p align = "justify">
<span class = "content">
In both cases, a message-passing library can take advantage of its knowledge of the structure of the machine to optimize
 and increase the parallelism in these operations. MPI has a large set of collective communication operations, and a mechanism 
by which users can provide their own. In addition, MPI provides operations for creating and managing groups in a scalable way. Such groups can be 
used to control the scope of collective operations. MPI has an extremely flexible mechanism for describing data movement routines. These are particularly 
powerful when used in conjunction with the derived data types.  Virtual topologies. <BR> <BR>

One can conceptualize process in an application-oriented topology, for convenience in programming. Both general graphs and
 grids of processes are supported. Topologies provide a high-level method for managing process groups without dealing with 
them directly. Since topologies are a standard part of MPI, we do not treat them as an exotic, advanced feature. We use 
them early in the book and freely from then on.

</span> </p> 

<div align ="right">
  <a href="#top"><img src="./../hypack13_images/top.gif" align="right" border="0" width="13"height="13"></a>
</div> 

<p align = "justify">
<span class = "content">

<B> Debugging and Profiling   </b> <BR> <BR> 

Rather than specify any particular interface, MPI requires the availability of "hooks" that allow users to intercept MPI 
calls and thus define their own debugging and profiling mechanisms.  <BR> <BR>

<B> Support for libraries   </b> <BR> <BR> 

The structuring of all communication through communicators provides to library writers for the first time the capabilities
 they need to write parallel libraries that are completely independent of user code and interoperable with other libraries.
 Libraries can maintain arbitrary data; called attributes, associated with the communicators they allocate, and can specify 
their own error handlers.  <BR> <BR>

 <B> Support for heterogeneous network  </b> <BR> <BR> 

MPI programs can run on networks of machines that have different lengths and formats for various fundamental data types,
 since each communication operation specifies a (possibly very simple) structure and all the component data types, so that 
the implementation always has enough information to do data format conversions if they are necessary. MPI does not specify 
how this is done, however thus allowing a variety of optimizations.
</span> </p>

						
<div align ="right">
  <a href="#top"><img src="./../hypack13_images/top.gif" align="right" border="0" width="13"height="13"></a>
</div> 

</td>
</tr>

</tbody>
</table>

<!-- ***************** About MPI-1.1 Ends **************** -->

<!-- ******************** About Basic MPI-1.X library Calls  Starts ******************** -->

<a name="mpi-1.x-c-fortran-lib-calls"> </a>

 <TABLE cellPadding=3  border=0> 
    <TBODY> <TR> 

      <TD bgColor = #cccdd77889>  

      <DIV align=Left><font size="2"  color = "Blue" face = "Arial"> 
        <p  align="justify"><b><font face="Verdana" color="black">
  Basic MPI 1.X library Calls    </b> </p>
      </font></DIV> 
</TD> 
</TR> 


<TR>
<TD>
<br><br>

<p align = "justify">
<span class = "content">

<B>Brief Introduction to MPI 1.X  Library  Calls</B> : <BR> <BR>

Most commonly used MPI Library calls in FORTRAN/C -Language
have been explained below. <BR> <BR>


<!-- ***** MPI Init Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>
<B>MPI_Init<I>(int *argc, char **argv</I>);</B> <BR> <BR>
</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>
<B>MPI_Init</B>(ierror)  <BR> 
<I> Integer </i> ierror <BR> <BR>

<FONT COLOR="red"><I> Initializes the MPI execution environment  </i> </FONT> 


<p align = "justify">
<span class = "content">
This call is required in every MPI program and must be the first MPI call. It 
establishes the MPI "environment". Only one invocation of MPI_Init can occur in each 
program execution. It takes the command line arguments as parameters. In a FORTRAN call to
 MPI_Init the only argument is the error code. Every Fortran MPI subroutine returns an error 
code in its last argument, which is either MPI_SUCCESS or an implementation-defined error code. 
It allows the system to do any special setup so that the MPI library can be used.  					

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI Init Libray call ends ***** -->


<!-- ***** MPI Comm Rank  Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>

<B>MPI_Comm_rank (<I>MPI_Comm</I> </B>comm<B>,&nbsp; <I>int</I> </B>rank<B>); </B> <BR> <BR>

</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>
<B>MPI_Comm_rank</B> (comm, rank, ierror) <BR> 
 

<I>integer</I></B> comm, rank, ierror <BR> <BR>


<FONT COLOR="red"><I> Determines the rank of the calling process in the communicator   
   </i> </FONT> 

<p align = "justify">
<span class = "content">
The first argument to the call is a communicator and the rank of the process is returned 
in the second argument. Essentially a communicator is a collection of processes 
that can send messages to each other. The only communicator needed for basic programs
 is MPI_COMM_WORLD and is predefined in MPI and consists of the processees running 
when program execution begins.  
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI Comm Rank Libray call ends ***** -->


<!-- ***** MPI Comm Size  Library call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>

<B>MPI_Comm_size (<I>MPI_Comm</I></B> comm, <B><I>int</I></B> num_of_processes); </B> <BR> <BR>

</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>
<B> MPI_Comm_size</B> (comm, size, ierror)<BR> 
 

<I>integer</I></B> comm, size, ierror <BR> <BR>


<FONT COLOR="red"><I> Determines the size of the group associated with a communicator     
  </i> </FONT>  

<p align = "justify">
<span class = "content">
This function determines the number of processes executing the program. Its first argument 
is the communicator and it returns the number of processes in the communicator in its second
 argument.   
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI Comm Size  Libray call ends ***** -->


<!-- ***** MPI Finalize   Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>

<B>MPI_Finalize() </B>

<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>

<B>MPI_Finazlise</B>(ierror) <BR>
<B><I>integer</I></B>  ierror <BR> <BR>

<FONT COLOR="red"><I> Terminates MPI execution environment  </i> </FONT> 



<p align = "justify">
<span class = "content">
This call must be made by every process in a MPI computation. It terminates the MPI "environment", 
<B> no </b>  MPI calls my be made by a process after its call to <B> MPI_Finalize. </B>
 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI Finalize   Libray call ends ***** -->




<!-- ***** MPI Send   Library call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>


<B>MPI_Send (<I>void </I></B>*message<B>,<I>&nbsp; int</I> </B>count<B>,&nbsp; <I>MPI_Datatype</I>
</B>datatype<B>,&nbsp; <I>int</I> </B>destination<B>,&nbsp; <I>int</I> </B>tag<B>,&nbsp;
<I>MPI_Comm</I> </B>comm<B>);</B> <BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>
<B> MPI_Send</B>(buf, count, datatype, dest, tag, comm, ierror)<BR> 

 &lt;type&gt; buf (*) <BR>
 <B><I>integer</I></B> count, datatype, dest, tag, comm, ierror <BR> <BR>




<FONT COLOR="red"><I> Basic send (It is a blocking send call)   </i> </FONT> 


<p align = "justify">
<span class = "content">
The first three arguments descibe the message as the address,count and the datatype. 
The content of the message are stored in the block of memory refrenced by the address. 
The count specifies the number of elements contained in the message which are of 
a MPI type MPI_DATATYPE. The next argument is the destination, an integer specifying 
the rank of the destination process. 
The tag argument helps identify messages.    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI Send  Libray call ends ***** -->


<!-- ***** MPI Recv   Library call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>


<B>MPI_Recv (<I>void</I> </B>*message<B>,&nbsp; <I>int</I> </B>count<B>,&nbsp; <I>MPI_Datatype</I>
</B>datatype<B>,&nbsp; <I>int</I> </B>source<B>,&nbsp; <I>int</I> </B>tag<B>,&nbsp; <I>MPI_Comm</I>
</B>comm<B>,&nbsp; <I>MPI_Status</I> </B>*status<B>)&nbsp;</B> <BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>
<B>MPI_Recv</B>(buf, count, datatype, source, tag, comm, status, ierror)<BR>
&lt;type&gt; buf (*)&nbsp;
<BR><B><I>integer</I></B>
count, datatype, source, tag, comm, status, ierror&nbsp; <BR> <BR>



<FONT COLOR="red"><I> Basic receive ( It is a blocking receive call)  </i> </FONT> 


<p align = "justify">
<span class = "content">
The first three arguments descibe the message as the address,count and the datatype. 
The content of the message are stored in the block of memory referenced by the address. 
The count specifies the number of elements contained in the message which are of a MPI 
type MPI_DATATYPE. The next argument is the source which specifies the rank of the sending 
process. MPI allows the source to be a "wild card". There is a predefined constant
 MPI_ANY_SOURCE that can be used if a process is ready to receive a message from any 
sending process rather than a particular sending process. The tag argument helps identify 
messages. The last argument returns information on the data that was actually received. 
It references a record with two fields - one for the source and the other for the tag.  
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI Recv  Libray call ends ***** -->

<!-- **** ..........MPI Advanced Point-to-Point Library Calls Starts.........******* -->


<!-- ***** MPI Sendrecv   Library call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>

<B>MPI_Sendrecv</B> (<B><I>void</I></B> *sendbuf, <B><I>int</I></B> sendcount, <B><I>MPI</I></B>_<B><I>Datatype</I></B>
sendtype, <B><I>int</I></B> dest, <B><I>int</I></B> sendtag, <B><I>void</I></B> 
*recvbuf , <B><I>int</I></B> recvcount, <B><I>MPI_Datatype</I></B> recvtype, <B><I>int</I></B>
source, <B><I>int</I></B> recvtag, <B><I>MPI_Comm</I></B> comm, <B><I>MPI_Status</I></B>
*status); 

<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>

<B>MPI_Sendrecv</B> (sendbuf, sendcount, sendtype, dest, sendtag, recvbuf, 
recvcount, recvtype, source, recvtag, comm, status, ierror)
<BR> <BR>
&lt;type&gt; sendbuf (*), recvbuf (*)&nbsp; <BR>

<B><I>integer </I></B>
sendcount, sendtype, dest, sendtag, recvcount, recvtype, source, recvtag <BR>

<B><I>integer </I></B>
comm,  status(*), ierror
<BR> <BR>

<FONT COLOR="red"><I> Sends and recevies a message  </i> </FONT>  



<p align = "justify">
<span class = "content">
The function MPI_Sendrecv, as its name implies, performs both a send ana a 
receive. The parameter list is basically just a concatenation of the parameter 
lists for the MPI_Send and MPI_Recv. The only difference is that the 
communicator parameter is not repeated. The destination and the source 
parameters can be the same. The "send" in an MPI_Sendrecv can be matched by an 
ordinary MPI_Recv, and the "receive" can be matched by and ordinary MPI_Send. 
The basic difference between a call to this function and MPI_Send followed by 
MPI_Recv (or vice versa) is that MPI can try to arrange that no deadlock occurs 
since it knows that the sends and receives will be paired.
<BR><BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ***** MPI Sendrecv Library Call Ends **** -->


<!-- ***** MPI Sendrecv_replace   Library call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>

<B>MPI_Sendrecv_replace </B>(<B><I>void</I></B>* buf, <B><I>int</I></B> count, <B><I>MPI_Datatype</I></B>
datatype, <B><I>int</I></B> dest, <B><I>int</I></B> sendtag, <B><I>int</I></B> source,
<B><I>int</I></B> recvtag, <B><I>MPI_Comm</I></B> comm, <B><I>MPI_Status</I></B>
*status)

<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>

<B>MPI_Sendrecv_replace</B> (buf, count, datatype, dest, sendtag, source, 
recvtag, comm, status, ierror)
<BR>
&lt;type&gt; buf (*)
<BR>
<B><I>integer</I></B>
count, datatype, dest, sendtag, source, recvtag <BR>

<B><I>integer </I></B> 
comm,  status(*), ierror
<BR> <BR>

<FONT COLOR="red"><I> Sends and receives using a single buffer  </i> </FONT>  



<p align = "justify">
<span class = "content">
MPI_Sendrecv_replace sends and receives using a single buffer.
<BR><BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ***** MPI Sendrecv_replace Library Call Ends **** -->


<!-- ***** MPI Bsend   Library call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>

<B>MPI_Bsend</B> (<B><I>void</I></B> *buf, <B><I>int</I></B> count, <B><I>MPI_Datatype</I></B>
datatype, <B><I>int</I></B> dest, <B><I>int</I></B> tag, <B><I>MPI_Comm</I></B> 
comm)

<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>

<B>MPI_Bsend</B> (buf, count, datatype, dest, tag, comm, ierror)
<BR>
&lt;type&gt; buf (*)
<BR>
<B><I>integer</I></B>
count, datatype, dest, tag, comm, ierror

<BR> <BR>

<FONT COLOR="red"><I> Basic send with user specified buffering </i> </FONT>  



<p align = "justify">
<span class = "content">
MPI_Bsend copies the data into a buffer and transfers the complete buffer to the 
user.
<BR><BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ***** MPI Bsend Library Call Ends **** -->


<!-- ***** MPI Isend   Library call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>
<B>MPI_Isend</B> (<B><I>void</I></B>* buf, <B><I>int</I></B> count, <B><I>MPI_Datatype</I></B>
datatype, <B><I>int</I></B> dest, <B><I>int</I></B> tag, <B><I>MPI_Comm</I></B> 
comm, <B><I>MPI_Request</I></B> *request)

<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>

<B>MPI_Isend</B> (buf, count, datatype, dest, tag, comm, request, ierror)
<BR>
&lt;type&gt; buf (*)
<BR>
<B><I>integer</I></B>
count, datatype, dest, tag, comm, request, ierror

<BR> <BR>

<FONT COLOR="red"><I> Begins a nonblocking send </i> </FONT> 



<p align = "justify">
<span class = "content">
MPI_Isend is a nonblocking send. The basic functions in MPI for starting 
non-blocking communications are MPI_Isend. The "I" stands for "immediate," 
i.e., they return (more or less) immediately.
<BR><BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ***** MPI Isend Library Call Ends **** -->


<!-- ***** MPI Irecv   Library call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>
<B>MPI_Irecv</B> (<B><I>void</I></B>* buf, <B><I>int</I></B> count, <B><I>MPI_Datatype</I></B>
datatype, <B><I>int</I></B> source, <B><I>int</I></B> tag, <B><I>MPI_Comm</I></B>
comm, <B><I>MPI_Request</I></B> *request)

<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>

<B>MPI_Irecv</B> (buf, count, datatype, source, tag, comm, request, 
ierror)&nbsp;
<BR>
&lt;type&gt; buf (*)&nbsp;
<BR>
<B><I>integer</I></B>
count, datatype, source, tag, comm, request, ierror

<BR> <BR>

<FONT COLOR="red"><I> Begins a nonblocking send </i> </FONT> 



<p align = "justify">
<span class = "content">
MPI_Irecv begins a nonblocking receive. The basic functions in MPI for starting 
non-blocking communications are MPI_Irecv. The "I" stands for "immediate," 
i.e., they return (more or less) immediately.
<BR><BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ***** MPI Irecv Library Call Ends **** -->



<!-- ***** MPI Wait   Library call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>
<B> MPI_Wait</B> (<B><I>MPI_Reques</I></B>t *request, <B><I>MPI_Status</I></B> *status);

<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>

<B>MPI_Wait</B> (request, status, ierror)&nbsp;
<BR>
<B><I>integer</I></B> request, status (*), ierror

<BR> <BR>

<FONT COLOR="red"><I> Waits for a MPI send or receive to complete </i> </FONT> 



<p align = "justify">
<span class = "content">
MPI_Wait waits for an MPI send or receive to complete. There are variety of functions 
that MPI uses to complete nonblocking operations. The simplest of these is 
MPI_Wait. It can be used to complete any nonblocking operation. The request 
parameter corresponds to the request parameter returned by MPI_Isend or 
MPI_Irecv.
<BR><BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ***** MPI Wait Library Call Ends **** -->


<!-- ***** MPI Ssend   Library call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>

<B>MPI_Ssend</B> (<B><I>void</I></B>* buf, <B><I>int</I></B> count, <B><I>MPI_Datatype</I></B>
datatype, <B><I>int</I></B> dest, <B><I>int</I></B> tag, <B><I>MPI_Comm</I></B> 
comm)&nbsp

<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>

<B>MPI_Ssend</B> (buf, count, datatype, dest, tag, comm, ierror)&nbsp;
<BR>
&lt;type&gt; buf (*)&nbsp;
<BR>
<B><I>integer</I></B> count, datatype, dest, tag, comm, ierror

<BR> <BR>

<FONT COLOR="red"><I> Builds a handle for a synchronous send </i> </FONT> 


<p align = "justify">
<span class = "content">
MPI_Ssend is one of the synchronous mode send operations 
provided by MPI.
<BR><BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ***** MPI Ssend Library Call Ends **** -->


<!-- **** ..........MPI Advanced Point-to-Point Library Calls Ends.........******* -->



<!-- **** ..........MPI Colelctive Comm. & Comp.  Library Calls Starts........******* -->


<!-- ***** MPI Broadcast   Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>


<B>MPI_Bcast (<I>void</I> </B>*message<B>,&nbsp; <I>int</I> </B>count<B>,&nbsp; 
<I>MPI_Datatype</I>
</B>datatype<B>,<I>&nbsp; int</I></B> root<B>,&nbsp; <I>MPI_Comm</I> </B>comm<B>)</B> 
<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>
<B>MPI_Bcast</B>(buffer, count, datatype, root, comm, ierror)<BR>
&lt;type&gt; buffer (*)<BR>
<I>integer</I> count, datatype, root, comm, ierror&nbsp; <BR> <BR>



<FONT COLOR="red"><I> Broadcast a message from the 
    process with rank "root" to all other processes of the group   </I> </FONT> 


<p align = "justify">
<span class = "content">
It is a collective communication call in which a single process sends same data 
to every process. It sends a copy of the data in <B>message</B> on process <B>root</B>
to each process in the communicator <B>comm</B>. It should be called by all 
processors in the communicator with the same arguments for root and comm.;
<BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI BroadCast  Libray call ends ***** -->

<!-- ***** MPI Scatter   Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>

<B>MPI_Scatter ((<I>void</I> <I>*</I></B>send_buffer<B>,&nbsp; <I>int</I> </B>send_count<B>,&nbsp;
<I>MPI_DATATYPE </I></B><I>send_type</I><B>,&nbsp; <I>void</I> *</B>recv_buffer<B>,&nbsp;&nbsp;
<I>int</I> </B>recv_count<B>, <I>MPI_DATATYPE</I> </B>recv_type<B><I>,&nbsp; int</I></B>
root<B>,&nbsp; <I>MPI_Comm</I> </B>comm<B>);</B>
 
<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>

<B>MPI_Scatter</B>(sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, 
root , comm, ierror)<BR>
&lt;type&gt; sendbuf (*), recvbuf (*)
<BR>
<B><I>integer</I></B>
sendcount, sendtype, recvcount, recvtype, root , comm, ierror <BR> <BR>


<FONT COLOR="red"><I> Sends data from one process to all other processes in a group   

 </i> </FONT> 


<p align = "justify">
<span class = "content">
The process with rank <B><I>root</I></B> distributes the contents of <B><I>send_buffer</I></B>
among the processes. The contents of send_buffer are split into <B><I>p</I></B> 
segments each consisting of <B><I>send_count</I></B> elements. The first 
segment goes to process 0, the second to process 1 ,etc. The send arguments are 
significant only on process root.
<BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI Scatter  Libray call ends ***** -->



<!-- ***** MPI Scatterv   Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>

<B>MPI_Scatterv</B> (<B><I>void</I></B>* sendbuf, <B><I>int</I></B> *sendcounts,
<B><I>int</I></B> *displs, <B>MPI_Datatype</B> sendtype, <B><I>void</I></B>* 
recvbuf, <B><I>int</I></B> recvcount, <B><I>MPI_Datatype</I></B> recvtype, <B><I>int</I></B>
root, <B><I>MPI_Comm</I></B> comm)

<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>

<B>MPI_Scatterv</B> (sendbuf, sendcounts, displs, sendtype, recvbuf, recvcount, 
recvtype, root, comm, ierror)
<BR>

&lt;type&gt; sendbuf (*), recvbuf (*)&nbsp;
<BR>
<B><I>integer</I></B>
sendcounts (*), displs (*), sendtype, recvcount, recvtype, root, comm, ierror

<BR> <BR>

<FONT COLOR="red"><I> Scatters a buffer in different/same  size of parts to all processes 
in a group    

 </i> </FONT> 


<p align = "justify">
<span class = "content">
A simple extension to MPI_Scatter is MPI_Scatterv. 
MPI_Scatterv allows the size 
of the data being sent by each process to vary.<BR> <BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI Scatterv  Libray call ends ***** -->


<!-- ***** MPI Gather   Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>

<B>MPI_Gather (<I>void</I> <I>*</I></B>send_buffer<B>, <I>int</I> </B>send_count<B>,
<I>MPI_DATATYPE </I></B><I>send_type</I><B>, <I>void</I> *</B>recv_buffer<B>,&nbsp;
<I>int</I> </B>recv_count<B>, <I>MPI_DATATYPE</I> </B>recv_type<B><I>,&nbsp; int</I></B>
root<B>,&nbsp; <I>MPI_Comm</I> </B>comm<B>)</B>
 
<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>

<B>MPI_Gather</B>(sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, 
root, comm, ierror)
<BR>
&lt;type&gt; sendbuf (*), recvbuf (*)
<BR>
<B><I>integer</I></B>
sendcount, sendtype, recvcount, recvtype, root, comm, ierror&nbsp; <BR> <BR>

<FONT COLOR="red"><I> Process gathers together values from a group of tasks    

 </i> </FONT>


<p align = "justify">
<span class = "content">
Each process in comm sends the contents of <B><I>send_buffer</I></B> to the 
process with rank <B><I>root</I></B>. The process root concatenates the 
received data in the process rank order in <B><I>recv_buffer</I></B>. The 
receive arguments are significant only on the process with rank root. The 
argument <B><I>recv_count</I></B> indicates the number of items received from 
each process - not the total number received.<BR> <BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI Gather  Libray call ends ***** -->



<!-- ***** MPI Gatherv   Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>

<B>MPI_Gatherv</B> (<B><I>void</I></B>* sendbuf, <B><I>int</I></B> sendcount, <B><I>MPI_Datatype</I></B>
sendtype, <B><I>void</I></B> *recvbuf, <B><I>int</I></B> *recvcounts, <B><I>int</I></B>
*displs, <B><I>MPI_Datatype</I></B> recvtype, <B><I>int</I></B> root, <B><I>MPI_Comm</I></B>
comm)</B>
 
<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>

<B>MPI_Gatherv</B> (sendbuf, sendcount, sendtype, recvbuf, recvcounts, displs, 
recvtype, root, comm, ierror)
<BR>
&lt;type&gt; sendbuf (*), recvbuf (*)&nbsp;
<BR>
<B><I>integer</I></B>
sendcount, sendtype, recvcounts (*), displs (*), recvtype, root, comm, 
ierror
<BR> <BR>

<FONT COLOR="red"><I> Gathers into specified locations from all processes 
in a group    </i> </FONT>

<p align = "justify">
<span class = "content">
A simple extension to MPI_Gather is MPI_Gatherv. MPI_Gatherv 
allows the size of 
the data being sent by each process to vary.<BR> <BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI Gatherv  Libray call ends ***** -->



<!-- ***** MPI Barrier   Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>

<B>MPI_Barrier</B> (<B><I>MPI_Comm</I></B> comm)
 
<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>

<B>MPI_Barrier</B> (comm, ierror)

<BR>
<B><I>integer</I></B> comm, ierror 
<BR> <BR>



<FONT COLOR="red"><I> Blocks until all process have reached this routine  </i> </FONT> 


<p align = "justify">
<span class = "content">
MPI_Barrier blocks the calling process until all processes 
in comm have entered 
the function.
<BR> 
    
</span> </p>				

</li>
</ul>

<HR>

</span> 
</p>

<!-- ******** MPI Barrier Libray call ends ***** -->



<!-- ***** MPI Reduce   Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>

<B>MPI_Reduce (<I>void</I> </B>*operand<B>,&nbsp; <I>void</I> </B>*result<B>,&nbsp; <I>int</I>
</B>count<B>,&nbsp; <I>MPI_Datatype</I> </B>datatype<B>,&nbsp; <I>MPI_Operator</I> </B>
op<B>,&nbsp; <I>int</I> </B>root<B>, <I>MPI_Comm</I> </B>comm<B>);</B>
 
<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>

<B>MPI_Reduce</B>(sendbuf, recvbuf, count, datatype, op, root, comm,ierror)
<BR>
&lt;type&gt; sendbuf (*), recvbuf (*)
<BR>
<B><I>integer</I></B> count, datatype, op, root, comm, ierror <BR> <BR>



<FONT COLOR="red"><I> Reduce values on all processes to a single value  </i> </FONT>

<p align = "justify">
<span class = "content">
MPI_Reduce combines the operands stored in *operand using operation <B>op</B> and 
stores the result on <B>*result</B> on the <B>root</B>. Both operand and result 
refer <B>count</B> memory locations with type <B>datatype</B>. MPI_Reduce must 
be called by all the process in the communicator comm, and count, datatype 
and op must be same on each process.
<BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI Reduce  Libray call ends ***** -->


<!-- ***** MPI All Reduce   Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>

<B>MPI_Allreduce</B> (<B><I>void</I></B>* sendbuf, <B><I>void</I></B>* recvbuf, <B><I>int</I></B>
count, <B><I>MPI</I></B>_<B><I>Datatype</I></B> datatype, <B><I>MPI_Op</I></B> op,
<B><I>MPI_Comm</I></B> comm)&nbsp;

 
<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>

<B>MPI_Allreduce</B> (sendbuf, recvbuf, count, datatype, op, comm, ierror)
<BR>

&lt;type&gt; sendbuf (*), recvbuf (*)
<BR>
<B><I>integer</I></B>
count, datatype, op, comm, ierror <BR> <BR>


<FONT COLOR="red"><I> Combines values from all processes and 
distribute the result to all process.  </i> </FONT>


<p align = "justify">
<span class = "content">
MPI_Allreduce combines values form all processes and 
distribute the result back 
to all processes.
<BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI All Reduce  Libray call ends ***** -->






<!-- ***** MPI AllGather   Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>

<B>MPI_Allgather (<I>void</I> *</B>send_buffer<B>, <I>int</I> </B>send_count<B>, 
<I>MPI_DATATYPE</I>
</B>send_type<B>, <I>void</I> *</B>recv_buffer<B>, <I>int</I> </B>recv_count<B>,
<I>MPI_Datatype</I> </B>recv_type<B>, <I>MPI_Comm</I> </B>comm<B>)</B>
 
<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>

<B>MPI_Allgather</B>(sendbuf, sendcount, sendtype, recvbuf, recvcount, recvtype, 
comm, ierror)
<BR>
&lt;type&gt; sendbuf(*), recvbuf(*)
<BR>
<B><I>integer</I></B>
sendcount, sendtype, recvcount, recvtype, comm, ierror <BR> <BR>

<FONT COLOR="red"><I> Gathers data from all processes and distribute it to all       
  </i> </FONT>


<p align = "justify">
<span class = "content">
MPI_Allgather gathers the contents of each <B><I>send_buffer</I></B> on each process. 
Its <I>effect</I> is the same as if there were a sequence of <B><I>p </I></B>
calls to MPI_Gather, each of which has a different process acting as a <B><I>root</I>.&nbsp;
</B><BR><BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI AllGather  Libray call ends ***** -->



<!-- ***** MPI AlltoAll   Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>

<B>MPI_Alltoall</B> (<B><I>void</I></B>* sendbuf, <B><I>int</I></B> sendcount, 
<B><I>MPI_Datatype</I></B>
sendtype, <B><I>void</I></B>* recvbuf, <B><I>int</I></B> recvcount, <B><I>MPI_Datatype</I></B>
recvtype, <B><I>MPI_Comm</I></B> comm)
 
<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>

<B>MPI_Alltoal<U>l</U></B> (sendbuf, sendcount, sendtype, recvbuf, recvcount, 
recvtype, comm, ierror)
<BR>

&lt;type&gt; sendbuf (*), recvbuf (*)
<BR>

<B><I>integer</I></B>
sendcount, sendtype, recvcount, recvtype, comm, ierror
 <BR> <BR>

<FONT COLOR="red"><I> Sends distinct collection of data from all to all processes       
  </i> </FONT>


<p align = "justify">
<span class = "content">
MPI_Alltoall is a collective communication operation 
in which every process 
sends distinct collection of data to every other process. This is an extension 
of gather and scatter operation also called as total-exchange.
</B>
<BR><BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI AlltoAll Libray call ends ***** -->


<!-- **** ..........MPI Colelctive Comm. & Comp.  Library Calls Ends........******* -->




<!-- ***** MPI Wtime    Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>

<B>Double</B>&nbsp;</I> <B>MPI_Wtime</B></B>( )
 
<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>

double precision</I></B> <B>MPI_Wtime</B>
<BR> <BR>

<FONT COLOR="red"><I> Returns an ellapsed time on the calling processes      
  </i> </FONT> 


<p align = "justify">
<span class = "content">
MPI provides a simple routine MPI_Wtime( ) that can be used to time programs or 
section of programs. MPI_Wtime( ) returns a double precision floating point 
number of seconds, since some arbitrary point of time in the past. The time 
interval can be measured by calling this routine at the beginning and at the 
end of program segment and subtracting the values returned.
<BR><BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI Wtime Libray call ends ***** -->



<!-- *** ....MPI Group Communicators; Derive Data Types & Cartesison Topologies starts...****** -->


<!-- ***** MPI Comm_split   Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>

<B>MPI_Comm_split ( <I>MPI_Comm</I> </B>old_comm, <B><I>int</I></B> split_key, <B><I>int
</I></B>rank_key, <B><I>MPI_Comm*</I></B> new_comm); 

<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>

<B>MPI_Comm_split </B>( comm, size, ierror)
<BR>
<B><I>integer</I></B>comm, size, ierror<BR> <BR>

<FONT COLOR="red"><I> Creates new communicator based on the colors and keys </i> </FONT> 



<p align = "justify">
<span class = "content">
The single call to MPI_Comm_split creates<B><I> q</I></B> new
 communicators, all 
of them having the same name, *new_comm. It creates a new communicator for each 
value of the <B><I>split_key</I></B>. Process with the same value of split_key 
form a new group. The rank in the new group is determined by the value of <B><I>rank_key</I></B>. 
If process A and process B call MPI_Comm split with the same value of 
split_key, and the rank_key argument passed by process A is less than that 
passed by process B, then the rank of A in underlying group&nbsp; new_comm will 
be less than the rank of process B. It is a collective call, and it must be 
called by all the processes in old_comm.&nbsp;
<BR><BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ***** MPI Comm_split   Library Call Ends **** -->



<!-- ***** MPI Comm_group   Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>

<B>MPI_Comm_group ( <I>MPI_Comm</I></B> comm, <B><I>MPI_Group</I></B> *group); 

<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>

<B>MPI_Comm_group</B> (comm, group, ierror);
<BR>
<B><I>integer</I></B>
comm, group, ierror&nbsp;</font> <BR> <BR>

<FONT COLOR="red"><I> Accesses the group associated 
with the given communicator </i> </FONT>  




</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ***** MPI Comm_group   Library Call Ends **** -->


<!-- ***** MPI Comm_Group_include  Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>

<B>MPI_Group_incl ( <I>MPI_Group </I></B>old_group, <B><I>int</I></B> new_group_size,
	<B><I>int*</I></B> ranks_in_old_group, <B><I>MPI_Group</I></B>* 
new_group) 

<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>

<B>MPI_Group_incl </B>(old_group, new_group_size, ranks_in_old_group , 
new_group, ierror)
<BR>
<B><I>integer</I></B>
old_group, new_group_size, ranks_in_old_group (*), new_group, ierror<BR> <BR>

<FONT COLOR="red"><I> Produces a group by reordering an existing group and taking 
only unlisted members </i> </FONT>  


</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ***** MPI Comm_Group_incl   Library call Ends **** -->


<!-- ***** MPI Comm_create   Library call Starts **** -->

<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>

<B>MPI_Comm_create(<I>MPI_Comm</I></B> old_comm, <B><I>MPI_Group </I></B>new_group,
<B><I>MPI_Comm</I></B> * new_comm); 

<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>

<B>MPI_Comm_create</B>(old_comm, new_group, new_comm, ierror); 
<BR>
<B><I>integer</I></B>
old_comm, new_group, new_comm, ierror<BR> <BR>

<FONT COLOR="red"><I> Creates a new communicator </i> </FONT>  



<p align = "justify">
<span class = "content">
Groups and communicators are opaque objects. From a parctical standpoint, this 
means that the details of their internal representation depend on the 
particular implementation of MPI, and, as a consequence, they cannot be 
directly accessed by the user. Rather the user access a handle that refrences 
the opaque object, and the objects are manipulated by special MPI functions 
MPI_Comm_create, MPI_Group_incl and MPI_Comm_group. Contexts are not explicitly 
used in any MPI functions. MPI_Comm_group simply returns the group underlying 
the communicator comm. MPI_Group_incl creates a new group from the list of 
process in the existing group old_group. The number of process in he new group 
is the new_group _size, and the processes to be included are listed in ranks_in 
_old_group. MPI_Comm_create associates a context with the group new_group and 
creates the communicator new_comm. All of the process in new_group belong to 
the group underlying old_comm. MPI_Comm_create is a collective operation. All 
the processes in old_comm must call MPI_Comm_create with the same 
arguments.
<BR><BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ***** MPI Comm_split   Library call Ends **** -->



<!-- ***** MPI Comm_create   Library call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>

<B>MPI_Comm_create(<I>MPI_Comm</I></B> old_comm, <B><I>MPI_Group </I></B>new_group,
<B><I>MPI_Comm</I></B> * new_comm); 

<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>

<B>MPI_Comm_create</B>(old_comm, new_group, new_comm, ierror); 
<BR>
<B><I>integer</I></B>
old_comm, new_group, new_comm, ierror<BR> <BR>

<FONT COLOR="red"><I> Creates a new communicator </i> </FONT>  



<p align = "justify">
<span class = "content">
Groups and communicators are opaque objects. From a parctical standpoint, this 
means that the details of their internal representation depend on the 
particular implementation of MPI, and, as a consequence, they cannot be 
directly accessed by the user. Rather the user access a handle that refrences 
the opaque object, and the objects are manipulated by special MPI functions 
MPI_Comm_create, MPI_Group_incl and MPI_Comm_group. Contexts are not explicitly 
used in any MPI functions. MPI_Comm_group simply returns the group underlying 
the communicator comm. MPI_Group_incl creates a new group from the list of 
process in the existing group old_group. The number of process in he new group 
is the new_group _size, and the processes to be included are listed in ranks_in 
_old_group. MPI_Comm_create associates a context with the group new_group and 
creates the communicator new_comm. All of the process in new_group belong to 
the group underlying old_comm. MPI_Comm_create is a collective operation. All 
the processes in old_comm must call MPI_Comm_create with the same 
arguments.
<BR><BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ***** MPI Comm_split   Library call Ends **** -->


<!-- ***** MPI Cart_create   Library call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>

<B>MPI_Cart_create</B> (<B><I>MPI_Comm</I></B> comm_old, <B><I>int</I></B>
ndims, <B><I>int</I></B> *dims, <B><I>int</I></B> *periods, <B><I>int</I></B> reorder,
<B><I>MPI_Comm</I></B> *comm_cart)

<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>



<B>MPI_Cart_create</B> (comm_old, ndims, dims, periods, reorder, comm_cart, 
ierror) 
<BR>

<B><I>integer</I></B>
comm_old, ndims, dims(*), comm_cart, ierror logical periods(*), reorder<BR> <BR>

<FONT COLOR="red"><I> Makes a new communicator to which 
topology information has been given in the form of Cartesian Coodinates </i> </FONT> 



<p align = "justify">
<span class = "content">
MPI_Cart_create creates a Cartersian decomposition of 
the processes, with the 
number of dimensions given by the number_of_dimensions argument. The user can 
specify the number of processes in any direction by giving a positive value to 
the corresponding element of dimensions_sizes.
arguments.
<BR><BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ***** MPI Cart_create   Library call Ends **** -->


<!-- ***** MPI Cart_rank   Library call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>
<B>MPI_Cart_rank</B> (<B><I>MPI_Comm</I></B> comm, <B><I>int</I></B> *coords, <B><I>int</I></B>
*rank)

<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>



<B>MPI_Cart_rank</B> (comm, coords, rank, ierror)&nbsp;
<BR>
<B><I>integer</I></B> comm, coords (*), rank, ierror

<BR> <BR>

<FONT COLOR="red"><I> Determines process rank in communicator 
given Cartesian location </i> </FONT> 



<p align = "justify">
<span class = "content">
MPI_Cart_rank returns the rank in the Cartesian communicator 
comm of the process 
with Cartesian coordinates. So coordinates is an array with order equal to the 
number of dimensions in the Cartesian topology associated with comm.
<BR><BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ***** MPI Cart_rank   Library call Ends **** -->


<!-- ***** MPI Cart_coords   Library call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>
<B>MPI_Cart_coords</B> (MPI_Comm comm, <B><I>int</I></B> rank, <B><I>int</I></B>
maxdims, <B><I>int</I></B> *coords)

<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>



<B>MPI_Cart_coords</B> (comm, rank, maxdims, coords, ierror)&nbsp;
<BR>
<B><I>integer</I></B>
comm, rank, maxdims, coords (*), ierror

<BR> <BR>

<FONT COLOR="red"><I> Determines process coords in Cartesian 
topology given ranks in  new Commincator </i> </FONT> 



<p align = "justify">
<span class = "content">
MPI_Cart_coords takes input as a rank in a communicator, 
returns the coordinates 
of the process with that rank. MPI_Cart_coords is the inverse to MPI_Cart_Rank; 
it returns the coordinates of the processes with rank rank in the Cartesian 
communicator comm. Note that both of these functions are local.
<BR><BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ***** MPI Cart_coords   Library call Ends **** -->


<!-- ***** MPI Cart_get   Library call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>

<B>MPI_Cart_get</B> (<B><I>MPI_Comm</I></B> comm, <B><I>int</I></B> maxdims, <B><I>int</I></B>
*dims, <B><I>int</I></B> *periods, <B><I>int</I></B> *coords)

<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>



<B>MPI_Cart_get</B> (comm, maxdims, dims, periods, cords, ierror)&nbsp;
<BR>
<B><I>integer</I></B>
comm, maxdims, dims (*), coords (*), ierror, logical periods (*)

<BR> <BR>

<FONT COLOR="red"><I> Retrieve  Cartesian topology information
 associated with a communicator </i> </FONT> 



<p align = "justify">
<span class = "content">
MPI_Cart_get retrieves the coordinates of the calling process in 
communicator.
<BR><BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ***** MPI Cart_get   Library call Ends **** -->



<!-- ***** MPI Cart_shift   Library call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>

<B>MPI_Cart_shift</B> (<B><I>MPI_Comm</I></B> comm, <B><I>int</I></B> direction,
<B><I>int</I></B> disp, <B><I>int</I></B> *rank_source, <B><I>int</I></B> *rank_dest)

<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>



<B>MPI_Cart_shift</B> (comm, direction, disp, rank_source, rank_dest, ierror)
<BR>

<B><I>integer</I></B> comm, direction, disp, rank_source, rank_dest, ierror

<BR> <BR>

<FONT COLOR="red"><I> Returns the  shifted source and destination 
ranks given a shift direction and amount </i> </FONT>  


<p align = "justify">
<span class = "content">
MPI_Cart_shift returns rank of source and destination 
processes in arguments  rank_source and rank_dest respectively.
<BR><BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ***** MPI Cart_shift   Library call Ends **** -->



<!-- ***** MPI Cart_sub   Library call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>

<B>MPI_Cart_sub</B> (MPI_Comm comm, <B><I>int</I></B> *remain_dims, <B><I>MPI_Comm</I></B>
*newcomm)&nbsp;

<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>



<B>MPI_Cart_sub</B> (old_comm, remain_dims, new_comm, ierror)&nbsp;
<BR>
<B><I>integer</I></B> old_comm, newcomm, ierror 

<B><I>logical</I></B> remain_dims(*)

<BR> <BR>

<FONT COLOR="red"><I> Partitions a communicator into subgroups that from 
													
lower-dimensional cartesian subgrids </i> </FONT>  



<p align = "justify">
<span class = "content">
MPI_Cart_sub partitions the processes in cart_comm into a 
collection of disjoint 
communicators whose union is cart_comm. Both cart_comm and each new_comm have 
associated Cartesian topologies.
<BR><BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ***** MPI Cart_sub   Library call Ends **** -->



<!-- ***** MPI Cart_Dims_create   Library call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>

<B>MPI_Dims_create</B> (<B><I>int</I></B> nnodes, <B><I>int</I></B> ndims, <B><I>int</I></B>
*dims)
<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>



<B>MPI_Dims_create</B> (nnodes, ndims, dims, ierror)&nbsp;
<BR>
<B><I>integer</I></B> nnodes, ndims, dims(*), ierror

<BR> <BR>

<FONT COLOR="red"><I> Create a division of processes in the 
Cartesian grid </i> </FONT>  



<p align = "justify">
<span class = "content">
MPI_Dims_create creates a division of processes in a 
Cartesian grid. It is 
useful to choose dimension sizes for a Cartesian coordinate system.
<BR><BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ***** MPI Dims_create   Library call Ends **** -->


<!-- *** ....MPI Group Communicators; Derive Data Types & Cartesison Topologies ends...****** -->

<!-- ***** MPI Waitall   Library call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>
<B>MPI_Waitall</B>(<B><I>int</I></B> count, <B><I>MPI_Request</I></B> *array_of_requests,
<B><I>MPI_Status </I></B>*array_of_statuses)
<BR> <BR>


</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>



<B>MPI_Waitall</B>(count, array_of_requests, array_of_statuses, ierror)
<BR>
<B><I>integer</I></B>
count, array_of_requests (*), array_of_statuses (MPI_status_size, *), ierrror

<BR> <BR>

<FONT COLOR="red"><I> Waits for all given communications to 
complete </i> </FONT>  



<p align = "justify">
<span class = "content">
MPI_Waitall waits for all given communications to 
complete and to test all or 
any of the collection of nonblocking operations.
<BR><BR> 
    
</span> </p>				

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ***** MPI Waitall   Library call Ends **** -->


<div align ="right">
  <a href="#top">
<img src="./../hypack13_images/top.gif" align="right" border="0" width="13"height="13"></a>
</div> 
 
</TD>
</TR>
</TBODY> 
</TABLE> 

<!-- *********************** About Basic MPI-1.X library Calls  Ends ******************** -->


<!-- **************....... About MPI 2.X Overview   Starts .....************ -->


<a name="mpi-2.X-Overview"> </a>

 <TABLE cellPadding=3  border=0> 
    <TBODY> <TR> 

      <TD bgColor = #cccdd77889>  

      <DIV align=Left><font size="2"  color = "Blue" face = "Arial"> 
        <p  align="justify"><b><font face="Verdana" color="black">
  MPI 2.X Overview   </b> </p>
      </font></DIV> 
       </TD> 
     
<TR>
<TD>
<br><br>


 <p align = "justify"> 
  <span class ="content">
The MPI-2 has three "large," completely new areas, which represent extensions of the MPI programming
 model substantially beyond the strict message-passing model represented by MPI-l. These areas are 
parallel I/O, remote memory operations, and dynamic process management. In addition, MPI-2 
introduces a number of features designed to make all of MPI more robust and convenient to use,
 such as external interface specifications, C++ and Fortran-90 bindings, support for threads, 
and mixed-language programming. 
</span> </p>



 <B> <font color ="red"> MPI-2 : Parallel I/O    </font> </B>

 <p align = "justify"> 
  <span class ="content">
The input/output (I/O) in MPI-2  (MPI-IO) can be thought of as Unix I/O plus quite a lot more. 
That is, MPI does including analogues of the basic operations of open, close, seek, read, and write. 
The arguments for these functions are similar to those of the corresponding Unix I/O operations, 
making an initial port of existing programs to MPI relatively straightforward. One of the aims  
of parallel I/O in MPI, is to achieve much higher performance than the Unix I/O API can deliver.  
That is, MPI does including analogues of the basic operations of open, close,
seek, read, and write. The arguments for these functions are similar to those of
the corresponding Unix I/O operations, making an initial port of existing
programs to MPI relatively straightforward. The parallel I/O in MPI, has 
more advanced features,
which includes</span> </p>

<ul>
  <li>noncontiguous access in both memory and file,</li>
  <li>collective I/O operations,</li>
  <li>use of explicit offsets to avoid separate seeks,</li>
  <li>both individual and shared file pointers,</li>
  <li>non blocking I/O,</li>
  <li>portable and customized data representations, and</li>
  <li>hints for the implementations and file system.</li>
</ul></font>

 <p align = "justify"> 
  <span class ="content">
MPI I/O uses the MPI model of communicators and derived data types to describe communication 
between processes and I/O devices. MPI I/O determines which processes are communicating with
 a particular I/O device. Derived data types
define the layout of data in memory and of data in a file on the I/O device. <BR>
</span> </p>



 <B> <font color ="red"> MPI-2 : Remote Memory Operations    </font> </B>

 <p align = "justify"> 
  <span class ="content">

In  message-passing model,  the data is moved from the
address space of one process to that of another by means of a cooperative operation
such as a send/receive pair. This restriction sharply distinguishes the
message-passing model from the shared-memory model, in which processes have
access to a common pool of memory and can simply perform ordinary memory
operations (load from, store into) on some set of addresses.
In MPI-2, an API is defined that provides elements of the shared-memory model
in an MPI environment. There are called MPI's&nbsp; &quot;one-sided&quot; or
&quot;remote memory&quot; operations, Their design was governed by the need to.
</span> </p>


<ul> <span class = "content"> 
  <li> <p align = "justify">
balance efficiency and portability across several classed of
    architectures, including shared-memory multiprocessors (SMPs), nonuniform memory
    access (NUMA) machines, distributed-memory massively parallel processors (MPPs),
    SMP clusters, and even heterogeneous networks; 
</p></li>

    <li> <p align = "justify"> retain the &quot;look and feel&quot; of MPI-1;</p></li>
    <li> <p align = "justify"> deal with subtle memory&nbsp; behavior issues, such as cache 
coherence and
    sequential consistency; and&nbsp;</p></li>
    <li> <p align = "justify"> separate synchronization from data movement to enhance 
performance.</p></li>

</span> 
</ul>


 <B> <font color ="red"> MPI-2 : Dynamic Process Management     </font> </B>

 <p align = "justify"> 
  <span class ="content">

MPI-2 supports creation of new MPI progresses or to establish communication with MPI processes
 that have been started separately. The aim is to design an API for dynamic process management.
 The key to correctness is to make the dynamic process management operations collective, both 
among the process doing the creation of new processes and among the new processes being created.
 The complete MPI 2.1 standard and information for getting involved in this effort are available 
at the MPI Forum Web site (www.mpi-forum.org). <BR> <BR>
</span> </p>

 <p align = "justify"> 
  <span class ="content"> 
The main issues faced in designing an API for dynamic process management are ; <BR> 


<ul> <span content ="class">
  <li> <p align = "justify"> maintaining simplicity and flexibility; </p> </li>
  <li> <p align = "justify">  interacting with the operating system, the resource manager, and the
    process manager in a complex system software environment; and </p> </li>
  <li> <p align = "justify"> avoiding race conditions that compromise correctness </p> </li>
</span>
</ul>

<p align = "justify"> 
  <span class ="content">
The key to correctness is to make the dynamic process management operations
collective, both among the process doing the creation of new processes and among
the new processes being created,. The resulting sets of processes are represented
in an intercommunicator. Intercommunicators ( communicators containing two
groups of processes rather than one) is another  feature of MPI-1, but are
fundamental for the MPI-2, both based on intercommunicators, are creating of new
sets of processes, called spawning, and establishing communications with
pre-existing MPI programs, called connecting.<BR> <BR>
 
The Message Passing Interface (MPI) 2.0 standard has served the parallel technical and scientific 
applications community with rich set of communications API. The new technical challenges, such as
 the emergence of high performance RDMA network support, the need to address scalability at the 
Peta-Scale order of magnitude, fault-tolerance at scale, and the many-core (Multi-Core Processors)
 require new MPI library calls for mixed programming environment. This work will be encapsulated 
in a standard called MPI 3.0. 
</span> </p>

<div align ="right">
  <a href="#top">
<img src="./../hypack13_images/top.gif" align="right" border="0" width="13"height="13"></a>
</div> 

 </TD> 
  </TR> 
  </TBODY> 
   </TABLE>

<!-- ************ --- MPI 2.X Overview Ends ************ -->


<!-- *********..... About MPI-C++ Design  starts ************  -->



<a name ="mpi-cpp-overview"> </a>

 <TABLE cellPadding=3  border=0> 
    <TBODY> <TR> 


    <TD >  

      <DIV align=Left><font size="2"  color = "black" face = "Arial"> 
        <p  align="justify"><b><font face="Verdana" color="black">
           About MPI-C++  Overview </b> </font> 

        <BR> <BR>

      </p>
      </DIV> 
       </TD> 
     </TR>


      <TD bgColor = #cccdd77889>  

      <DIV align=Left><font size="2"  color = "black" face = "Arial"> 
        <p  align="justify"><b><font face="Verdana" color="black">
           About MPI-C++  : Design  </b> </p></font> 
      </DIV> 
       </TD> 
     </TR>
   
<TR>
<TD>

<P align = "justify">
<span class="content">

<BR> <BR>

The C++ language interface for  MPI is designed according to the  
following criteria. The C++ bindings for MPI can almost be deduced
from the C bindings, and there is a simiilarity correspondence
between C++ functions and C functions. <BR> <BR>
</span> </p>
</td>
</tr>


<tr>
<td>

<ul>
<li>
<P align = "justify">
<span class="content">
 The C++ language interface consists of a small set of classes  
  with a lightweight functional interface to  MPI.  The classes are  
  based upon the fundamental  MPI object types (e.g., communicator,  
  group, etc.).  

</span> </p>

</li>

<li>
 
<P align = "justify">
<span class="content">
The  MPI C++ language bindings provide a semantically correct  
  interface to  MPI.   
</span> </p>

</li>
 
<li>
<P align = "justify">
<span class="content">
To the greatest extent possible, the C++ bindings for  MPI  
  functions are member functions of  MPI classes.  
</span> </p> 
 
</li>

</ul>


<BR> 
<P align = "justify">
<span class="content">
<I> <B> Rationale : </b> </I>  

Providing a lightweight set of  MPI objects that correspond to the  
  basic  MPI types is the best fit to  MPI's implicit object-based  
  design; methods can be supplied for these objects to realize  MPI  
  functionality.  The existing C bindings can be used in C++ programs,  
  but much of the expressive power of the C++ language is forfeited.  
  On the other hand, while a comprehensive class library would make  
  user programming more elegant, such a library it is not suitable as  
  a language binding for  MPI since a binding must provide a direct  
  and unambiguous mapping to the specified functionality of  MPI.  <BR>
 </span> </p>

<!-- ********* ........C++ MPI Design ends ****** ....... -->


<!-- *********....... C++ Classes for   MPI starts  ........****** ....... -->

<DIV align=Left><font size="2"  color = "black" face = "Arial"> 
        <p  align="justify"><b><font face="Verdana" color="red">
           C++  classes for MPI  </b> </p></font> 
      </DIV> 
 
<P align = "justify">
<span class="content">
All  MPI classes, constants, and functions are declared within the  
scope of an <B> MPI</B> <B> namespace</B>.  
Thus, instead of the 
<font size = 2, face = " Courier New " color ="#FF00FF" > 


<B> MPI_</B>prefix</font> that is used in C and Fortran,  
 MPI functions essentially have an 

<font size = 2, face = " Courier New " color ="#FF00FF" > 
<B> MPI::</B>prefix</font>.  

</span> </p>


<P align = "justify">
<span class="content">
<B> <I> Note:</I> </B> :  
Although <I><B> namespace</I></B> is officially part of the draft ANSI C++   
  standard, as of this writing it not yet widely implemented in C++ compilers.  
  Implementations using compilers without <I><B> namespace</I></B> may obtain  
  the same scoping through the use of  
  a non-instantiable <I><B> MPI</I></B> class. (To make the <I><B> MPI</I></B> class  
  non-instantiable, all constructors must be <I><B> private</I></B>.)  
 <BR>  <BR>


The members of the <I><B> MPI</I></B> namespace are those classes  
corresponding to objects implicitly used by  MPI.  An abbreviated  
definition of the <I><B> MPI</I></B> namespace for  MPI-1 and its member classes is as  
follows: 
</span> </p>
 
 
<P align = "justify">
<span class="content">

<pre>
<font size = 2, face = " Courier New " color ="#FF00FF" > 
  namespace MPI { 
  &nbsp; class Comm                             {...}; 
  &nbsp; class Intracomm : public Comm          {...}; 
  &nbsp; class Graphcomm : public Intracomm     {...}; 
  &nbsp; class Cartcomm  : public Intracomm     {...}; 
  &nbsp; class Intercomm : public Comm          {...}; 
  &nbsp; class Datatype                         {...}; 
  &nbsp; class Errhandler                       {...}; 
  &nbsp; class Exception                        {...}; 
  &nbsp; class Group                            {...}; 
  &nbsp; class Op                               {...}; 
  &nbsp; class Request                          {...}; 
  &nbsp; class Prequest  : public Request       {...}; 
  &nbsp; class Status                           {...}; 
};
 
</font>
</pre>


</span> </p>

<P align = "justify">
<span class="content">

Addtionally, the following classes are defined for MPI-2:

<pre>
<font size = 2, face = " Courier New " color ="#FF00FF" > 
  namespace MPI { 
  &nbsp;class File                             {...}; 
  &nbsp;class Grequest  : public Request       {...}; 
  &nbsp;class Info                             {...}; 
  &nbsp;class Win                              {...}; 
};
 
</font>
</pre>

 
Note that there are a small number of derived classes, and that virtual  
inheritance is <i> not</i> used.  
</span> </p>

<!-- ********* C++ classes for MPI ends ****** ....... -->



<DIV align=Left><font size="2"  color = "black" face = "Arial"> 
        <p  align="justify"><b><font face="Verdana" color="red">
           Semantics  </b> </p></font> 
      </DIV> 


<P align = "justify">
<span class="content">
The semantics of the member functions constituting the C++ language  
binding for  MPI are specified by the  MPI function description  
itself.  Here, we specify the semantics for those portions of the C++  
language interface that are not part of the language binding.  
In this subsection, functions are prototyped using the type  

<font size = 2, face = " Courier New " color ="#FF00FF" >
    <B> MPI::&lt;CLASS&gt;</B>  </font>



rather than listing each function  
for every  MPI class; the word 

<font size = 2, face = " Courier New " color ="#FF00FF" >
<B> &lt;CLASS&gt;</B> 
</font>

can be  replaced with any valid  MPI class name (e.g., <I><B> Group</I></B>),  
except as noted.  <BR> <BR>
  
</span> </p>

<P align = "justify">
<span class="content">
<B> Construction / Destruction :</b> The default constructor and destructor 
are prototyped as follows:  <BR> <BR> 
 
  
<font size = 2, face = " Courier New " color ="#FF00FF" >
  <B>  MPI::&lt;CLASS&gt;() </B> </font> <BR>

<font size = 2, face = " Courier New " color ="#FF00FF" >
 <B>   MPI::&lt;CLASS&gt;() </B> </font>


<BR> <BR>
 
In terms of construction and destruction, opaque  MPI user level  
objects behave like handles.  Default constructors for all  MPI  
objects   
  
except  MPI::Status  
  
create corresponding  MPI::*_NULL handles.  That  
is, when an  MPI object is instantiated, comparing it with its  
corresponding  MPI::*_NULL object will return <I><B> true</I></B>.  
The default constructors do not create new  MPI opaque objects.  Some  
classes have a member function <I><B> Create()</I></B> for this purpose.  
<BR> <BR>
 
<b> Example : </b>  
  In the following code fragment, the test will return <I><B> true</I></B> and  
  the message will be sent to <I><B> cout</I></B>.  

<pre>
<font size = 2, face = " Courier New " color ="#FF00FF" > 
void foo() 
{ 
  MPI::Intracomm bar; 
  if (bar == MPI::COMM_NULL)  
    cout &lt;&lt; "bar is MPI::COMM_NULL" &lt;&lt; endl; 
} 
</font></pre> 

 
The destructor for each  MPI user level object does <em> not</em> invoke  
the corresponding 
<font size = 2, face = " Courier New " color ="#FF00FF" > 
 MPI_*_FREE function 
</font> (if it exists).  

 
<BR> <BR>

 
 <font size = 2, face = " Courier New " color ="#FF00FF" > 
 MPI_*_FREE </font> function 
 functions are not automatically invoked for  
  the following reasons:  

<ul> 
<li>   
<P align = "justify">
<span class="content">
Automatic destruction contradicts the shallow-copy semantics  
    of the  MPI classes.  
</span> </p>
</li>
 
<li>
 <P align = "justify">
<span class="content">
 The model put forth in  MPI makes memory allocation and  
    deallocation the responsibility of the user, not the implementation. 
</span> </p>
 
</li> 
 
<li>
<P align = "justify">
<span class="content">
 Calling  
 <font size = 2, face = " Courier New " color ="#FF00FF" > 
  MPI_*_FREE </font>
 
      upon destruction could have  
    unintended side effects, including triggering collective  
    operations (this also affects the copy, assignment, and  
    construction semantics).  In the following example, we would want  
    neither <I><B> foo_comm</I></B> nor <I><B> bar_comm</I></B> to automatically invoke  
    <font size = 2, face = " Courier New " color ="#FF00FF" > 
    MPI_*_FREE </font> upon exit from the function.
  </span> </p>

</li>


</ul>
  
<P align = "justify">
<span class="content">

<pre>
<font size = 2, face = " Courier New " color ="#FF00FF" > 

void example_function()  
{ 
  MPI::Intracomm foo_comm(MPI::COMM_WORLD), bar_comm; 
  bar_comm = MPI::COMM_WORLD.Dup(); 
  // rest of function 
} 
</font>
</pre> 
</span> </p>

<P align = "justify">
<span class="content">
<B> Copy / Assignment ; </b> The copy constructor and assignment operator are 
prototyped as follows:  
<BR> <BR> 
  
 <font size = 2, face = " Courier New " color ="#FF00FF" > 
 <B> 
   MPI::&lt;CLASS&gt;(const MPI::&lt;CLASS&gt;&amp; data) <BR>
 </B> 

 <B> 
  MPI::&lt;CLASS&gt;&amp; MPI::&lt;CLASS&gt;::operator=(const MPI::&lt;CLASS&gt;&amp; data) 
 </B> 
  </font> <BR> <BR>

</span> </p>
 
<P align = "justify">
<span class="content">
In terms of copying and assignment, opaque  MPI user level objects  
behave like handles.  Copy constructors perform handle-based (shallow)  
copies.  


 <font size = 2, face = " Courier New " color ="#FF00FF" > 
  MPI::Status </font>
objects are exceptions to this rule.  These  
objects perform deep copies for assignment and copy construction.  
<BR> <BR>
 
<BR> 
<B> Note:  </b>
 
Each  MPI user level object is likely to contain, by value or by  
  reference, implementation-dependent state information.  The  
  assignment and copying of  MPI object handles may simply copy this  
  value (or reference).  <BR> <BR>

  
   <B> Example using assignment operator:  </b>  In this example, 


<font size = 2, face = " Courier New " color ="#FF00FF" > 
  MPI::Intracomm::Dup() </font>
 
   
 is <i> not</i> called for 
<font size = 2, face = " Courier New " color ="#FF00FF" > 
   foo_comm</font>.




 The   object 
<font size = 2, face = " Courier New " color ="#FF00FF" > 
   foo_comm</font>

 is simply an alias for 

<font size = 2, face = " Courier New " color ="#FF00FF" > 
   MPI::COMM_WORLD</font>.


  But 
<font size = 2, face = " Courier New " color ="#FF00FF" > 
   bar_comm</font>

is created with a call to  
  
<font size = 2, face = " Courier New " color ="#FF00FF" > 
  MPI::Intracomm::Dup() </font>
 


and is therefore a different communicator  
    than 
<font size = 2, face = " Courier New " color ="#FF00FF" > 
   foo_comm</font>

(and thus different from 

<font size = 2, face = " Courier New " color ="#FF00FF" > 
   MPI::COMM_WORLD</font>).    
   
<font size = 2, face = " Courier New " color ="#FF00FF" > baz_comm</font>


 becomes an alias for 
<font size = 2, face = " Courier New " color ="#FF00FF" >  bar_comm.</font> 

 If one   
<font size = 2, face = " Courier New " color ="#FF00FF" >  bar_comm</font>
or 
   
<font size = 2, face = " Courier New " color ="#FF00FF" > baz_comm</font> 
 
is freed with 
<font size = 2, face = " Courier New " color ="#FF00FF" > 
   MPI::COMM_NULL</font>
it will be set to   
<font size = 2, face = " Courier New " color ="#FF00FF" > 
   MPI::COMM_NULL.</font> 


 The state of the other handle will be  
    undefined --- it will be invalid, but not  necessarily set to 
<font size = 2, face = " Courier New " color ="#FF00FF" > 
   MPI::COMM_NULL.</font> 


<pre>
<font size = 2, face = " Courier New " color ="#FF00FF" >
<B> MPI::Intracomm foo_comm, bar_comm, baz_comm; <BR>
  foo_comm = MPI::COMM_WORLD; 
  bar_comm = MPI::COMM_WORLD.Dup(); 
  baz_comm = bar_comm; 
  </B>
</font>
</pre> 
  

<B> Comparison  :  </b> 

The comparison operators are prototyped as follows:  <BR> <BR>

  <font size = 2, face = " Courier New " color ="#FF00FF" >
 <B>bool MPI::&lt;CLASS&gt;::operator == (const MPI::&lt;CLASS&gt;&amp; data) const </B> <BR>


  <B> bool MPI::&lt;CLASS&gt;::operator != (const MPI::&lt;CLASS&gt;&amp; data) const  </B> 

</font>

<BR> <BR>
 
The member function <font size = 2, face = " Courier New " color ="#FF00FF" > 
   operator==()</font> returns <I><B> true</I></B> only when  
the handles reference the same internal  MPI object, <I><B> false</I></B> 
otherwise.  <font size = 2, face = " Courier New " color ="#FF00FF" > 
   operator==()</font> returns the boolean complement of 
<font size = 2, face = " Courier New " color ="#FF00FF" > 
   operator==()</font>.  
  
However, since the <I><B> Status</I></B> class is not a handle to an underlying  
 MPI object, it does not make sense to compare 

<font size = 2, face = " Courier New " color ="#FF00FF" >    Status</font> 
 instances.  

Therefore, the 
<font size = 2, face = " Courier New " color ="#FF00FF" >  operator==()</font>


and 
<font size = 2, face = " Courier New " color ="#FF00FF" >  operator!=()</font>


functions are not  defined on the 
<font size = 2, face = " Courier New " color ="#FF00FF" > 
   Status</font> class.   
 
<BR><BR>

  

<B> Constants : </b> Constants are singleton objects and are declared <I><B> const</I></B>.  Note  
that not all globally defined  MPI objects are constant.  For  
example, 
<font size = 2, face = " Courier New " color ="#FF00FF" > 
   MPI::COMM_WORLD</font>

and 
<font size = 2, face = " Courier New " color ="#FF00FF" > 
   MPI::COMM_SELF</font>
are not <I><B>  
    const</I></B>.  
  
</span> </p>

<!-- *********....... C++ Semantics for MPI ends ........****** ....... -->



<!-- *********....... C++ Data types  MPI starts  ........****** ....... -->


<DIV align=Left><font size="2"  color = "black" face = "Arial"> 
        <p  align="justify"><b><font face="Verdana" color="red">
           C++ Datatypes </b> </p></font> 
</DIV>


<P align = "justify">
<span class="content"> 

Some of the  C++ predefiend MPI datatypes, C MPI datatypes, and Fortran MPI datatypes 
 are same. For complete information, please refer MPI web-site.

<b> Communicators : </B> 

The 
<font size = 2, face = " Courier New " color ="#FF00FF" > 
   MPI::COMM_SELF</font>

class hierarchy makes explicit the different  
kinds of communicators implicitly defined by  MPI and allows them to  
be strongly typed.  Since the original design of  MPI defined only  
one type of handle for all types of communicators, the following  
clarifications are provided for the C++ design.  
<BR> <BR>


<b> Types of communicators </B> : 

There are five different types of communicators:  
<font size = 2, face = " Courier New " color ="#FF00FF">MPI::Comm</font>,
<font size = 2, face = " Courier New " color ="#FF00FF">MPI::InterComm</font>,
<font size = 2, face = " Courier New " color ="#FF00FF">MPI::IntraComm</font>,
<font size = 2, face = " Courier New " color ="#FF00FF">MPI::CartComm</font>,
and 
<font size = 2, face = " Courier New " color ="#FF00FF">MPI::GraphComm</font>.

<font size = 2, face = " Courier New " color ="#FF00FF">MPI::Comm</font>,
 is the abstract base communicator class,  
 encapsulating the functionality common to all  MPI  communicators.


<font size = 2, face = " Courier New " color ="#FF00FF">MPI::InterComm</font>,
and 
<font size = 2, face = " Courier New " color ="#FF00FF">MPI::IntraComm</font>,
are derveid from 
<font size = 2, face = " Courier New " color ="#FF00FF">MPI::Comm</font>.

<font size = 2, face = " Courier New " color ="#FF00FF">MPI::CartComm</font>,
and 
<font size = 2, face = " Courier New " color ="#FF00FF">MPI::GraphComm</font>
are derveid from 
<font size = 2, face = " Courier New " color ="#FF00FF">MPI::IntraComm</font>.

<BR> <BR>

Note that functions for MPI collective communications are members of the 
<font size = 2, face = " Courier New " color ="#FF00FF">MPI::Comm</font>
class. Most importantly, initializing a derveid class wth an instance of a
base class is not legal in C++. <BR>  <BR>



The C++ language interface implementation for <font size = 2, face = " Courier New " color ="#FF00FF">
MPI_COMM_NULL</font> is implementation dependent. 

<font size = 2, face = " Courier New " color ="#FF00FF">
MPI_COMM_NULL</font> must be able to be used in comparisons and initializations with 
all types of communciators.



 Note that there are several possibilities for implementation of 
<font size = 2, face = " Courier New " color ="#FF00FF">MPI::
COMM_NULL</font> 
an dit may be  implementattion dependent. <BR> <BR>

The C++ language interface for MPI includes a new function

<font size = 2, face = " Courier New " color ="#FF00FF"> Clone()</font>.

<font size = 2, face = " Courier New " color ="#FF00FF"> MPI::Comm::Clone()</font> is
a pure virtual function.

</span> </p>



<!-- *********......... C++ Datatypes  for MPI C++ ends ..........****** ....... -->


<!-- *********.........  MPI C++ Exceptions ends ..........****** ....... -->


<DIV align=Left><font size="2"  color = "black" face = "Arial"> 
        <p  align="justify"><b><font face="Verdana" color="red">
           C++ Exceptions  </b> </p></font> 
      </DIV>


<P align = "justify">
<span class="content">
The  C++ language interface for  MPI includes the predefined error  
handler 


<font size = 2, face = " Courier New " color ="#FF00FF"> 
MPI::ERRORS_THROW_EXCEPTIONS
</font>

for use with  the 
<font size = 2, face = " Courier New " color ="#FF00FF"> 
Set_errhandler()
</font> member functions.  
  
<font size = 2, face = " Courier New " color ="#FF00FF"> 
MPI::ERRORS_THROW_EXCEPTIONS
</font>

can only be set or retrieved by C++ functions.  
  If a non-C++ program causes an error that invokes the  

<font size = 2, face = " Courier New " color ="#FF00FF"> 
MPI::ERRORS_THROW_EXCEPTIONS
</font> 


error handler, the exception  will pass up the calling stack until C++ code can catch it. 
 If there  is no C++ code to catch it, the behavior is undefined.  In a  
multi-threaded environment or if a non-blocking  MPI call throws an  
exception while making progress in the background, the behavior is  
implementation dependent.  The error handler 

<font size = 2, face = " Courier New " color ="#FF00FF"> 
MPI::ERRORS_THROW_EXCEPTIONS
</font>

 causes an  

<font size = 2, face = " Courier New " color ="#FF00FF"> 
MPI::Exception
</font>



 to be thrown for any  MPI result code other  than 

<font size = 2, face = " Courier New " color ="#FF00FF"> 
MPI::Success
</font> 


The public interface to 
 
<font size = 2, face = " Courier New " color ="#FF00FF"> 
MPI::Exception </font>  
class is defined as follows:  


<pre>
<font size = 2, face = " Courier New " color ="#FF00FF" > 
namespace MPI { 
  class Exception { 
  public: 
 
    Exception(int error_code);  
 
    int Get_error_code() const; 
    int Get_error_class() const; 
    const char *Get_error_string() const; 
  }; 
}; 
</font></pre> 
  

<b> Note:</B>  The exception will be thrown within the body of  
<font size = 2, face = " Courier New " color ="#FF00FF"> 
MPI::ERRORS_THROW_EXCEPTIONS</font>.


 It is expected that control  
  will be returned to the user when the exception is thrown.  
  Some  MPI functions specify certain return information in their  
  parameters in the case of an error and 


<font size = 2, face = " Courier New " color ="#FF00FF"> 
MPI::ERRORS_RETURN
</font> 
  is specified.  The same type of return information must be provided  
  when exceptions are thrown.  <BR> <BR>


For example,  <font size = 2, face = " Courier New " color ="#FF00FF"> 
MPI_WAITALL
</font>  puts an error code for each  
  request in the corresponding entry in the status array and returns  
   MPI_ERR_IN_STATUS.  
<font size = 2, face = " Courier New " color ="#FF00FF"> 
 MPI_ERR_IN_STATUS</font>. 


When using  
<font size = 2, face = " Courier New " color ="#FF00FF"> 
MPI::ERRORS_THROW_EXCEPTIONS</font>, 


it is expected that the  
  error codes in the status array will be set appropriately before the  
  exception is thrown.  

</span> </p>



<!-- *********......... Mixed-Language Operability  for MPI C++ ends .......****** ....... -->


<DIV align=Left><font size="2"  color = "black" face = "Arial"> 
        <p  align="justify"><b><font face="Verdana" color="red">
           Mixed-Language Opearability</b> </p></font> 
</DIV>

<P align = "justify">
<span class="content">

 
The C++ language interface provides functions listed below for  
mixed-language operability.  These functions provide for a  
seamless transition between C and C++.   
  
For the case where the C++  
class corresponding to <font size = 2, face = " Courier New " color ="#FF00FF"> 
 &lt;CLASS&gt;</font>. 


has derived classes, functions  
are also provided for converting between the derived classes and the C  
<font size = 2, face = " Courier New " color ="#FF00FF"> 
MPI_&lt;CLASS&gt;</font>. 

<BR> <BR>
  

<font size = 2, face = " Courier New " color ="#FF00FF"> 
 MPI::&lt;CLASS&gt;&amp; MPI::&lt;CLASS&gt;::operator=(const MPI_&lt;CLASS&gt;&amp; data)
</font> <BR>

<font size = 2, face = " Courier New " color ="#FF00FF"> 
 MPI::&lt;CLASS&gt;(const MPI_&lt;CLASS&gt;&amp; data)
</font> <BR>

<font size = 2, face = " Courier New " color ="#FF00FF"> 
MPI::&lt;CLASS&gt;::operator MPI_&lt;CLASS&gt;() const
</font> <BR>

</span> </p>



<!-- *********......... Mixed-Language Operability  for MPI C++ ends .......****** ....... -->




<DIV align=Left><font size="2"  color = "black" face = "Arial"> 
        <p  align="justify"><b><font face="Verdana" color="red">
           Profiling </b> </p></font> 
</DIV>

<P align = "justify">
<span class="content">

 
In <b> Profiling</b>, the MPI functions used in code are intercepted. Most importantly,
how to layer that underlying implementation to allow functios calls to be intercepted 
and profiled. Usually,  MPI implementation of the MPI C++ bindings is layered on
the top of MPI bindings and hence no extra profiling interace is necessary.
High-quality implementations are rquired to promote portable C++ profiling libraries.
</span>
</p>


</font>
<DIV ALIGN=right><a href="mpi-cpp-overview.html">
 <IMG src="./up.gif " border="0" width="13" height="13"></a>

</TD>
</TR>
</TBODY>
</TABLE>



<!-- *********......... MPI C++ Profiling Ends .......****** ....... -->




<!-- ***************** ......... About MPI-C++ Ends ....... ******************** -->


<!-- *********** ............... MPI 2.X Libraray Calls Starts ........... *********** -->


<a name="mpi-2.x-c-fortran-lib-calls"> </a>

 

<TABLE cellPadding=3  border=0> 
    <TBODY> <TR> 

      <TD bgColor = #cccdd77889>  

      <DIV align=Left><font size="2"  color = "Blue" face = "Arial"> 
        <p  align="justify"><b><font face="Verdana" color="black">
 MPI 2.X library Calls    </b> </p>
      </font></DIV> 
       </TD> 
     </TR> 


<TR>
<TD>
<br><br>

<p align = "justify">
<span class = "content">

The MPI-2 has three "large," completely new areas, which represent extensions of the MPI
 programming model substantially beyond the strict message-passing model represented by MPI-l. 
These areas are parallel I/O, remote memory operations, and dynamic process management. 
In addition, MPI-2 introduces a number of features designed to make all of MPI more robust 
and convenient to use, such as external interface specifications, C++ and Fortran-90 bindings, 
support for threads, and mixed-language programming. 
Most commonly used MPI-2.X Library calls in FORTRAN/C -Language
have been explained below. 
</span> </p>

<BR> <BR>


<!-- ***** MPI-2.X Win Create Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>
<B>int MPI_Win_create </B>(<B><i>void *</i></B>base,<B><i> 
 MPI_Aint</i> </B>size,<B><i> int </i></B>
 disp_unit,<B><i> MPI_Info</i> </B>info,<B>
 <i>MPI_Comm</i> </B>comm,<B> <i>MPI_Win 
 *</i></B>win)

<BR> <BR>
</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>
<B> MPI_Win_Create</B>(base, size, disp_unit,info,comm,win,ierror)<br>

<b> <i> choice</i> </b>base,<B> <i>integer</i> </B>size,
<B> <i>integer</i> </B>disp_unit,<B> <i>integer</i> </B>info,
<B> <i>integer</i> </B>comm, <BR>
<B> <i>integer</i> </B>win,
<B> <i>integer</i> </B>ierror
<BR> <BR>

<FONT COLOR="red"><I> Memory Window Creation</i> </FONT> 

<p align = "justify">
<span class = "content">
Allows each task in an 
intracommunicator group to specify a 
 &quot;window&quot; in its memory that is made 
 accessible to accesses by remote tasks.  					
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI-2.X Win Create Library Call ends ***** -->


<!-- ***** MPI-2.X Win Fence Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>
<B>int MPI_Win_fence (<i>int</i> </B>
 assert,<B> <i>MPI_Win</i> </B>win)
<BR> <BR>
</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>
 <B> MPI_Win_Fence</B>(assert, win, ierror)<br>
 
<b><i>integer</i></b> assert, 
<b><i>integer</i></b> win, 
<b><i>integer</i></b> ierror<br>


<BR> <BR>

<FONT COLOR="red"><I> RMA 
Synchronization</i> </FONT>  


<p align = "justify">
<span class = "content">
Synchronizes RMA calls on a window  <br> 					
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI-2.X Win Fence Library Call ends ***** -->



<!-- ***** MPI-2.X Win Free Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>
<B>int MPI_Win_free(<i>int</i> </B>
 assert,<B> <i>MPI_Win</i> </B>*win)
<BR> <BR>
</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>
 <B> MPI_Win_free</B>(win, ierror)<br>
 

<b><i>integer</i></b> win, 
<b><i>integer</i></b> ierror<br>


<BR> <BR>

<FONT COLOR="red"><I> Frees a window object </i> </FONT>  


<p align = "justify">
<span class = "content">
The program finishes by freeing the window objects it has created with MPI_Win_free  <br> 					
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI-2.X Win Free Library Call ends ***** -->



<!-- ***** MPI-2.X Put Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>
 <B>int MPI_Put </B>(
<B><i>void *       </i></B>origin_addr,
<B><i>int          </i></B>origin_count,
<B><i>MPI_Datatype </i></B>origin_datatype, <BR>
<B><i>int          </i></B>target_rank, 
<B><i>MPI_Aint     </i></B>target_disp,
<B><i>int          </i></B>target_count, <BR>
<B><i>MPI_Datatype </i></B>target_datatype,
<B><i>MPI_Win      </i></B>win)  <br>
 
<BR> <BR>
</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>
<B> 
MPI_Put</B>(origin_addr, origin_count, origin_datatype, 
 target_rank, <BR> target_disp, target_count,  target_datatype,win, ierror) <BR> <BR>


&lt;type&gt; <b> <i> origin_addr(*) </i> </b>

<b><i> integer </i> </b>(kind=MPI_ADDRESS_KIND) target_disp,

<b><i> integer</i> </b>origin_count,
<b><i> integer</i> </b>origin_datatype,
<b><i> integer</i> </b>target_rank, 
<b><i> integer</i> </b>target_count, <BR>
<b><i> integer</i> </b>target_datatype,
<b><i> integer</i> </b>win,
<b><i> integer</i> </b>ierror

<BR> <BR>

<FONT COLOR="red"><I> One-sided communication </i> </FONT>  


<p align = "justify">
<span class = "content">
Transfers data from the origin task to a window at the target task.
MPI_Put to put data into a remote memory window.
  <br> 					
</span> </p>
</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI-2.X Put Library Call ends ***** -->


<!-- ***** MPI-2.X Get Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>
 <B>int MPI_Get </B>(
<B><i>void *       </i></B>origin_addr,
<B><i>int          </i></B>origin_count,
<B><i>MPI_Datatype </i></B>origin_datatype, <BR>
<B><i>int          </i></B>target_rank, 
<B><i>MPI_Aint     </i></B>target_disp,
<B><i>int          </i></B>target_count, <BR>
<B><i>MPI_Datatype </i></B>target_datatype,
<B><i>MPI_Win      </i></B>win)  <br>
 
<BR> <BR>
</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>
<B> 
MPI_get</B>(origin_addr, origin_count, origin_datatype, 
 target_rank, <BR> target_disp, target_count,  target_datatype, win, ierror) <BR> <BR>

&lt;type&gt; <b> <i> origin_addr(*) </i> </b>

<b><i> integer </i> </b>(kind=MPI_ADDRESS_KIND) target_disp,

<b><i> integer</i> </b>origin_count,
<b><i> integer</i> </b>origin_datatype,
<b><i> integer</i> </b>target_rank, 
<b><i> integer</i> </b>target_count, <BR>
<b><i> integer</i> </b>target_datatype,
<b><i> integer</i> </b>win,
<b><i> integer</i> </b>ierror

<BR> <BR>

<FONT COLOR="red"><I> starts a one-sided receive operation</FONT></I> 


<p align = "justify">
<span class = "content">
gets  data from the remoe process and returns data to the calling process. This takes 
the same arguements as MPI_Put but the data moves in the opposite direction. 
MPI_Get to get data from  a remote memory window.
 <br> 					
</span> </p>
</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI-2.X Get Library Call ends ***** -->


<!-- ***** MPI-2.X Accumalate Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>
 <B>int MPI_Accumulate </B>(
<B><i>void *       </i></B>origin_addr,
<B><i>int          </i></B>origin_count,
<B><i>MPI_Datatype </i></B>origin_datatype, <BR>
<B><i>int          </i></B>target_rank, 
<B><i>MPI_Aint     </i></B>target_disp,
<B><i>int          </i></B>target_count, <BR>
<B><i>MPI_Datatype </i></B>target_datatype,
<B><i>MPI_Op </i></B>op,
<B><i>MPI_Win      </i></B>win)  <br>
 
<BR> <BR>
</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>
<B> 
MPI_Accumulate</B>(origin_addr, origin_count, origin_datatype, 
 target_rank, <BR> target_disp, target_count,  target_datatype, op, win, ierror) <BR> <BR>

&lt;type&gt; <b> <i> origin_addr(*) </i> </b>

<b><i> integer </i> </b>(kind=MPI_ADDRESS_KIND) target_disp,

<b><i> integer</i> </b>origin_count,
<b><i> integer</i> </b>origin_datatype,
<b><i> integer</i> </b>target_rank, 
<b><i> integer</i> </b>target_count, <BR>
<b><i> integer</i> </b>target_datatype,
<b><i> integer</i> </b>op,
<b><i> integer</i> </b>win,
<b><i> integer</i> </b>ierror

<BR> <BR>

<FONT COLOR="red"><I> Move and  combines data as a single operation </i> </FONT>  


<p align = "justify">
<span class = "content">
MPI_Accumalate allows data to be moved and combined, at the destination, using 
any of the predefined MPI reduction oepration, such as MPI_SUM. 

MPI_Accumalate operation is a nonblocking function, and MPI_Win_fence library 
should be called to complete the operation.
 <br> 					
</span> </p>
</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI-2.X Get Library Call ends ***** -->



<!-- ***** MPI-2.X File Open Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>
 <b>int MPI_File_open </b>(

 <b> <i>MPI_Comm</i> </b>comm,
 <b> <i>char *</i>  </b>filename,
 <b> <i>int</i> </b>amode,
 <b> <i>MPI_Info</i> </b>info,
 <b> <i>MPI_File </i></b>*fh)<br>


 
<BR> <BR>
</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>
<B> 
MPI_File_open</b>(comm,filename(*),amode,info,fh,ierror)<BR>


<b><i>integer   </i></b> comm, 
<b><i>character </i></b> filename(*), 
<b><i>integer   </i></b> amode, 
<b><i>integer   </i></b> info, 
<b><i>integer   </i></b> fh, 
<b><i>integer   </i></b> ierror 
<BR> <BR>


<FONT COLOR="red"><I> Opens a file </i> </FONT>  


<p align = "justify">
<span class = "content">
MPI_File_open opens the file referred to by filename, sets the default view on 
the file, and sets the access mode amode. MPI_File_open returns a file handle fh 
used for all subsequent operations on the file.
</span> </p>
</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI-2.X File Open  Library Call ends ***** -->



<!-- ***** MPI-2.X File write Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>
 <b>int MPI_File_write</b>(

<B> <i>MPI_File     </i></b>fh,
<b> <i>void *       </i></b>buf,
<b> <i>int          </i></b>count,
<b> <i>MPI_Datatype </i></b>datatype,
<b> <i>MPI_Status </i></b>*status) 

<BR> <BR>
</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>
<B> 
MPI_File_write</b>(fh,buf,count,datatype,status(MPI_STATUS_SIZE),ierror)

<b><i>integer </i> </b> fh,
<b><i>choice  </i> </b>buf,
<b><i>integer </i> </b>count,
<b><i>integer </i> </b>datatype,
<b><i>integer </i> </b>status(mpi_status_size),
<b> <i>integer</i> </b>ierror
 
<BR> <BR>


<FONT COLOR="red"><I> Writes to a file</FONT></I> 


<p align = "justify">
<span class = "content">
This subroutine tries to write, into the file referred to by 
fh, count items of type datatype out of the buffer buf, starting at the
current file location as determined by the value of the individual file
pointer.
</span> </p>
</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI-2.X File write  Library Call ends ***** -->


<!-- ***** MPI-2.X File close Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C  </font> <BR>
 <b>int MPI_File_close </b>(<B> <i>MPI_File</i></b>*fh)


<BR> <BR>
</li>

<font size = 2, face = " Courier New " color ="#FF00FF"> Syntax : Fortran  </font> <BR>
<B> 
MPI_File_close</b>(fh,ierror)

<b><i>integer </i> </b> fh,

<b> <i>integer</i> </b>ierror
 
<BR> <BR>


<FONT COLOR="red"><I> closes a file </i> </FONT>  


<p align = "justify">
<span class = "content">
Closes the file referred to by its file handle fh. It may also delete the file 
if the appropriate mode was set when the file was opened.
</span> </p>
</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI-2.X File close  Library Call ends ***** -->

<div align ="right">
  <a href="#top"><img src="./../hypack13_images/top.gif" align="right" border="0" width="13"height="13"></a>
</div> 
 
</TD>
</TR>
</TBODY> 
</TABLE> 


<!-- *********** ............... MPI 2.X Libraray Calls Ends ........... *********** -->


<!-- ******** ................. MPI C++ Library calls Starts ........ *********** -->




<a name="mpi-cpp-lib-calls"> </a>

 <TABLE cellPadding=3  border=0> 
    <TBODY> <TR> 

      <TD bgColor = #cccdd77889>  

      <DIV align=Left><font size="2"  color = "Blue" face = "Arial"> 
        <p  align="justify"><b><font face="Verdana" color="black">
 MPI C++ library Calls    </b> </p>
      </font></DIV> 
       </TD> 
     </TR> </TBODY> 
   </TABLE> 

       


<TABLE cellPadding=3  border=0> 
 <TBODY>
<TR>
<TD>
<br><br>

<p align = "justify">
<span class = "content">


The C++ language interface for MPI  consists of a small set of classes with a lightweight
 functional
 interface to MPI. The classes are based upon the fundamental MPI object
 types (e.g., communicator, group, etc.). The MPI C++ language bindings provide a 
semantically correct interface to MPI. 
To the greatest extent possible, the C++ bindings for MPI functions are member 
functions of MPI classes. Most commonly used MPI C++ library calls are 
 explained below. 
</span> </p>




<!-- ***** MPI::Init  Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>
<B>void MPI::Init</b><I>(int& argc, char**& argv) </i> <BR>
<B>void MPI::Init</b><I>() </i> <BR>


<BR> <BR>
</li>



<FONT COLOR="red"><I> Initializes the MPI execution environment </i> </FONT>  


<p align = "justify">
<span class = "content">
This call is required in every MPI program and must be the <I>first</I> MPI call. It 
establishes the MPI "environment". Only one invocation of MPI_Init can occur in each 
program execution. It takes the command line arguments as parameters. 					
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI::Init  Library Call ends ***** -->


<!-- ***** MPI::Get_size  Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>
<B>int MPI::Comm::Get_size</b><i>( ) const</i>


<BR> <BR>
</li>



<FONT COLOR="red"><I> Determines the size of the group associated with a communicator </i> </FONT>  


<p align = "justify">
<span class = "content">
This function determines the number of processors executing the program. 
Its first argument is the communicator and it returns the number of processes 
in the communicator in its second argument.				
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI::Get_size  Library Call ends ***** -->

<!-- ***** MPI::Get_rank  Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>
<B>int MPI::Comm::Get_rank</b><i>( ) const</i>


<BR> <BR>
</li>



<FONT COLOR="red"><I> Determines the rank of the calling process in the communicator </i> </FONT>  


<p align = "justify">
<span class = "content">
The first argument to the call is a <I>communicator</I> and the rank of the 
	process is returned in the second argument. Essentially a communicator is 
a collection of processes that can send messages to each other. The only communicator 
needed for basic programs is <B>MPI_COMM_WORLD</B> 
and is predefined in MPI and consists of the processees running when program 
execution begins.				
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI::Get_rank  Library Call ends ***** -->


<!-- ***** MPI::Finalize  Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>
<B>void MPI::Finalize</b><i>( ) const</i>


<BR> <BR>
</li>



<FONT COLOR="red"><I> Terminates MPI execution environment  </i> </FONT>  


<p align = "justify">
<span class = "content">
This call must be made by every process in a MPI computation. It terminates 
the MPI "environment", <B> no </b> MPI calls my be made by a process after 
its call to MPI_Finalize.  
				
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI::Finalize Library Call ends ***** -->



<!-- ***** MPI::Wtime  Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>
<B>Double MPI_Wtime</b><i>(void)</i>


<BR> <BR>
</li>



<FONT COLOR="red"><I> Returns an ellapsed time on the calling processor   </i> </FONT>  


<p align = "justify">
<span class = "content">
MPI provides a simple routine MPI_Wtime( ) that can be used to time programs or section 
of programs. MPI_Wtime( ) returns a double precision floating point number of seconds, 
since some arbitrary point of time in the past. The time interval can be measured by 
calling this routine at the beginning and at the end of program segment and subtracting 
the values returned. 
  				
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI::Finalize Library Call ends ***** -->


<!-- ***** MPI::Send  Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>
<B>void MPI::Comm::Send</b>(<I>const void*</I>buf, <B><I>int</I></B> count,

<B><I> const Datatype&</I></B> datatype, <B><I>int</I></B> dest, 
<B> <I> int</i></B> tag <B><I>) </i></B> const 


<BR> <BR>
</li>


<FONT COLOR="red"><I> Basic send (It is a blocking send call) </i> </FONT>  


<p align = "justify">
<span class = "content">
The first three arguments descibe the message as the address,count and the datatype. 
The content of the message are stored in the block of memory reference by the address. 
The count specifies the number of elements contained in the message which are of a <I>MPI</I> 
type MPI_DATATYPE.
The next argument is the destination, an integer specifying the rank of the destination 
process. The tag argument helps identify messages 
				
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI::Send  Library Call ends ***** -->



<!-- ***** MPI::Recv  Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>
<B>void MPI::Comm::Recv</b>(<I> void*</I>buf, <B><I>int</I></B> count,

<B> <I> const Datatype&</I></B> datatype, <B><I>int</I></B> dest, 
<B> <I> int</i></B> tag <B><I>) </i></B> const 


<BR> <BR>
</li>


<FONT COLOR="red"><I> Basic receive (It is a blocking receive call) </i> </FONT> 


<p align = "justify">
<span class = "content">
The first three arguments descibe the message as the address,count and the datatype. 
The content of the message are stored in the block of memory 
referenced by the address. The count specifies the number of elements contained in 
the message which are of a MPI type MPI_DATATYPE. The next argument
is the 	source which specifies the rank of the sending process.MPI allows the <I>source</I>to be 
a "wild card". There is a predefined constant
 MPI_ANY_SOURCE that can be used if a process is ready to receive a message from <I>any</I> 
sending process rather than a particular sending process. 
The tag argument helps identify messages. The last argument returns information on the data 
that was actually received. It references a record with two fields - one for the source and 
the other for the tag.
			
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI::Recv  Library Call ends ***** -->


<!-- ***** MPI::Bcast  Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>
<B>void MPI::Intracomm::Bcast</b>(<I> void*</I>buffer, <B><I>int</I></B> count,
<B><I> const Datatype&</I></B> datatype, <B><I>int</I></B> root <B><I>)</i></B> const 


<BR> <BR>
</li>


<FONT COLOR="red"><I> Broadcast a message from the process with rank &quot; root &quot; to all 
 other processes of the group  
</i> </FONT> 

<p align = "justify">
<span class = "content">
It is a collective communication call in which a single process sends same data to every process.
It sends a copy of the 
data in <B>message</B> on 
process <B>root</B>to each process in the communicator <B>comm</B>. It should be called by 
all processors in the communicator with the same arguments for root and comm.

			
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI::Bcast  Library Call ends ***** -->


<!-- ***** MPI::Reduce  Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>

<B>void MPI::Intracomm:Reduce(</b>
<B> <I> const void*</I></B> sendbuf, 
<B> <I>  void*</I></B> recvbuf, 
<B><I>int</I></B> count,
<B><I> const Datatype&</I></B> datatype, 
<B><I> const Op&</I></B> op, 
<B><I>int</I></B> root, 
<B><I>) </i></B> const 


<BR> <BR>
</li>


<FONT COLOR="red"><I> Reduce values on all processes to a single value   </i> </FONT> 


<p align = "justify">
<span class = "content">
MPI_Reduce combines the operands stored in *operand using operation <B>op</B> and stores 
the result on <B>*result</B> on the <B>root</B>. Both operand and result refer <B>count</B> 
memory locations with type <B>datatype</B>. MPI_Reduce must be called by all the processor 
in the communicator comm, and count, datatype and op must be same on each processor.
		
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI::Reduce  Library Call ends ***** -->

<!-- ***** MPI::Allreduce  Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>

<B>void MPI::Intracomm:Allreduce(</b>
<B> <I> const void*</I></B> sendbuf, 
<B> <I>  void*</I></B> recvbuf, 
<B><I>int</I></B> count,
<B><I> const Datatype&</I></B> datatype, 
<B><I> const Op&</I></B> op 
<B><I>) </i></B> const 


<BR> <BR>
</li>


<FONT COLOR="red"><I> Combines values from all 
 processes and distribute the result back to all processes     </i> </FONT> 


<p align = "justify">
<span class = "content">
MPI_Allreduce combines values form all processes and
 distribute the result back to all processes
		
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI::Allreduce  Library Call ends ***** -->


<!-- ***** MPI::Scatter   Library Call Starts **** -->


<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>

<B>void MPI::Intracomm::Scatter(</b>
<B> <I> const void*</I></B> sendbuf, 
<B><I>int</I></B> sendcount,
<B><I> const Datatype&</I></B> sendtype,
<B> <I>  void*</I></B> recvbuf,
<B> <I> int </I> </B> recvcount,
<B><I> const Datatype&</I></B> recvtype,
<B><I>int</I></B> root, 
<B><I>) </i></B>
 const 

<BR> <BR>
</li>


<FONT COLOR="red"><I> scatter a group of values to all processes   </i> </FONT> 


<p align = "justify">
<span class = "content">
The process with rank <B><I>root</I></B> distributes
 the contents of <B><I>send_buffer</I></B>among the processes. The contents of send_buffer are
 split into <B><I>p</I></B> segments each consisting of <B><I>send_count</I></B> elements. 
The first segment goes to process 0, the second to process 1 ,etc. The send arguments are 
significant only on process root.
		
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI::Scatter  Library Call ends ***** -->



<!-- ***** MPI::Gather   Library Call Starts **** -->

<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>

<B>void MPI::Intracomm::Gather(</b>
<B> <I> const void*</I></B> sendbuf, 
<B><I>int</I></B> sendcount,
<B><I> const Datatype&</I></B> sendtype,
<B> <I>  void*</I></B> recvbuf,
<B> <I> int </I> </B> recvcount,
<B><I> const Datatype&</I></B> recvtype,
<B><I>int</I></B> root, 
<B><I>) </i></B>
 const 

<BR> <BR>
</li>


<FONT COLOR="red"><I> Gathers together values from a group of tasks   </i> </FONT> 


<p align = "justify">
<span class = "content">
Each process in comm sends the contents of <B><I>send_buffer</I></B> to the process
 with rank <B><I>root</I></B>. The process root concatenates the received data in the
 process rank order in <B><I>recv_buffer</I></B>.&nbsp; The receive arguments are significant 
only on the process with rank root. The argument <B><I>recv_count</I></B> indicates the number
 of items received from each process - not the total number received
		
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI::Gather  Library Call ends ***** -->




<!-- ***** MPI::Allgather   Library Call Starts **** -->

<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>

<B>void MPI::Intracomm::Allgather(</b>
<B> <I> const void*</I></B> sendbuf, 
<B><I>int</I></B> sendcount,
<B><I> const Datatype&</I></B> sendtype,
<B> <I>  void*</I></B> recvbuf,
<B> <I> int </I> </B> recvcount,
<B><I> const Datatype&</I></B> recvtype
<B><I>) </i></B>
 const 

<BR> <BR>
</li>


<FONT COLOR="red"><I> Gathers data from all processes and distribute it to all      </i> </FONT> 


<p align = "justify">
<span class = "content">
MPI_Allgather gathers the contents of each <B><I>send_buffer</I></B> on each process.
 Its <I>effect</I> is the same as if there were a sequence of <B><I>p </I></B>calls to 
MPI_Gather, each of which has a different process acting as a <B><I>root</I></b>.
		
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI::Allgather  Library Call ends ***** -->


<!-- ***** MPI::Alltoall   Library Call Starts **** -->

<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>

<B>void MPI::Intracomm::Alltoall(</b>
<B> <I> const void*</I></B> sendbuf, 
<B><I>int</I></B> sendcount,
<B><I> const Datatype&</I></B> sendtype,
<B> <I>  void*</I></B> recvbuf,
<B> <I> int </I> </B> recvcount,
<B><I> const Datatype&</I></B> recvtype
<B><I>) </i></B>
 const 

<BR> <BR>
</li>


<FONT COLOR="red"><I> Sends data from all to all processes        </i> </FONT> 


<p align = "justify">
<span class = "content">
MPI_Alltoall is a collective communication operation in which every process 
sends distinct collection of data to every other process. This is an extension of 
gather and scatter operation also called as total-exchange. 
 		
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI::Allgather  Library Call ends ***** -->



<!-- ***** MPI::barrier   Library Call Starts **** -->

<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>

<B>void MPI::Intracomm::Barrier(</b>
<B><I>) </i></B>
 const 

<BR> <BR>
</li>


<FONT COLOR="red"><I> Blocks until all process have reached this routine       </i> </FONT> 


<p align = "justify">
<span class = "content">
MPI_Barrier blocks the calling process until all processes in comm have entered the function.
		
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** MPI::barrier  Library Call ends ***** -->



<!-- ***** Intracomm::split   Library Call Starts **** -->

<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>

<B>MPI::Intracomm </B> <BR>
<B>MPI::Intracomm::Split(</b>
<B><I>int </i></B> color,
 <B><I>int </i></B> key
<B><I>) </i></B>
 const 

<BR> <BR>
</li>


<FONT COLOR="red"><I> Creates new communicator based on the colors and keys        </i> </FONT> 


<p align = "justify">
<span class = "content">

The single call to MPI_Comm_split creates<B><I> q</I></B> new communicators, all of 
them having the same name, *new_comm. It creates a new communicator for each value of 
the <B><I>split_key</I></B>. Process with the same value of split_key form a new group.
 The rank in the new group is determined by the value of <B><I>rank_key</I></B>. 
If process A and process B call MPI_Comm split with the same value of split_key, 
and the rank_key argument passed by process A is less than that passed by process B, 
then the rank of A in underlying group&nbsp; new_comm will be less than the rank of 
process B. It is a collective call, and it must be called by all the processes 
in old_comm.		
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** Intracomm::split Library Call ends ***** -->



<!-- ***** Group Comm::Get_group   Library Call Starts **** -->

<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>

<B>MPI::Group </B> <BR>
<B>MPI::Comm::Get_group(</b>
<B><I>) </i></B>
 const 

<BR> <BR>
</li>


<FONT COLOR="red"><I> Accesses the group associated with the given communicator </i> </FONT> 



</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** Group Comm::Get_group Library Call ends ***** -->




<!-- ***** Group Group::Incl   Library Call Starts **** -->

<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>

<B>MPI::Group </B> <BR>
<B> MPI::Group::Incl(</b>
<B><I>int </i></B> n,
 <B><I>const int </i></B> ranks[]
<B><I>) </i></B>

 const 

<BR> <BR>
</li>


<FONT COLOR="red"><I> Produces a group by reordering an existing group 
     and taking only unlisted members  </i> </FONT> 

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** Group Group ::Get_group Library Call ends ***** -->



<!-- ***** Intracomm::Create  Library Call Starts **** -->

<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>

<B>MPI::Intracomm </B> <BR>
<B> MPI::Intracomm::Create(</b>
<B><I>const Group& </i></B> group
<B><I>) </i></B>

<BR> <BR>
</li>


<FONT COLOR="red"><I> Creates new communicator </i> </FONT> 


<p align = "justify">
<span class = "content">

Groups and communicators are opaque objects. From a parctical standpoint, this means
 that the details of their internal representation depend on the 
particular implementation of MPI, and, as a consequence, they cannot be directly 
accessed by the user. Rather the user access a handle that refrences the opaque object, 
and the objects are manipulated by special MPI functions MPI_Comm_create, MPI_Group_incl 
and MPI_Comm_group. Contexts are not explicitly used in any MPI functions. MPI_Comm_group 
simply returns the group underlying the communicator comm. Mpi_Group_incl creates a new 
group from the list of process in the existing group old_group. The number of process in
 the new group is the new_group _size, and the processes to be included are listed in 
ranks_in_old_group. MPI_Comm_create associates a context with the group new_group and 
creates the communicator new_comm. All of the process in new_group belong to the 
group underlying old_comm. MPI_Comm_create is a collective operation. All processes 
in old_comm must call MPI_Comm_create with the same 
arguments.		
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** Intracomm::Create Library Call ends ***** -->



<!-- ***** Cartcomm Intracomm::Create_cart  Library Call Starts **** -->

<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>

<B>MPI::Cartcomm </B> <BR>
<B>MPI::Intracomm::Create_cart(</b>
<B><I>int </i></B> ndims,
<B><I>const int </i></B> dims[],
<B><I>const bool </i></B> periods[],
<B><I>boo </i></B> reorder
<B><I>) </i></B>
const
<BR> <BR>
</li>


<FONT COLOR="red"><I> Makes a new communicator to which topology information has been attached </i></FONT> 


<p align = "justify">
<span class = "content">

MPI_Cart_create creates a Cartersian decomposition of the processes, with the number of 
dimensions given by the number_of_dimensions argument. The user can specify the number of 
processes in any direction by giving a positive value to the corresponding element of 
dimensions_sizes. 
		
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** Cartcomm Intracomm::Create_cart  Library Call ends ***** -->



<!-- ***** Cartcomm Cartcomm::Sub  Library Call Starts **** -->

<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>

<B>MPI::Cartcomm </b> <BR>
<B>MPI::Cartcomm::Sub(</b>
<B><I>const bool </i></B> remain_dims[]
<B><I>) </i></B>
const
<BR> <BR>
</li>


<FONT COLOR="red"><I> Partitions a communicator into subgroups that from lower-dimensional 
cartesian subgrids</i></FONT> 


<p align = "justify">
<span class = "content">

MPI_Cart_sub partitions the processes in cart_comm into a collection of disjoint communicators
whose union is cart_comm. Both cart_comm and each new_comm have associated Cartesian 
topologies.		
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** Cartcomm Cartcomm::Sub  Library Call ends ***** -->



<!-- ***** int Cartcomm::Get_cart_rank  Library Call Starts **** -->

<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>

<B>int MPI::Cartcomm::Get_cart_rank(</b>
<B><I>const int </i></B> coords[]
<B><I>) </i></B>
const
<BR> <BR>
</li>


<FONT COLOR="red"><I> Determines process rank in communicator given Cartesian location  </i></FONT> 


<p align = "justify">
<span class = "content">

MPI_Cart_rank returns the rank in the Cartesian communicator comm of the process with Cartesian 
coordinates. So coordinates is an array with order equal to the number of dimensions in the 
Cartesian topology associated with comm. 
		
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** Cartcomm Cartcomm::Sub  Library Call ends ***** -->



<!-- *****void Cartcomm::Get_topo  Library Call Starts **** -->

<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>

<B>void MPI::Cartcomm::Get_topo(</b>
<B><I> int </i> </b> maxdim, 
<B><I> int </i></B> dims[],
<B><I> bool </I> </B> periods[], 
<B><I> int </I> </B> coords[] 
<B><I>) </i></B>
const
<BR> <BR>
</li>


<FONT COLOR="red"><I> 
    Retrieve Cartesian topology information associated with a communicator </i></FONT> 


<p align = "justify">
<span class = "content">

MPI_Cart_get retrieves the coordinates of the calling process in communicator. 

		
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>

</span> 
</p>

<!-- ******** void Cartcomm::topo  Library Call ends ***** -->



<!-- *****void Cartcomm::Shift  Library Call Starts **** -->

<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>

<B>void MPI::Cartcomm::Shift(</b>
<B><I> int </i> </b> direction, 
<B><I> int </i></B> disp,
<B><I>int& </I> </B> rank_source, 
<B><I> int& </I> </B> rank_dest 
<B><I>) </i></B>
const
<BR> <BR>
</li>


<FONT COLOR="red"><I> 
   Returns the  shifted source and destination ranks given a shift direction and amount 
 </i></FONT> 


<p align = "justify">
<span class = "content">

MPI_Cart_shift returns rank of source and destination processes in arguments 
rank_source and rank_dest respectively. 
	
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>


<!-- ******** Cartcomm Cartcomm::topo  Library Call ends ***** -->



<!-- *****MPI File Open Library Call Starts **** -->

<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>

<B>MPI::File </B><BR> 
<B>MPI::File::Open(</b>
<B><I> const  MPI::Intracomm&,  </i> </b> comm, 
<B><I> const char*  </i> </b> filename,
<B><I> int </i> </b> amode, 
<B><I>const  MPI::Info& </i> </b> info 
<B><I>) </i></B>
<BR> <BR>
</li>


<FONT COLOR="red"><I>  Opens a file  </i></FONT> 


<p align = "justify">
<span class = "content">
MPI_File_open opens the file referred to by filename, sets the default view on 
the file, and sets the access mode amode.
 
	
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>


<!-- ******** MPI File Open  Library Call ends ***** -->



<!-- *****MPI Offset Library Call Starts **** -->

<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>

<B>MPI::Offset </B> &nbsp; &nbsp; <B>  MPI::File::Get_size </b> const <BR>
<BR> <BR>

</li>
</ul>

<blockquote> <HR> </blockquote>


<!-- ******** MPI OffSet   Library Call ends ***** -->



<!-- *****MPI Seek Library Call Starts **** -->

<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>

<B>void MPI::Seek(</B>  
<B> <I> MPI::Offset </i> </b> offset,
<B> <i> int </i> </B> whence
<B><I>) </i></B>
</li>

</ul>

<blockquote> <HR> </blockquote>


<!-- ******** MPI Seek   Library Call ends ***** -->


<!-- *****MPI File SetView Library Call Starts **** -->

<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>

<B>void MPI::File::Set_view(</b>
<B><I> MPI::Offset </i> </b> disp, 
<B><I> const  MPI::Datatype& </i></B> etype, 
<B><I>const MPI::Datatype&</I> </B> filetype,
<B><I> const char*</i></B> datarep[],
<B><I>const MPI::Info&</I> </B>  info
<B><I>)</i></B>
<BR> <BR>
</li>


<FONT COLOR="red"><I> 
Multiple processes can be instructed to share single file
 </i></FONT> 


<p align = "justify">
<span class = "content">
MPI_File_set_view show how multiple processes can be instructed to share a single file.

	
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>


<!-- ******** MPI File Set View  Library Call ends ***** -->


<!-- *****MPI File Read Library Call Starts **** -->

<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>

<B>void MPI::File::Read(</b>
<B><I> void* </i> </b> buf, 
<B><I> int </i> </B> count,
<B><I> const  MPI::Datatype& </i></B> datatype, 
<B><I> MPI::Status&</I> </B> status
<B><I>) </i></B>
<BR> <BR>


<B>void MPI::File::Read(</b>
<B><I> void* </i> </b> buf, 
<B><I> int </i> </B> count,
<B><I> const  MPI::Datatype& </i></B> datatype
<B><I>) </i></B>
<BR> <BR>
</li>

<FONT COLOR="red"><I> 
Read a single file
 </i></FONT> 


<p align = "justify">
<span class = "content">
MPI_File_Read show how file can be read.

	
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>


<!-- ******** MPI File Read  Library Call ends ***** -->



<!-- *****MPI File Write Library Call Starts **** -->

<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>

<B>void MPI::File::Write</b>
<B><I> void* </i> </b> buf, 
<B><I> int </i> </B> count,
<B><I> const  MPI::Datatype& </i></B> datatype, 
<B><I> MPI::Status&</I> </B> status
<B><I>) </i></B>
<BR> <BR>


<B>void MPI::File::Write(</b>
<B><I> void* </i> </b> buf, 
<B><I> int </i> </B> count,
<B><I> const  MPI::Datatype& </i></B> datatype
<B><I>) </i></B>
<BR> <BR>
</li>

<FONT COLOR="red"><I> 
Read a single file
 </i></FONT> 


<p align = "justify">
<span class = "content">
MPI_File_Read show how file can be read.

	
</span> </p>

</li>
</ul>

<blockquote> <HR> </blockquote>


<!-- ******** MPI File Write  Library Call ends ***** -->


<!-- *****MPI Close Library Call Starts **** -->

<ul>

<li>
<font size = 2, face = " Courier New " color ="#FF00FF">  Syntax : C++  </font> <BR>

<B>void MPI::close </B>  <BR>
<BR> <BR>
</li>

</li>
</ul>

<blockquote> <HR> </blockquote>


<!-- ******** MPI close   Library Call ends ***** -->


<div align ="right">
  <a href="#top">
   <img src="./../hypack13_images/top.gif" align="right" border="0" width="13"height="13">
 </a>
</div> 
 
</TD>
</TR>
</TBODY> 
</TABLE> 


<!-  ********* ................. MPI C++ Library calls  Ends ......... *********** -->





<!-- ****************** MPI Peformance Visulization Tools Starts ********************* --> 
<a name="MPI-Performance">

 <TABLE cellPadding=3  border=0> 
    <TBODY> <TR> 

      <TD bgColor = cccdd77889>  

      <DIV align=Left><font size="2"  color = "Blue" face = "Arial"> 
        <p  align="justify"><b><font face="Verdana" color="black">
     MPI Performance Tools    </b> </p>
      </font></DIV> 
       </TD> 
     </TR> </TBODY> 
   </TABLE> 



<TABLE cellPadding=3  border=0> 
<TBODY>
<TR>
<TD>

<BR> <BR>

<UL> 
<LI> 

<p align = "justify">
<span class = "content">

<b><FONT color="red">MPIE : </FONT></b>
MPIE is  MultiProcessing Environment. 

All parallelism is explicit, i.e. the programmer is responsible for correctly identifying 
parallelism and implementing 
the resulting algorithm using <B>MPI</B> constructs.
</span> </p>
</LI>


<LI>
	
<p align = "justify">
<span class = "content">

    <b><FONT color="red">MPIX : </FONT></b>
       MPIX is a Message Passing Interface extension library.
    The MPIX library has been developed at the Mississippi State University NSF Engineering Research Center and 
          currently contains a set of extensions to <B>MPI</B> that allow many functions that previously only worked with 
         intracommunicators to work with intercommunicators. Extensions include enhanced support for  <BR> <BR>
		
<blockquote>
  1. Construction <BR>
  2. Collective operations <BR>
  3. Topologies <BR>	
</blockquote>							
 
</span>
</p>

</li> 



<li>

<p align = "justify">
<span class = "content">

    <b><FONT color="red">UPSHOT : </FONT></b>


	Upshot (Pacheo, 1997, Gropp et. all 1994a-b, 1996a-b, <B><B>MPI</B></B> forum, 1994) is a parallel program 
       performance analysis tool that comes bundled with 
	the public domain mpich implementation of <B>MPI</B>. We discuss it here 
	because it has many features in the common with other parallel performance 
	analysis tools, and it is readily available to use with <B>MPI</B>. Upshot 
	provides some of the information that is not easily determined if we use data 
	generated by serial tools such as prof of simply add counters and/or timers 
       using <B>MPI</B>'s profiling interface. It attempts to provide a unified view 
	of the profiling data generated by each process buy modifying the time stamps 
	of events on different processes so that all the processes start and end at the 
	same time. It also provides a convenient form for visualizing the profiling 
	data in a Gantt chart. There are basically two methods of using Upshot. In the 
	simpler approach, we link our source code with appropriate libraries and obtain 
	information on the time spent by our program in each <B>MPI</B> function. If we 
	desire information on more general segment segments or states of our program, 
	we can get it to provide custom profiling data by adding appropriate function 
	calls to our source code. Upshot is a parallel program performance analysis 
	tool that comes bundled with the public domain mpich implementation of <B>MPI</B>. 
	We discuss it here because it has many features in the common with other 
	parallel performance analysis tools, and it is readily available to use with <B>MPI</B>.
         &nbsp;<BR>
 </span> </p>
									 
</li>

<LI>
<p align = "justify">
<span class = "content">

    <b><FONT color="red">JUMPSHOT : </FONT></b>


	JUMPSHOT is a  Java Based Visualization tool.
	The MPE (Multi-Processing Environment) library is distributed with the freely 
	available <B>MPICH</B> implementation and provides a number of useful 
	facilities, including debugging, logging, graphics, and some common utility 
	routines. MPE was developed for use with <B>MPICH</B> but can be used with 
	other <B>MPI</B> implementations. MPE provides several ways to generate 
	logfiles, which can then be viewed with graphical tools also distributed with 
	MPE. The easiest way to generate logfiles is to link with an MPE library that 
	uses the <B>MPI</B> profiling interface. The user can also insert calls to the 
	MPE logging routines into his or her code. MPE provides two different logfile 
	formats, CLOG and SLOG. CLOG files consist of a simple list of timestamped 
	events and are understood by the Jumpshot-2 viewer. SLOG stands for Scalable 
	LOGfile format and is based on doubly timestamped states. 
      The Jumpshot-3 viewer understands SLOG files.
	</span> </p>
</LI>
</UL>

<div align ="right">
  <a href="#top"><img src="./../hypack13_images/top.gif" align="right" border="0" width="13"height="13"></a>
</div> 	
	
</TD>
</TR>
</TBODY> 
</TABLE> 					

<!-- ***********************  MPI Peformance Visulization Tools  Ends ******************** -->




 <!-- ********************** Compilation and Execution of MPI program starts ****************** -->

<!-- **************** Compilation and Execution of MPI program ***************** -->

<a name="mpi-comp-exec-param-padma">

 <TABLE cellPadding=3  border=0> 
    <TBODY> <TR> 

       <TD bgColor = cccdd77889>
<p align = "justify"> <span class = "content">
      <font color = "black">
  <b>   MPI Prog Env. on PARAM Padma Cluster : Compilation &amp;  Execution     </b> 
      </font> </span> </p>
       </TD> 
     </TR> </TBODY> 
   </TABLE> 

<TABLE cellPadding=3  border=0> 
  <TBODY> 
<TR> 
 <TD>

 
<p align = "justify"> 
<span class = "content">
Compilation & Execution of MPI  programs on </B></FONT>

<font color="red">

<b>PARAM  Padma IBM AIX p630/p690 Node (Power 5/ Power 6) cluster</b></font>

<BR> <BR>


The  MPI programming environment supported on PARAM Padma  are given below.
</span> </P>
  
  <UL>
  <LI>
<p align = "justify"> 
<span class = "content">
	FastEthernet (TCP/IP)/Gigabit Ethernet as system-area network using IBM MPI ("Parallel
          Operating Environment (poe)", of  IBM Parallel Prog, Environment for
  IBM AIX.) 
</span> </p>

</li>

<LI><p align = "justify"> 
<span class = "content">
	FastEthernet (TCP/IP)/Gigabit Ethernet as  system-area network using public domain MPI, mpich-1.2.4<BR
    
</span> </p>
</li> 
</UL>

<p align = "justify"> <span class = "content">

The compilation and execution details of a parallel program that uses <B>MPI</B> 
may vary on different parallel computers. The essential steps of common to all 	parallel systems are same, provided we
 execute one process on each processor.	The three important steps are described below :
</span> </p>


<UL>
<LI>
After compilation, linking with <B>MPI</B> library and creation of executable, 
a copy of the executable program is placed on each processor.&nbsp;
</span> </p>
</li>


<LI> 
<p align = "justify"> <span class = "content">
Each processor begins execution of its copy of the executable.&nbsp;
</span> </p>
</LI>

<LI> 
<p align = "justify"> <span class = "content">
Different processes can execute different statements by branching within the 
program based on their process ranks.&nbsp;
</span> </p>
</LI>

</UL>


<p align = "justify"> <span class = "content">
The application users commonly use two types of <B>MPI</B> programming Paradigm: <B>  SPMD </B> 
(Single Program Multiple Data)
and <B>MPMD </B> (Multiple Program Multiple Data).&nbsp; In <B>SPMD </B> model 
(Single Program Multiple Data), each process runs  the same program in which branching statements may 
 be used. The statement executed by various processes may be different in various segments of the program, 
 but one executable (same program) file runs on all processes. <BR> <BR>



<p align = "justify"> <span class = "content">

In <B>MPMD </B> programming 
     Paradigms, each process may execute different programs, depending on the rank 
      of processes. More than one executable (program) is needed in <B> MPMD </B> 
      model. The application user writes several distinct programs, which may or 
      may not depend on the rank of the processes.&nbsp; Most of the programs in 
      the hands-on-session use <B> SPMD </B>  models  unless specified. Compiling and linking MPMD programs 
    are no different than for <B> SPMD </B> programs other than the fact that there are multiple programs to compile 
  instead of one. 
</span> </p>

<!-- ******** .... PARMA Padma ......... ***** -->


  <BR>
<a name="LINK_AND_RUN_B"></a>

<B><FONT color="red">PARAM Padma : Compilation & Execution </font>  </b> 

  

<p align = "justify"> <span class = "content">

   The following lines show sample compilation using IBM MPI for SPMD/MPMD Programs.
  IBM& MPI provides tools that simplify creation of 
    <B>MPI</B> executables.

As MPI programs may require special libraries and compile options, you should use 
      commands that IBM MPI provides for compiling and linking programs. <BR> <BR>




      The MPI implementation provides two commands for compiling and linking. <BR> <BR> 

 <center>  <FONT color="#FF00FF">
  C programs  (<I>mpcc / mpcc_r </I>) &amp; Fortran programs
   (<I>mpxlf / mpxlf_r / mpxlf_r</I> / <I>mpxlf90</I>)  </font>  </center> <BR> <BR>



     For compilation following commands are used depending on the C or Fortran (mpcc/mpxl<I>f</I>/mpxlf<I>90</I>) program.
	<BR> <BR>
 <FONT color="red"><b> Using MPI 1.X : </b> </font> <BR> <BR>
	&nbsp;
	<CENTER><FONT size = 2 color="#FF00FF"><I>mpcc hello_world.c</I>&nbsp;</FONT></CENTER> <BR>
	<CENTER><FONT size = 2  color="#FF00FF"><I>mpxlf hello_world.f</I>&nbsp;</FONT></CENTER> <BR>
	<CENTER><FONT size = 2 color="#FF00FF"><I>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mpxlff90 hello_world.f90</I></FONT></CENTER>
</span> </p>


<p align = "justify"> <span class = "content">
<P>Commands for linker may include additional libraries.&nbsp; 
For example, to use some routines from the <B>MPI</B> library and math library (ESSL library on AIX), one can use the following command&nbsp;
	<BR> <BR>

	<CENTER><FONT size = 2 color="#FF00FF">
        <I>mpcc -o hello_world hello_world.c -lmass</I>&nbsp;</FONT></CENTER>
<BR> <BR>


These commands are 
located in the directory that contains the <B>MPI</B> libraries i.e., 
     <i>  /usr/lpp/ppe.poe/bin  </I> on IBM AIX. <BR><BR> 

</p></span> 

<!-- ******** ... Using MPI-2 ...........***** -->


<p align = "justify"> <span class = "content">
 <FONT color="red"><b> Using MPI 2.X : </b> </font> <BR> <BR>

For compiliation of  programs which consist of MPI-2  standard calls like Remote Memory Access, Parallel  I/O, Dynamic 
Process Management, you can use the  compilers <BR> <BR>

<CENTER>  <I> mpcc_r  </i> and <i> mpxlf_r. </i> </CENTER> <BR> <BR>
The guidelines are  same as for <BR> <BR>
<CENTER> <i> mpcc/mpxlf  </i> </CENTER> <BR> <BR>

 for compilation and linking  of the programs.
</span></p>

<CENTER>                      
  <FONT color="#FF00FF">
   <I>mpcc_r sum_rma.c</I>&nbsp;<br>  <BR>

  <I>mpxlf_r sum_rma.f</I>&nbsp;

</FONT>
</CENTER>


<p align = "justify"> <span class = "content">
Compiling and linking MPMD programs are no different than for <B> SPMD </B> programs other than the fact that there are multiple programs to compile 
instead of one. For further details,please refer below subsections.  
</span> </p>

                           
<div align ="right">
  <a href="#top"><img src="./../hypack13_images/top.gif" align="right" border="0" width="13"height="13"></a>
</div> 

<!-- ********* .... Using Makefile  ............ ***** -->


<FONT color="red">Using Makefile </B> for SPMD/MPMD MPI 1.X Programs </FONT> <BR> <BR>
 
<p align = "justify"> <span class = "content">

For more control over the process of compiling and linking programs for <I>mpich</I>, 
you should use a '<I>Makefile</I>'. You may also use these commands in <I>Makefile</I>
particularly for programs contained in a large number of files. In addition, 
you can also provide a simple interface to the profiling libraries of <B>MPI</B>
in this <I>Makefile</I>.&nbsp; The <B>MPI</B> profiling interface provides a 
convenient way for you to add performance analysis tools of any <B>MPI</B> implementation.&nbsp; 
The user has to specify the names of the program (s) and appropriate paths to 
link <B>MPI</B> libraries in the <I>Makefile</I>. To compile and link a <B>MPI</B>
program in C or Fortran, you can use the command 

</span> </p>

								
<CENTER><I><FONT color="#FF00FF">make</FONT></I></CENTER>

<p align = "justify"> <span class = "content">

<!--For the hands-on-session on PARAM Padma, the  application user can refer to 
</span> </p>


<blockquote>

  <a href="./mpi-2x-codes/Makefile_C.SPMD"> <i>  <font color = "blue"> Makefile </font> </i> </a>  for SPMD C  <BR>

 
  <a href="./mpi-2x-codes/Makefile_Fort.SPMD"> <i>  <font color = "blue"> Makefile </font> </i> </a> for SPMD Fortran
   for SPMD programs and <BR> 
  <a href="./mpi-2x-codes/Makefile_C.MPMD">  <i> <font color = "blue"> Makefile </font>  </i>  </a> for MPMD C <BR>

  <a href="./mpi-2x-codes/Makefile_Fort.MPMD"> <i> <font color = "blue"> Makefile </font> </i>  </a> for MPMD Fortran <BR> <BR>

</blockquote>  -->

<p align = "justify"> <span class = "content">
For MPMD programs, compilation using <I>IBM  MPI </i>. 

All the <I>makefiles</I> are equivalent and they differ only in specification of executable on the process depending 
on whether the model is MPMD or MPMD program. The command for using

<FONT color="#000000">makefiles for SPMD and MPMD programs is as follows &nbsp;</FONT>

 &nbsp;<P align=center>&nbsp;<I><FONT color="#FF00FF">make -f&nbsp;  Makefile (or) 
 MakeMaster</FONT></I>

</p></span> 

<!-- ******** Using Makefile ***** -->

<div align ="right">
  <a href="#top"><img src="./../hypack13_images/top.gif" align="right" border="0" width="13"height="13"></a>
</div> 

<FONT color="red"><b> Using Makefile </B> for SPMD/MPMD MPI 2.X Programs </FONT> <BR> <BR>
							
<p align = "justify"> <span class = "content">

For compilation and linking of  programs which use MPI-2 standard calls, you need to  
edit the corresponding 
       <i>Makefile </i>as per the guidelines given in the <i>Makefile </i>before  proceeding 
for using make command.

</span> </p>

							

<A name="EXEC_USE_MPIRUN"></A><B><FONT color="red"> 

<p align = "justify"> <span class = "content">
Executing a program: Using&nbsp;poe </B> for SPMD /MPMD Programming Paradigms </FONT><P>To run an <B>MPI</B> program, 

use the&nbsp; <I>poe</I> command, which is located  in&nbsp;<BR>  &nbsp;

<CENTER><FONT color="#FF00FF">`/<I>usr/lpp/ppe.poe/bin</I>'</FONT></CENTER>

</p></span> 



<p align = "justify"> <span class = "content">

For execution of an <STRONG>SPMD</STRONG>   program, you can use this command &nbsp;
</span> </p>



<CENTER><I><FONT color="#FF00FF">

         poe&nbsp;a.out -procs &lt;number of processes&gt;&nbsp;-hfile(or)-hostfile &lt;hostfile name&gt;&nbsp;&nbsp;
</FONT></I></CENTER> <BR> 


    

<p align = "justify"> <span class = "content">

      The argument -procs&nbsp; gives the number of processes that will be associated with the MPI_COMM_WORLD 
      communicator and a.out is the executable file running on all processors.  <BR> <BR>

      The hostnames of the machines on which the MPI  program has to run is specified in&nbsp;

      a hostfile.&nbsp;The executable a.out should be present in the same directory (current directory from 
      where the command has been issued) on all the machines.
   <BR> <BR>


    To execute on 4 processes, type the command <BR> <BR>

      <DIV align=center><I><FONT color="#FF00FF">
         poe&nbsp;a.out  -procs&nbsp;4&nbsp; -hfile&nbsp;hosts&nbsp; <BR> <BR>
      </FONT></I></DIV>

    where the  <i>'hosts'</i> file is as shown given below.</FONT></DIV>


      <CENTER><FONT color="#FF00FF">tf01</FONT></CENTER>
      <CENTER><FONT color="#FF00FF">tf01</FONT></CENTER>
      <CENTER><FONT color="#FF00FF">tf02</FONT></CENTER>
      <CENTER><FONT color="#FF00FF">tf03</FONT></CENTER>
</span> </p>

<p align = "justify"> <span class = "content">

Execution of <EM>poe </EM>command as shown above will 
      execute 4 processes of <Ei>a.out </i>on three nodes of the cluster


       <i> <FONT color="#FF00FF">tf01</FONT></i>, 
       <i> <FONT color="#FF00FF">tf02</FONT></i>, and 
       <i> <FONT color="#FF00FF">tf03</FONT></i>. 

    </span> </p>


<!-- ******* .... MPMD Program **** ..... -->


      <P align="justify"> <span class = "content">

     For execution of an <STRONG>MPMD</STRONG> program, you 
      can use this command <BR> <BR>

<p align = "justify"> <span class = "content">

     <I><FONT color="#FF00FF">
         poe&nbsp;-procs &lt;no of processes&gt;&nbsp;-hfile(or) &nbsp; -hostfile hosts &nbsp;-pgmmodel mpmd 
      </FONT></I>

     </span> </p>


      <P align="justify"> <span class = "content">


The command when executed will prompt 
      for the master and slave executables to be entered on each node name 
      specified in the <i>hostfile </i>as shown below.
</span></P>

      <P align="center"> <span class = "content">

       <FONT color="#FF00FF">&nbsp; &nbsp; 0: tf01 &gt; &nbsp; master</FONT>  <BR>
       <FONT color="#FF00FF">&nbsp; 1: tf01 &gt; &nbsp; slave  </FONT> <BR>
       <FONT color="#FF00FF">&nbsp; 2: tf02 &gt; &nbsp; slave </FONT>  <BR>
       <FONT color="#FF00FF">&nbsp; 3: tf03 &gt; &nbsp; slave  </FONT> <BR> 
    </span> </p>

    <P align="justify"> <span class = "content">

     "master" is the executable to be run 
      with rank 0 and "slave" is the executable for processes with rank other 
      than 0.
     </span></P>


  <P align="justify"> <span class = "content">
  Another version of the same command (i.e. to run an MPMD program) is <BR> <BR>


  <I> <FONT color="#FF00FF">

      poe&nbsp;-procs &lt;no. of processes&gt;&nbsp; -hfile 
      hosts&nbsp;-pgmmodel mpmd -cmdfile &lt;nodefile&gt; </font> <BR>
      <BR> <CENTER> (or)  </CENTER> <BR> 
       <FONT color="#FF00FF">
       poe&nbsp;-procs &lt;no. of processes&gt;&nbsp;
        -hostfile hosts&nbsp;-pgmmodel mpmd -cmdfile &lt;nodefile&gt;

      </FONT></I> <BR> <BR>


   The sample <I>nodefile </I>for running  4 processes is as shown</FONT></P>


   <FONT color="#FF00FF">master</FONT> <BR>
   
   <FONT color="#FF00FF">slave</FONT> <BR>

   <FONT color="#FF00FF">slave</FONT><BR>

   <FONT color="#FF00FF">slave</FONT><BR>

</span> </p>

<div align ="right">
  <a href="#top"><img src="./../hypack13_images/top.gif" align="right" border="0" width="13"height="13"></a>
</div> 
							
</TD> 
</TR> 
</TBODY>   
</TABLE> 

<!-- ******** Compilation and Execution PARAM Padma  Ends *********** --> 



<a name="MPI-compile-execute">



 <TABLE cellPadding=3  border=0> 
    <TBODY> <TR> 

      <TD bgColor = #cccdd77889>  

      <DIV align=Left><font size="2"  color = "Blue" face = "Arial"> 

        <p  align="justify"><b><font face="Verdana" color="black">
    MPI Programming Environment - Compilation and Execution     </b> </p>
      </font></DIV> 

       </TD> 
     </TR> </TBODY> 
   </TABLE> 

<BR> <BR>
   <a name="LINK_AND_RUN"> </a> 
  <FONT color="red"><B>Compilation and Execution of MPI  programs  </B> </font> 


 <TABLE cellPadding=3  border=0> 
 <TBODY>
 <TR> 
  <TD>


<P align = "justify"> <span class = "content">

The  MPI programming environment supported  in <B> HemP-2011 </B> are given below.
</span> </p>
  


<!-- ********** compilation & Linking MPI Prog. ... -->

<a name="LINK_AND_RUN_B"></a>

      
<P align = "justify"> <span class = "content">

The following lines show sample compilation using Intel MPI. Intel-MPI provides tools that simplify 
   creation of 
    <B>MPI</B> executables. As MPI programs may require special libraries and compile 
  options, you should use 
      commands that  Intel MPI provides for compiling and linking programs. <BR> <BR>

 
      The MPI implementation provides two commands for compiling and linking C
    (<I>mpiicc</I>) and Fortran (<I>mpif77</I>/<I>mpif90</I>) programs. 

     For compilation,  following commands are used depending on the C or Fortran 
      (mpiicc/mpi<I>f77</I>) program. <BR>
	
</span> </p>
	
   <P align = "justify"> <span class = "content">
    <blockquote> 

   <FONT color="#FF00FF"><I>mpiicc hello_world.c</I>&nbsp;</FONT>  <BR> <BR> 
   <FONT color="#FF00FF"><I>mpif77 hello_world.f</I>&nbsp;</FONT> <BR> <BR>
   <FONT color="#FF00FF"><I>mpif90 hello_world.f90</I></FONT> <BR> <BR>
    </blockquote>
   
    which are located at   <BR> <BR>

    <blockquote>
   <FONT color="#FF00FF"><I>opt/intel/mpi/3.1/bin64</I>&nbsp;</FONT>
   </blockquote>

   </span> </p>

<P align = "justify"> <span class = "content">
Compiling and linking MPMD programs are no different than for <B> SPMD </B> programs other than the fact that there are multiple programs to compile 
instead of one. For further details,please refer below subsections. </span> </p>
 
                           
<div align ="right">
  <a href="#top"><img src="./../hypack13_images/top.gif" align="right" border="0" width="13"height="13"></a>
</div> 


<!-- ********* ... SPMD MPMD Prog. ****** ..... -->

						
<div align ="right">
  <a href="#top"><img src="./../hypack13_images/top.gif" align="right" border="0" width="13"height="13"></a>
</div> 


</p>

<!-- ********* .......... SPMD/MPMD Prog . ******** ..... -->


<A name="EXEC_USE_MPIRUN"></A>

<B><FONT color="red">  Executing a program: Using mpirun </B> for SPMD /MPMD Programs. </FONT>


<P align = "justify"> 
<span class = "content">

 To run an <B>MPI</B> program, use the&nbsp;<I>mpirun</I> command, which is located  in&nbsp;<BR> 
 <blockquote> <FONT color="#FF00FF">/<I>opt/intel/mpi/3.1/bin64</I>'</FONT></blockquote>
 For execution of an <STRONG>SPMD</STRONG> program, you can use this command &nbsp; <BR> 



 <blockquote>
 <I><FONT color="#FF00FF">

         mpirun&nbsp;-n &lt;no  of processes&gt;&lt;Executable&gt;&nbsp;&nbsp;
	</FONT></I>
 </blockquote> 
 <BR> 


      The argument <i> -n </i>gives the no of processes that will be associated with 
      the MPI_COMM_WORLD communicator and <B> a.out </b> is the executable file running 
      on all processors.  <BR> <BR>


    The executable <B> a.out </b>  should be present in the same directory (current directory from 
    where the command has been issued) on all the machines. 
     Consider a sample command <BR> <BR>

     <blockquote>
         <I><FONT color="#FF00FF">mpirun&nbsp; -n&nbsp;4&nbsp; ./hello_world&nbsp;
      </FONT></I></blockquote>


      For execution of an <STRONG>MPMD</STRONG> program, you 
      can use this command <BR> <BR>

     <blockquote> <font color = "#FF00FF" >
         <i>   mpiexec -n 1 -host dcds2 ./master : -n 8 -host dcds2 ./slave  </i> </font>
     <BR> <BR>


    <I><FONT color="#FF00FF">
         mpirun&nbsp;-n &lt;no. of processes&gt;&nbsp;-host &nbsp;&lt;no. of Hosts&gt; 
                 &nbsp;&lt;Master Executable&gt;<BR> 
 
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

         -n &lt;no. of processes&gt;&nbsp;-host &nbsp;&lt;no of Hosts&gt;
                 &nbsp;&lt;Slave Executable&gt; 
       </font> <BR> <BR>
    </I>
  </blockquote>
    
   In above command "master" is the executable to be run 
      with rank 0 and "slave" is the executable for processes with rank other 
      than 0. <BR> <BR>

</span> 
</p>





<!-- ******************** executing program on cluster : starts ********************* -->


<p align=left> <span class = "content">

  <font color="red" size = "2" ><b> 

  Executing MPI-C program on Message Passing cluster : </b> </font> </b>

To Execute the above Programs on  Message Passing Cluster, the user should submit job to 
scheduler. To submit the job use the following command. </font> <Br> <BR>


   <font  COLOR="#FF00FF">
   qsub -q &lt;queue-name&gt;  -n[no. of processes] [options] mpirun -srun ./&lt;executablename&gt;</font>

<BR> <BR>
       
</span>
</p>


<!-- ****************** executing program on MPI based cluster : ends ************************ -->

<div align ="right">
  <a href="#top"><img src="./../hypack13_images/top.gif" align="right" border="0" width="13"height="13"></a>
</div> 

</TD> 
</TR> 

</TBODY> 
</TABLE> 

<!-- ****************** Compilation and Execution Ends ***************** --> 



<!-- *************** Example Prorgam MPI -C  Starts *************** --> 
<a name="mpi-2.X-example-c-lang">


 <TABLE cellPadding=3  border=0> 
    <TBODY> <TR> 

      <TD bgColor = #cccdd77889>  

      <DIV align=Left><font size="2"  color = "Blue" face = "Arial"> 
        <p  align="justify"><b><font face="Verdana" color="black">
    Example Program  : MPI 2.X - C-language    </b> </p>
      </font></DIV> 
       </TD> 
     </TR> </TBODY> 
   </TABLE> 
<BR> 


<TABLE cellPadding=3  border=0> 
<TBODY> 
<TR>
<TD> 

<p align = "justify">
<span class = "content">

The first C parallel program is "<I>hello_world</I>" program, which simply 
prints the message "<I>Hello _World</I>". Each process sends a message consists 
of characters to another process. If there are <I>p</I> processes executing a 
program, they will have ranks 0, 1,......,<I> p</I>-1. In this example, process 
with rank 1, 2, ......, <I>p</I>-1 will send message to the process with rank 0 
which we call as <I>Root</I>. The <I>Root</I> process receives the message from 
processes with rank 1, 2, ......<I>p</I>-1 and print them out. <BR> <BR>

The simple <B>MPI</B> program in C language prints <I>hello_world</I> message on 
the process with rank <I>0</I> is explained below. We describe the features of 
the entire code and describe the program in details. First few lines of the 
code explain variable definitions, and constants. Followed by these 
declarations, <B>MPI</B> library calls for initialization of <B>MPI</B> environment, 
and MPI communication associated with a communicator are declared in the 
program. The communication describes the communication context and an 
associated group of processes. The calls <I>MPI_Comm_Size</I> returns <I>Numprocs</I>
the number of processes that the user has started for this program. Each 
process finds out its rank in the group associated with a communicator by 
calling <I>MPI_Comm_rank</I>. The following segment of the code explains these features.

</span> </p>


<p align = "justify"> <span class = "content">
The description of program is as follows: 
</span> </p>

<P align = "left"><FONT Color="red" size="2" face="verdana">

#include &lt;stdio.h&gt;&nbsp;

<BR>
#include "mpi.h"

#define BUFLEN 512
int main(argc,argv)

<BR>
int argc; char *argv[];&nbsp;&nbsp;
<BR>
{
<BR>
<Blockquote>
 int MyRank; /* rank of processes */
<BR>
 int Numprocs; /* number of processes */

<BR>

int Destination; /* rank of receiver */
<BR>
 int source; /* rank of sender */
<BR> int tag = 0; /* tag for messages */

<BR>

 int Root = 0; /* Root processes with rank 0 */
<BR>
 char Send_Buffer[BUFLEN],Recv_Buffer[BUFLEN]; /* 
Storage for message 
<BR>
 MPI_Status status; /* returns status for receive */
<BR>
 int iproc,Send_Buffer_Count,Recv_Buffer_Count;</FONT>
<BR> <BR>

<font size="2" face="verdana">
 <I>MyRank</I> is the rank of process and Numprocs 
is the number of processes in the communicator <I>MPI_COMM_WORLD.</I>
<BR> <BR>

<FONT color="red"> /*....<B>MPI</B> initialization.... */ <BR> <BR>

 MPI_Init(&amp;argc,&amp;argv);&nbsp;&nbsp;
<BR>

MPI_Comm_rank(MPI_COMM_WORLD,&amp;MyRank);
<BR>
 MPI_Comm_size(MPI_COMM_WORLD,&amp;Numprocs);</FONT> <BR> <BR>

 Now, each process with <I>MyRank</I> not equal to <I>Root</I>
sends message to <I>Root</I>, i.e., process with rank 0. Process with rank <I>Root</I>
receives message from all processes  and prints the 
message.<BR> <BR>
<FONT color="red"> if(MyRank != 0) 
<BR> 
{  

<blockquote>

 sprintf(Send_Buffer, "Hello World from process with <BR>
   &nbsp; &nbsp; &nbsp; rank %d \n", MyRank); 
<BR>
 Destination = Root;								
<BR>


Send_Buffer_Count=(strlen(Send_Buffer)+1);<BR>

MPI_Send(Send_Buffer,Send_Buffer_Count,MPI_CHAR, Destination, tag, 
MPI_COMM_WORLD);
									

</blockquote>
}
<BR>
else
<BR>
{
<BR>								
 &nbsp; &nbsp;for(iproc = 1; iproc &lt; Numprocs; iproc++) 
<BR>
&nbsp; &nbsp;{
									
<BR>

&nbsp; &nbsp; &nbsp; source = iproc;
									
<BR>


&nbsp; &nbsp; &nbsp; Recv_Buffer_Count=BUFLEN;<BR>

&nbsp; &nbsp; &nbsp; MPI_Recv(Recv_Buffer,Recv_Buffer_Count, 
<BR>
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; MPI_CHAR, source, tag, MPI_COMM_WORLD, 
&amp;status);

<BR> 
&nbsp; &nbsp; &nbsp; printf("\n %s from Process %d *** \n", Recv_Buffer, MyRank);

<BR>
&nbsp;  &nbsp; }
<BR>
 }</FONT>&nbsp;
<BR>
&nbsp;
<BR>
After, this <I>MPI_Finalize</I> is called to terminate the program. Every 
process in <B>MPI</B> computation must make this call. It terminates the <B>MPI</B>
"environment".&nbsp;</FONT>

<P>&nbsp;&nbsp;&nbsp;&nbsp;<FONT color="red" size="2" face="verdana"> 
  /* ....Finalizing the <B>MPI</B>....*/&nbsp;
									
<P>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MPI_Finalize();&nbsp;&nbsp;
<P>
}</FONT>

</blockquote>

</span> 
</p>

<div align ="right">
  <a href="#top"><img src="./../hypack13_images/top.gif" align="right" border="0" width="13"height="13"></a>
</div> 

</TD>
</TR>
</TBODY> 
</TABLE> 

<!-- ************* Example Prorgam MPI - C Ends  ****************** --> 



<!-- ***************  Example Prorgam MPI - Fortran 77 Ends  ****************** --> 
<a name="mpi-2.X-example-f77">

<TABLE cellPadding=3  border=0> 
    <TBODY> <TR> 

      <TD bgColor = #cccdd77889>  

      <DIV align=Left><font size="2"  color = "Blue" face = "Arial"> 
        <p  align="justify"><b><font face="Verdana" color="black">
    Example Program  : MPI 2.X   fortran      </b> </p>
      </font></DIV> 
       </TD> 
     </TR> </TBODY> 
   </TABLE> 
<BR> <BR>

<TABLE cellPadding=3  border=0> 
    <TBODY>
<TR><TD>
<FONT color="red" size = "2" face="verdana">
        program main<BR>
	include "mpif.h"&nbsp;&nbsp;<BR>
	integer&nbsp;&nbsp;&nbsp;MyRank, Numprocs&nbsp;&nbsp;<BR>
	integer&nbsp;&nbsp;&nbsp;Destination, Source, iproc&nbsp;<BR>
		integer&nbsp;&nbsp;&nbsp;Destination_tag, Source_tag&nbsp;<BR>
	integer&nbsp;&nbsp;&nbsp;Root,Send_Buffer_Count,Recv_Buffer_Count &nbsp;<BR>
		integer&nbsp;&nbsp;&nbsp;status(MPI_STATUS_SIZE)&nbsp;<BR>
        character*12&nbsp;&nbsp;&nbsp;Send_Buffer,Recv_Buffer&nbsp;<BR> <BR>
      
C.........Define input Data & MPI parameters<BR>
        data Send_Buffer/'Hello World'/&nbsp;<BR> 
	Root&nbsp;=&nbsp;0&nbsp;<BR>
	Send_Buffer_Count&nbsp;=&nbsp;12&nbsp;<BR>
	Recv_Buffer_Count&nbsp;=&nbsp;12&nbsp;<BR> <BR>

C.........MPI initialization.... &nbsp;<BR>
	call&nbsp;&nbsp; MPI_INIT(ierror)&nbsp;<BR>
	call&nbsp;&nbsp; MPI_COMM_SIZE(MPI_COMM_WORLD, Numprocs, ierror)&nbsp;<BR>
	call&nbsp;&nbsp; MPI_COMM_RANK(MPI_COMM_WORLD, MyRank, ierror)&nbsp;<BR>
	<BR> <BR>
	
	if(MyRank&nbsp;.ne.&nbsp;Root)&nbsp;then<BR> <BR>
	&nbsp;&nbsp;&nbsp;Destination&nbsp;=&nbsp;Root<BR>
	&nbsp;&nbsp;&nbsp;Destination_tag&nbsp;=&nbsp;0<BR>
	&nbsp;&nbsp;&nbsp;call MPI_SEND(Send_Buffer,Send_Buffer_Count,MPI_CHARACTER,<BR>
        $ &nbsp;&nbsp;  &nbsp;&nbsp;Destination, Destination_tag,MPI_COMM_WORLD,ierror)<BR> <BR>
	else<BR> <BR>
	&nbsp;&nbsp;&nbsp;do iproc = 1,Numprocs-1<BR>
	&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; Source =iproc<BR>
	&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; Source_tag&nbsp;=&nbsp;0<BR>
	&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; call MPI_RECV(Recv_Buffer,Recv_Buffer_Count,MPI_CHARACTER,<BR>
        $ &nbsp;&nbsp;&nbsp;&nbsp; &nbsp; Source, Source_tag,MPI_COMM_WORLD,status,ierror)<BR>
	&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;print *,Recv_Buffer,'from Process 
	with Rank',iproc<BR> 
	&nbsp;&nbsp;&nbsp;enddo<BR>
	&nbsp;&nbsp;&nbsp;endif<BR> <BR>

	call MPI_FINALIZE(ierror)<BR> <BR>
	stop<BR>
	end</FONT><BR> <BR>

</TD>
</TR>
</TBODY> 
</TABLE> 
<!-- ****************** Example Prorgam MPI fortran 77  Ends ******************** --> 


</TD>
</TR>
</TBODY> 
</TABLE> 

<!-- ********************* MPI Information and MPI Reference   Ends ******************** -->

     
<div align ="right">
  <a href="#top"><img src="./../hypack13_images/top.gif" align="right" border="0" width="13"height="13"></a>
</div>

</TD></TR>
 <!--  content of web page End here  --> 
 
 	
</TBODY></TABLE>

</TD></TR></TBODY></TABLE>
</TD></TR></TBODY></TABLE></TD></TR></TBODY></TABLE> 


      <TABLE  class=footarea cellSpacing=0 cellPadding=0 border=0 >
        <TBODY>
        <TR>

          <TD class=footertext align=center>
           <A href="http://www.cdac.in" target=_blank><FONT color=blue size=2>
            Centre for Development of Advanced Computing </FONT></A> 
         </TD>
 
        </TR></TBODY></TABLE>


</TD></TR></TBODY></TABLE>

</TD></TR></TBODY></TABLE></BODY></HTML>
